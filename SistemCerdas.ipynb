{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NClcpBPNy9Bx","RddhKbcP1eog","6uTpl9Pg6aO_","jahFHv4MZPqv"],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyPMrCUVTgVnuSUzP84m5zH7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Stage 1 : Preprocessing"],"metadata":{"id":"NClcpBPNy9Bx"}},{"cell_type":"code","source":["#cek column tiap data\n","# ===================================================================\n","# üîç DATASET COLUMN AUDITOR - Check All 11 Files Structure\n","# ===================================================================\n","\n","import pandas as pd\n","import os\n","from collections import Counter\n","\n","# üìÅ Dataset filenames (sesuai dengan yang lu punya)\n","DATASET_FILES = [\n","    'AAPL_al.csv', 'AMZN_yf.csv', 'BAC_al.csv', 'BBCA_yf.csv',\n","    'GOOGL_yf.csv', 'JPM_al.csv', 'META_al.csv', 'MSFT_yf.csv',\n","    'NFLX_al.csv', 'NVDA_al.csv', 'TSLA_yf.csv'\n","]\n","\n","def audit_all_datasets():\n","    \"\"\"Comprehensive audit of all 11 datasets\"\"\"\n","    print(\"üîç STARTING DATASET COLUMN AUDIT\")\n","    print(\"=\"*80)\n","\n","    audit_results = []\n","    all_columns = []\n","    file_status = {}\n","\n","    for i, file_path in enumerate(DATASET_FILES, 1):\n","        print(f\"\\n[{i}/11] üìä Auditing: {file_path}\")\n","        print(\"-\" * 50)\n","\n","        try:\n","            # Load file\n","            df = pd.read_csv(file_path)\n","            ticker = file_path.split('_')[0]\n","\n","            # Basic info\n","            print(f\"‚úÖ File loaded successfully\")\n","            print(f\"   Shape: {df.shape}\")\n","            print(f\"   Columns: {list(df.columns)}\")\n","            print(f\"   Data types: {dict(df.dtypes)}\")\n","\n","            # Sample data\n","            print(f\"   First row sample:\")\n","            for col in df.columns:\n","                sample_val = df[col].iloc[0] if len(df) > 0 else \"N/A\"\n","                print(f\"     {col}: {sample_val}\")\n","\n","            # Missing values\n","            missing = df.isnull().sum()\n","            if missing.sum() > 0:\n","                print(f\"   ‚ö†Ô∏è  Missing values: {dict(missing[missing > 0])}\")\n","            else:\n","                print(f\"   ‚úÖ No missing values\")\n","\n","            # Date column detection\n","            date_cols = [col for col in df.columns if any(word in col.lower() for word in ['date', 'time', 'timestamp'])]\n","            print(f\"   üìÖ Potential date columns: {date_cols}\")\n","\n","            # Store results\n","            audit_results.append({\n","                'File': file_path,\n","                'Ticker': ticker,\n","                'Shape': f\"{df.shape[0]}x{df.shape[1]}\",\n","                'Columns': list(df.columns),\n","                'Column_Count': len(df.columns),\n","                'Missing_Values': missing.sum(),\n","                'Date_Columns': date_cols,\n","                'Status': 'SUCCESS'\n","            })\n","\n","            # Collect all unique columns\n","            all_columns.extend(df.columns.tolist())\n","            file_status[file_path] = 'SUCCESS'\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading file: {str(e)}\")\n","            audit_results.append({\n","                'File': file_path,\n","                'Ticker': file_path.split('_')[0],\n","                'Shape': 'ERROR',\n","                'Columns': [],\n","                'Column_Count': 0,\n","                'Missing_Values': 0,\n","                'Date_Columns': [],\n","                'Status': f'ERROR: {str(e)}'\n","            })\n","            file_status[file_path] = f'ERROR: {str(e)}'\n","\n","    return audit_results, all_columns, file_status\n","\n","def analyze_column_patterns(audit_results, all_columns):\n","    \"\"\"Analyze column patterns across all files\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üìä COLUMN PATTERN ANALYSIS\")\n","    print(\"=\"*80)\n","\n","    # Count column occurrences\n","    column_counts = Counter(all_columns)\n","    print(f\"\\nüî¢ Column Frequency Across All Files:\")\n","    print(\"-\" * 40)\n","    for col, count in column_counts.most_common():\n","        percentage = (count / len(DATASET_FILES)) * 100\n","        status = \"‚úÖ\" if count == len(DATASET_FILES) else \"‚ö†Ô∏è\" if count >= len(DATASET_FILES) * 0.7 else \"‚ùå\"\n","        print(f\"{status} {col:<20} : {count:>2}/11 files ({percentage:5.1f}%)\")\n","\n","    # Standard columns check\n","    standard_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n","    print(f\"\\nüéØ Standard Columns Availability:\")\n","    print(\"-\" * 40)\n","\n","    for std_col in standard_cols:\n","        # Check variations\n","        variations = [col for col in column_counts.keys() if std_col.lower() in col.lower()]\n","        if variations:\n","            print(f\"‚úÖ {std_col} variants found: {variations}\")\n","        else:\n","            print(f\"‚ùå {std_col} NOT FOUND in any file\")\n","\n","    # Unique column sets\n","    print(f\"\\nüìã Unique Column Sets:\")\n","    print(\"-\" * 40)\n","    column_sets = {}\n","    for result in audit_results:\n","        if result['Status'] == 'SUCCESS':\n","            col_tuple = tuple(sorted(result['Columns']))\n","            if col_tuple not in column_sets:\n","                column_sets[col_tuple] = []\n","            column_sets[col_tuple].append(result['Ticker'])\n","\n","    for i, (cols, tickers) in enumerate(column_sets.items(), 1):\n","        print(f\"Set {i}: {tickers}\")\n","        print(f\"  Columns: {list(cols)}\")\n","        print()\n","\n","def generate_column_mapping_suggestions(audit_results):\n","    \"\"\"Generate smart column mapping suggestions\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üß† SMART COLUMN MAPPING SUGGESTIONS\")\n","    print(\"=\"*80)\n","\n","    # Collect all unique columns\n","    all_unique_cols = set()\n","    for result in audit_results:\n","        if result['Status'] == 'SUCCESS':\n","            all_unique_cols.update(result['Columns'])\n","\n","    # Smart mapping\n","    mapping_suggestions = {}\n","\n","    for col in all_unique_cols:\n","        col_lower = col.lower().strip()\n","\n","        # Date mapping\n","        if any(word in col_lower for word in ['date', 'time', 'timestamp', 'datetime']):\n","            mapping_suggestions[col] = 'Date'\n","\n","        # Price mappings\n","        elif 'open' in col_lower:\n","            mapping_suggestions[col] = 'Open'\n","        elif 'high' in col_lower:\n","            mapping_suggestions[col] = 'High'\n","        elif 'low' in col_lower:\n","            mapping_suggestions[col] = 'Low'\n","        elif any(word in col_lower for word in ['close', 'adj close', 'adjusted', 'price']) and 'open' not in col_lower:\n","            mapping_suggestions[col] = 'Close'\n","        elif any(word in col_lower for word in ['volume', 'vol']):\n","            mapping_suggestions[col] = 'Volume'\n","        else:\n","            mapping_suggestions[col] = f'UNKNOWN_{col}'\n","\n","    print(\"üìù Suggested Column Mappings:\")\n","    print(\"-\" * 50)\n","    for original, suggested in mapping_suggestions.items():\n","        status = \"‚úÖ\" if suggested != f'UNKNOWN_{original}' else \"‚ùì\"\n","        print(f\"{status} '{original}' ‚Üí '{suggested}'\")\n","\n","    return mapping_suggestions\n","\n","def create_summary_table(audit_results):\n","    \"\"\"Create summary table\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üìã DATASET SUMMARY TABLE\")\n","    print(\"=\"*80)\n","\n","    # Create DataFrame for better formatting\n","    summary_data = []\n","    for result in audit_results:\n","        summary_data.append({\n","            'Ticker': result['Ticker'],\n","            'Shape': result['Shape'],\n","            'Columns': result['Column_Count'],\n","            'Missing': result['Missing_Values'],\n","            'Status': result['Status'][:20] + '...' if len(result['Status']) > 20 else result['Status']\n","        })\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    print(summary_df.to_string(index=False))\n","\n","    # Statistics\n","    success_count = len([r for r in audit_results if r['Status'] == 'SUCCESS'])\n","    print(f\"\\nüìä Summary Statistics:\")\n","    print(f\"   ‚úÖ Successful files: {success_count}/11\")\n","    print(f\"   ‚ùå Failed files: {11 - success_count}/11\")\n","\n","    return summary_df\n","\n","def generate_preprocessing_code(mapping_suggestions):\n","    \"\"\"Generate custom preprocessing code based on audit\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üõ†Ô∏è  CUSTOM PREPROCESSING CODE GENERATOR\")\n","    print(\"=\"*80)\n","\n","    print(\"Based on audit, here's your custom column mapping:\")\n","    print()\n","    print(\"```python\")\n","    print(\"def smart_column_mapping(df):\")\n","    print('    \"\"\"Custom column mapping based on audit results\"\"\"')\n","    print(\"    column_mapping = {}\")\n","    print()\n","\n","    for original, suggested in mapping_suggestions.items():\n","        if suggested.startswith('UNKNOWN_'):\n","            print(f\"    # TODO: Handle '{original}' - couldn't auto-map\")\n","        else:\n","            print(f\"    if '{original}' in df.columns:\")\n","            print(f\"        column_mapping['{original}'] = '{suggested}'\")\n","\n","    print()\n","    print(\"    return column_mapping\")\n","    print(\"```\")\n","\n","# üöÄ MAIN EXECUTION\n","def main_audit():\n","    \"\"\"Main audit execution\"\"\"\n","    print(\"üîç Starting comprehensive dataset audit...\")\n","\n","    # Step 1: Audit all files\n","    audit_results, all_columns, file_status = audit_all_datasets()\n","\n","    # Step 2: Analyze patterns\n","    analyze_column_patterns(audit_results, all_columns)\n","\n","    # Step 3: Generate mapping suggestions\n","    mapping_suggestions = generate_column_mapping_suggestions(audit_results)\n","\n","    # Step 4: Create summary\n","    summary_df = create_summary_table(audit_results)\n","\n","    # Step 5: Generate custom code\n","    generate_preprocessing_code(mapping_suggestions)\n","\n","    print(f\"\\nüéâ AUDIT COMPLETED!\")\n","    print(f\"üí° Now you can run preprocessing with confidence!\")\n","\n","    return audit_results, mapping_suggestions, summary_df\n","\n","# üöÄ RUN THE AUDIT\n","if __name__ == \"__main__\":\n","    audit_results, mapping_suggestions, summary = main_audit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8XRw5PfzBRx","executionInfo":{"status":"ok","timestamp":1753419306789,"user_tz":-420,"elapsed":187,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"f4c5851a-f532-4b57-fd35-c162ff159d67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Starting comprehensive dataset audit...\n","üîç STARTING DATASET COLUMN AUDIT\n","================================================================================\n","\n","[1/11] üìä Auditing: AAPL_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 80.0\n","     High: 80.69\n","     Low: 77.37\n","     Close: 77.62\n","     Volume: 2487300.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[2/11] üìä Auditing: AMZN_yf.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: AMZN\n","     High: AMZN\n","     Low: AMZN\n","     Close: AMZN\n","     Volume: AMZN\n","     Ticker: nan\n","   ‚ö†Ô∏è  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   üìÖ Potential date columns: ['Date']\n","\n","[3/11] üìä Auditing: BAC_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 64.5\n","     High: 65.19\n","     Low: 63.94\n","     Close: 64.87\n","     Volume: 4018200.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[4/11] üìä Auditing: BBCA_yf.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (2594, 6)\n","   Columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n","   Data types: {'Price': dtype('O'), 'Close': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Open': dtype('O'), 'Volume': dtype('O')}\n","   First row sample:\n","     Price: Ticker\n","     Close: BBCA.JK\n","     High: BBCA.JK\n","     Low: BBCA.JK\n","     Open: BBCA.JK\n","     Volume: BBCA.JK\n","   ‚ö†Ô∏è  Missing values: {'Close': np.int64(1), 'High': np.int64(1), 'Low': np.int64(1), 'Open': np.int64(1), 'Volume': np.int64(1)}\n","   üìÖ Potential date columns: []\n","\n","[5/11] üìä Auditing: GOOGL_yf.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: GOOGL\n","     High: GOOGL\n","     Low: GOOGL\n","     Close: GOOGL\n","     Volume: GOOGL\n","     Ticker: nan\n","   ‚ö†Ô∏è  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   üìÖ Potential date columns: ['Date']\n","\n","[6/11] üìä Auditing: JPM_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 86.62\n","     High: 87.12\n","     Low: 83.37\n","     Close: 83.56\n","     Volume: 4510867.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[7/11] üìä Auditing: META_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (3314, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 2012-05-18\n","     Open: 42.05\n","     High: 45.0\n","     Low: 38.0\n","     Close: 38.2318\n","     Volume: 573576400.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[8/11] üìä Auditing: MSFT_yf.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: MSFT\n","     High: MSFT\n","     Low: MSFT\n","     Close: MSFT\n","     Volume: MSFT\n","     Ticker: nan\n","   ‚ö†Ô∏è  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   üìÖ Potential date columns: ['Date']\n","\n","[9/11] üìä Auditing: NFLX_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (5830, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 2002-05-23\n","     Open: 16.19\n","     High: 17.4\n","     Low: 16.04\n","     Close: 16.75\n","     Volume: 7485000.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[10/11] üìä Auditing: NVDA_al.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 21.75\n","     High: 24.38\n","     Low: 21.75\n","     Close: 23.5\n","     Volume: 1630300.0\n","   ‚úÖ No missing values\n","   üìÖ Potential date columns: []\n","\n","[11/11] üìä Auditing: TSLA_yf.csv\n","--------------------------------------------------\n","‚úÖ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: TSLA\n","     High: TSLA\n","     Low: TSLA\n","     Close: TSLA\n","     Volume: TSLA\n","     Ticker: nan\n","   ‚ö†Ô∏è  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   üìÖ Potential date columns: ['Date']\n","\n","================================================================================\n","üìä COLUMN PATTERN ANALYSIS\n","================================================================================\n","\n","üî¢ Column Frequency Across All Files:\n","----------------------------------------\n","‚úÖ Open                 : 11/11 files (100.0%)\n","‚úÖ High                 : 11/11 files (100.0%)\n","‚úÖ Low                  : 11/11 files (100.0%)\n","‚úÖ Close                : 11/11 files (100.0%)\n","‚úÖ Volume               : 11/11 files (100.0%)\n","‚ùå Unnamed: 0           :  6/11 files ( 54.5%)\n","‚ùå Date                 :  4/11 files ( 36.4%)\n","‚ùå Ticker               :  4/11 files ( 36.4%)\n","‚ùå Price                :  1/11 files (  9.1%)\n","\n","üéØ Standard Columns Availability:\n","----------------------------------------\n","‚úÖ Date variants found: ['Date']\n","‚úÖ Open variants found: ['Open']\n","‚úÖ High variants found: ['High']\n","‚úÖ Low variants found: ['Low']\n","‚úÖ Close variants found: ['Close']\n","‚úÖ Volume variants found: ['Volume']\n","\n","üìã Unique Column Sets:\n","----------------------------------------\n","Set 1: ['AAPL', 'BAC', 'JPM', 'META', 'NFLX', 'NVDA']\n","  Columns: ['Close', 'High', 'Low', 'Open', 'Unnamed: 0', 'Volume']\n","\n","Set 2: ['AMZN', 'GOOGL', 'MSFT', 'TSLA']\n","  Columns: ['Close', 'Date', 'High', 'Low', 'Open', 'Ticker', 'Volume']\n","\n","Set 3: ['BBCA']\n","  Columns: ['Close', 'High', 'Low', 'Open', 'Price', 'Volume']\n","\n","\n","================================================================================\n","üß† SMART COLUMN MAPPING SUGGESTIONS\n","================================================================================\n","üìù Suggested Column Mappings:\n","--------------------------------------------------\n","‚ùì 'Ticker' ‚Üí 'UNKNOWN_Ticker'\n","‚úÖ 'Open' ‚Üí 'Open'\n","‚úÖ 'Low' ‚Üí 'Low'\n","‚úÖ 'Date' ‚Üí 'Date'\n","‚ùì 'Unnamed: 0' ‚Üí 'UNKNOWN_Unnamed: 0'\n","‚úÖ 'Volume' ‚Üí 'Volume'\n","‚úÖ 'High' ‚Üí 'High'\n","‚úÖ 'Price' ‚Üí 'Close'\n","‚úÖ 'Close' ‚Üí 'Close'\n","\n","================================================================================\n","üìã DATASET SUMMARY TABLE\n","================================================================================\n","Ticker  Shape  Columns  Missing  Status\n","  AAPL 6471x6        6        0 SUCCESS\n","  AMZN 2515x7        7        2 SUCCESS\n","   BAC 6471x6        6        0 SUCCESS\n","  BBCA 2594x6        6        5 SUCCESS\n"," GOOGL 2515x7        7        2 SUCCESS\n","   JPM 6471x6        6        0 SUCCESS\n","  META 3314x6        6        0 SUCCESS\n","  MSFT 2515x7        7        2 SUCCESS\n","  NFLX 5830x6        6        0 SUCCESS\n","  NVDA 6471x6        6        0 SUCCESS\n","  TSLA 2515x7        7        2 SUCCESS\n","\n","üìä Summary Statistics:\n","   ‚úÖ Successful files: 11/11\n","   ‚ùå Failed files: 0/11\n","\n","================================================================================\n","üõ†Ô∏è  CUSTOM PREPROCESSING CODE GENERATOR\n","================================================================================\n","Based on audit, here's your custom column mapping:\n","\n","```python\n","def smart_column_mapping(df):\n","    \"\"\"Custom column mapping based on audit results\"\"\"\n","    column_mapping = {}\n","\n","    # TODO: Handle 'Ticker' - couldn't auto-map\n","    if 'Open' in df.columns:\n","        column_mapping['Open'] = 'Open'\n","    if 'Low' in df.columns:\n","        column_mapping['Low'] = 'Low'\n","    if 'Date' in df.columns:\n","        column_mapping['Date'] = 'Date'\n","    # TODO: Handle 'Unnamed: 0' - couldn't auto-map\n","    if 'Volume' in df.columns:\n","        column_mapping['Volume'] = 'Volume'\n","    if 'High' in df.columns:\n","        column_mapping['High'] = 'High'\n","    if 'Price' in df.columns:\n","        column_mapping['Price'] = 'Close'\n","    if 'Close' in df.columns:\n","        column_mapping['Close'] = 'Close'\n","\n","    return column_mapping\n","```\n","\n","üéâ AUDIT COMPLETED!\n","üí° Now you can run preprocessing with confidence!\n"]}]},{"cell_type":"code","source":["# ===================================================================\n","# üõ†Ô∏è STAGE 1: CUSTOM PREPROCESSING - Based on Audit Results\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# üìÅ Dataset classification based on audit\n","ALPHA_VANTAGE_FILES = [\n","    'AAPL_al.csv', 'BAC_al.csv', 'JPM_al.csv',\n","    'META_al.csv', 'NFLX_al.csv', 'NVDA_al.csv'\n","]\n","\n","YFINANCE_CORRUPTED_FILES = [\n","    'AMZN_yf.csv', 'GOOGL_yf.csv', 'MSFT_yf.csv', 'TSLA_yf.csv'\n","]\n","\n","BBCA_FILES = ['BBCA_yf.csv']\n","\n","def extract_ticker(filename):\n","    \"\"\"Extract ticker from filename\"\"\"\n","    return filename.split('_')[0]\n","\n","def process_alpha_vantage_file(file_path):\n","    \"\"\"Process Alpha Vantage format files\"\"\"\n","    print(f\"üîß Processing Alpha Vantage: {file_path}\")\n","\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    # Rename 'Unnamed: 0' to 'Date'\n","    df = df.rename(columns={'Unnamed: 0': 'Date'})\n","\n","    # Add ticker column\n","    df['Ticker'] = ticker\n","\n","    # Standardize column order\n","    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ‚úÖ Success: {len(df)} records, Date range: {df['Date'].min()} to {df['Date'].max()}\")\n","    return df\n","\n","def process_yfinance_corrupted_file(file_path):\n","    \"\"\"Process corrupted YFinance files\"\"\"\n","    print(f\"üîß Processing Corrupted YFinance: {file_path}\")\n","\n","    # Read file - header is corrupted, data starts from row 1\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    print(f\"   Original shape: {df.shape}\")\n","    print(f\"   First row sample: {df.iloc[0].to_dict()}\")\n","\n","    # Check if first row contains ticker data (corruption pattern)\n","    if df.iloc[0]['Open'] == ticker or str(df.iloc[0]['Open']).upper() == ticker:\n","        print(f\"   üö® Detected corruption: removing header row\")\n","        # Remove first row (contains ticker symbols)\n","        df = df.iloc[1:].copy()\n","        df = df.reset_index(drop=True)\n","\n","    # Handle missing/corrupted columns\n","    if 'Date' in df.columns:\n","        # Remove rows where Date is NaN or contains ticker symbol\n","        df = df[df['Date'].notna()]\n","        df = df[df['Date'] != ticker]\n","        df = df[~df['Date'].astype(str).str.upper().eq(ticker)]\n","\n","    # Clean data types\n","    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n","    for col in numeric_cols:\n","        if col in df.columns:\n","            # Remove ticker symbols from numeric columns\n","            df[col] = df[col].astype(str).str.replace(ticker, '')\n","            df[col] = df[col].str.replace(ticker.upper(), '')\n","            # Convert to numeric\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Remove rows with all NaN numeric data\n","    df = df.dropna(subset=numeric_cols, how='all')\n","\n","    # Add/fix ticker column\n","    df['Ticker'] = ticker\n","\n","    # Standardize column order\n","    standard_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","    df = df[standard_cols]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","    df = df.dropna(subset=['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ‚úÖ Cleaned: {len(df)} records, Date range: {df['Date'].min()} to {df['Date'].max()}\")\n","    return df\n","\n","def process_bbca_file(file_path):\n","    \"\"\"Process BBCA format file\"\"\"\n","    print(f\"üîß Processing BBCA: {file_path}\")\n","\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    print(f\"   Original shape: {df.shape}\")\n","    print(f\"   Original columns: {list(df.columns)}\")\n","    print(f\"   First row: {df.iloc[0].to_dict()}\")\n","\n","    # Check for corruption pattern\n","    if df.iloc[0]['Open'] == 'BBCA.JK' or 'BBCA' in str(df.iloc[0]['Open']):\n","        print(f\"   üö® Detected corruption: removing header row\")\n","        df = df.iloc[1:].copy()\n","        df = df.reset_index(drop=True)\n","\n","    # Handle Price column (appears to be Date)\n","    if 'Price' in df.columns:\n","        # Rename Price to Date if it contains date-like values\n","        sample_val = str(df['Price'].iloc[0]) if len(df) > 0 else \"\"\n","        if any(char in sample_val for char in ['-', '/', '20']):  # Date-like pattern\n","            df = df.rename(columns={'Price': 'Date'})\n","\n","    # Clean numeric columns\n","    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n","    for col in numeric_cols:\n","        if col in df.columns:\n","            # Remove ticker symbols\n","            df[col] = df[col].astype(str).str.replace('BBCA.JK', '')\n","            df[col] = df[col].astype(str).str.replace('BBCA', '')\n","            # Convert to numeric\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Remove rows with all NaN\n","    df = df.dropna(subset=numeric_cols, how='all')\n","\n","    # Add ticker\n","    df['Ticker'] = ticker\n","\n","    # Standardize columns\n","    if 'Date' not in df.columns:\n","        # If still no Date column, create index-based dates (last resort)\n","        df['Date'] = pd.date_range(start='2014-01-01', periods=len(df), freq='D')\n","        print(f\"   ‚ö†Ô∏è No date column found, created synthetic dates\")\n","\n","    # Reorder columns\n","    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","    df = df.dropna(subset=['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ‚úÖ Processed: {len(df)} records\")\n","    return df\n","\n","def validate_processed_data(df, ticker):\n","    \"\"\"Validate processed data quality\"\"\"\n","    issues = []\n","\n","    # Check required columns\n","    required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","    missing_cols = [col for col in required_cols if col not in df.columns]\n","    if missing_cols:\n","        issues.append(f\"Missing columns: {missing_cols}\")\n","\n","    # Check data types\n","    if 'Date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['Date']):\n","        issues.append(\"Date column not datetime\")\n","\n","    # Check for negative prices\n","    price_cols = ['Open', 'High', 'Low', 'Close']\n","    for col in price_cols:\n","        if col in df.columns and (df[col] < 0).any():\n","            issues.append(f\"Negative values in {col}\")\n","\n","    # Check for missing data\n","    missing_data = df.isnull().sum()\n","    critical_missing = missing_data[missing_data > len(df) * 0.1]  # >10% missing\n","    if len(critical_missing) > 0:\n","        issues.append(f\"High missing data: {dict(critical_missing)}\")\n","\n","    # Check date range\n","    if 'Date' in df.columns:\n","        date_range = df['Date'].max() - df['Date'].min()\n","        if date_range.days < 365:  # Less than 1 year\n","            issues.append(f\"Short date range: {date_range.days} days\")\n","\n","    return issues\n","\n","def save_processed_data(df, ticker, output_dir='processed_data'):\n","    \"\"\"Save processed data\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    filename = f\"{output_dir}/{ticker}_processed.csv\"\n","    df.to_csv(filename, index=False)\n","\n","    print(f\"   üíæ Saved: {filename}\")\n","    return filename\n","\n","def generate_processing_report(results):\n","    \"\"\"Generate comprehensive processing report\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üìã STAGE 1 PROCESSING REPORT\")\n","    print(\"=\"*80)\n","\n","    successful = [r for r in results if r['status'] == 'SUCCESS']\n","    failed = [r for r in results if r['status'] != 'SUCCESS']\n","\n","    print(f\"‚úÖ Successfully processed: {len(successful)}/11 files\")\n","    print(f\"‚ùå Failed: {len(failed)}/11 files\")\n","\n","    if successful:\n","        print(f\"\\nüìä Successful Files:\")\n","        for result in successful:\n","            print(f\"   ‚úÖ {result['ticker']}: {result['records']} records ({result['date_range']})\")\n","\n","    if failed:\n","        print(f\"\\n‚ùå Failed Files:\")\n","        for result in failed:\n","            print(f\"   ‚ùå {result['ticker']}: {result['error']}\")\n","\n","    # Data quality issues\n","    quality_issues = []\n","    for result in successful:\n","        if result['issues']:\n","            quality_issues.extend([(result['ticker'], issue) for issue in result['issues']])\n","\n","    if quality_issues:\n","        print(f\"\\n‚ö†Ô∏è Data Quality Issues:\")\n","        for ticker, issue in quality_issues:\n","            print(f\"   ‚ö†Ô∏è {ticker}: {issue}\")\n","\n","    return {'successful': len(successful), 'failed': len(failed), 'quality_issues': len(quality_issues)}\n","\n","def main_preprocessing():\n","    \"\"\"Main preprocessing function\"\"\"\n","    print(\"üöÄ STARTING CUSTOM PREPROCESSING BASED ON AUDIT\")\n","    print(\"=\"*80)\n","\n","    results = []\n","\n","    # Process Alpha Vantage files\n","    print(f\"\\nüìÇ Processing Alpha Vantage Files ({len(ALPHA_VANTAGE_FILES)} files)\")\n","    for file_path in ALPHA_VANTAGE_FILES:\n","        try:\n","            df = process_alpha_vantage_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ‚ùå Error processing {file_path}: {str(e)}\")\n","\n","    # Process corrupted YFinance files\n","    print(f\"\\nüìÇ Processing Corrupted YFinance Files ({len(YFINANCE_CORRUPTED_FILES)} files)\")\n","    for file_path in YFINANCE_CORRUPTED_FILES:\n","        try:\n","            df = process_yfinance_corrupted_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ‚ùå Error processing {file_path}: {str(e)}\")\n","\n","    # Process BBCA file\n","    print(f\"\\nüìÇ Processing BBCA File ({len(BBCA_FILES)} files)\")\n","    for file_path in BBCA_FILES:\n","        try:\n","            df = process_bbca_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ‚ùå Error processing {file_path}: {str(e)}\")\n","\n","    # Generate report\n","    report = generate_processing_report(results)\n","\n","    print(f\"\\nüéâ STAGE 1 PREPROCESSING COMPLETED!\")\n","    print(f\"üìÇ Clean data ready in: processed_data/ directory\")\n","    print(f\"üîú Ready for STAGE 2: Feature Engineering\")\n","\n","    return results, report\n","\n","# üöÄ RUN PREPROCESSING\n","if __name__ == \"__main__\":\n","    results, report = main_preprocessing()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StxG8YkHzNfC","executionInfo":{"status":"ok","timestamp":1753419318633,"user_tz":-420,"elapsed":619,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"0d45fc40-9f27-4876-efdb-23fe8d2f7e63"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STARTING CUSTOM PREPROCESSING BASED ON AUDIT\n","================================================================================\n","\n","üìÇ Processing Alpha Vantage Files (6 files)\n","üîß Processing Alpha Vantage: AAPL_al.csv\n","   ‚úÖ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/AAPL_processed.csv\n","üîß Processing Alpha Vantage: BAC_al.csv\n","   ‚úÖ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/BAC_processed.csv\n","üîß Processing Alpha Vantage: JPM_al.csv\n","   ‚úÖ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/JPM_processed.csv\n","üîß Processing Alpha Vantage: META_al.csv\n","   ‚úÖ Success: 3314 records, Date range: 2012-05-18 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/META_processed.csv\n","üîß Processing Alpha Vantage: NFLX_al.csv\n","   ‚úÖ Success: 5830 records, Date range: 2002-05-23 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/NFLX_processed.csv\n","üîß Processing Alpha Vantage: NVDA_al.csv\n","   ‚úÖ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/NVDA_processed.csv\n","\n","üìÇ Processing Corrupted YFinance Files (4 files)\n","üîß Processing Corrupted YFinance: AMZN_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'AMZN', 'High': 'AMZN', 'Low': 'AMZN', 'Close': 'AMZN', 'Volume': 'AMZN', 'Ticker': nan}\n","   üö® Detected corruption: removing header row\n","   ‚úÖ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/AMZN_processed.csv\n","üîß Processing Corrupted YFinance: GOOGL_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'GOOGL', 'High': 'GOOGL', 'Low': 'GOOGL', 'Close': 'GOOGL', 'Volume': 'GOOGL', 'Ticker': nan}\n","   üö® Detected corruption: removing header row\n","   ‚úÖ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/GOOGL_processed.csv\n","üîß Processing Corrupted YFinance: MSFT_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'MSFT', 'High': 'MSFT', 'Low': 'MSFT', 'Close': 'MSFT', 'Volume': 'MSFT', 'Ticker': nan}\n","   üö® Detected corruption: removing header row\n","   ‚úÖ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/MSFT_processed.csv\n","üîß Processing Corrupted YFinance: TSLA_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'TSLA', 'High': 'TSLA', 'Low': 'TSLA', 'Close': 'TSLA', 'Volume': 'TSLA', 'Ticker': nan}\n","   üö® Detected corruption: removing header row\n","   ‚úÖ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   üíæ Saved: processed_data/TSLA_processed.csv\n","\n","üìÇ Processing BBCA File (1 files)\n","üîß Processing BBCA: BBCA_yf.csv\n","   Original shape: (2594, 6)\n","   Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n","   First row: {'Price': 'Ticker', 'Close': 'BBCA.JK', 'High': 'BBCA.JK', 'Low': 'BBCA.JK', 'Open': 'BBCA.JK', 'Volume': 'BBCA.JK'}\n","   üö® Detected corruption: removing header row\n","   ‚ö†Ô∏è No date column found, created synthetic dates\n","   ‚úÖ Processed: 2592 records\n","   üíæ Saved: processed_data/BBCA_processed.csv\n","\n","================================================================================\n","üìã STAGE 1 PROCESSING REPORT\n","================================================================================\n","‚úÖ Successfully processed: 11/11 files\n","‚ùå Failed: 0/11 files\n","\n","üìä Successful Files:\n","   ‚úÖ AAPL: 6471 records (1999-11-01 to 2025-07-24)\n","   ‚úÖ BAC: 6471 records (1999-11-01 to 2025-07-24)\n","   ‚úÖ JPM: 6471 records (1999-11-01 to 2025-07-24)\n","   ‚úÖ META: 3314 records (2012-05-18 to 2025-07-24)\n","   ‚úÖ NFLX: 5830 records (2002-05-23 to 2025-07-24)\n","   ‚úÖ NVDA: 6471 records (1999-11-01 to 2025-07-24)\n","   ‚úÖ AMZN: 2514 records (2015-07-27 to 2025-07-24)\n","   ‚úÖ GOOGL: 2514 records (2015-07-27 to 2025-07-24)\n","   ‚úÖ MSFT: 2514 records (2015-07-27 to 2025-07-24)\n","   ‚úÖ TSLA: 2514 records (2015-07-27 to 2025-07-24)\n","   ‚úÖ BBCA: 2592 records (2014-01-01 to 2021-02-04)\n","\n","üéâ STAGE 1 PREPROCESSING COMPLETED!\n","üìÇ Clean data ready in: processed_data/ directory\n","üîú Ready for STAGE 2: Feature Engineering\n"]}]},{"cell_type":"markdown","source":["\n","# Stage 2 : Feature Engineering"],"metadata":{"id":"RddhKbcP1eog"}},{"cell_type":"code","source":["!pip install ta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhFNyKtV191h","executionInfo":{"status":"ok","timestamp":1753419339106,"user_tz":-420,"elapsed":7554,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"c15ae67d-c1f5-4905-d5cb-9518a5509e5e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ta\n","  Downloading ta-0.11.0.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n","Building wheels for collected packages: ta\n","  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=8639749a9e3b1d6f88f7cc9bd328505902575a07e4db28393f1a3e1a4887946b\n","  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n","Successfully built ta\n","Installing collected packages: ta\n","Successfully installed ta-0.11.0\n"]}]},{"cell_type":"code","source":["# ===================================================================\n","# üß™ STAGE 2: FEATURE ENGINEERING - FIXED VERSION\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import ta\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def load_processed_data(processed_dir='processed_data'):\n","    \"\"\"Load all processed data from Stage 1\"\"\"\n","    print(\"üìÇ Loading processed data from Stage 1...\")\n","\n","    datasets = {}\n","    processed_files = [f for f in os.listdir(processed_dir) if f.endswith('_processed.csv')]\n","\n","    for file in processed_files:\n","        ticker = file.replace('_processed.csv', '')\n","        file_path = os.path.join(processed_dir, file)\n","\n","        try:\n","            df = pd.read_csv(file_path)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df = df.sort_values('Date').reset_index(drop=True)\n","            datasets[ticker] = df\n","            print(f\"   ‚úÖ Loaded {ticker}: {len(df)} records\")\n","        except Exception as e:\n","            print(f\"   ‚ùå Error loading {ticker}: {str(e)}\")\n","\n","    print(f\"üìä Total datasets loaded: {len(datasets)}\")\n","    return datasets\n","\n","def add_moving_averages(df):\n","    \"\"\"Add Simple and Exponential Moving Averages\"\"\"\n","    print(\"   üìà Adding Moving Averages...\")\n","\n","    # Simple Moving Averages\n","    df['SMA_5'] = ta.trend.sma_indicator(df['Close'], window=5)\n","    df['SMA_10'] = ta.trend.sma_indicator(df['Close'], window=10)\n","    df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n","    df['SMA_50'] = ta.trend.sma_indicator(df['Close'], window=50)\n","\n","    # Exponential Moving Averages\n","    df['EMA_5'] = ta.trend.ema_indicator(df['Close'], window=5)\n","    df['EMA_10'] = ta.trend.ema_indicator(df['Close'], window=10)\n","    df['EMA_20'] = ta.trend.ema_indicator(df['Close'], window=20)\n","    df['EMA_50'] = ta.trend.ema_indicator(df['Close'], window=50)\n","\n","    # Moving Average Crossovers\n","    df['SMA_5_10_Cross'] = (df['SMA_5'] > df['SMA_10']).astype(int)\n","    df['EMA_5_20_Cross'] = (df['EMA_5'] > df['EMA_20']).astype(int)\n","\n","    print(f\"      ‚úÖ Added 10 Moving Average indicators\")\n","    return df\n","\n","def add_rsi_indicators(df):\n","    \"\"\"Add RSI (Relative Strength Index) indicators\"\"\"\n","    print(\"   üìä Adding RSI indicators...\")\n","\n","    # Standard RSI\n","    df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n","    df['RSI_7'] = ta.momentum.rsi(df['Close'], window=7)\n","    df['RSI_21'] = ta.momentum.rsi(df['Close'], window=21)\n","\n","    # RSI signals\n","    df['RSI_Overbought'] = (df['RSI_14'] > 70).astype(int)\n","    df['RSI_Oversold'] = (df['RSI_14'] < 30).astype(int)\n","    df['RSI_Neutral'] = ((df['RSI_14'] >= 30) & (df['RSI_14'] <= 70)).astype(int)\n","\n","    print(f\"      ‚úÖ Added 6 RSI indicators\")\n","    return df\n","\n","def add_macd_indicators(df):\n","    \"\"\"Add MACD (Moving Average Convergence Divergence) indicators\"\"\"\n","    print(\"   üìâ Adding MACD indicators...\")\n","\n","    # MACD components\n","    df['MACD'] = ta.trend.macd(df['Close'])\n","    df['MACD_Signal'] = ta.trend.macd_signal(df['Close'])\n","    df['MACD_Histogram'] = ta.trend.macd_diff(df['Close'])\n","\n","    # MACD signals\n","    df['MACD_Bullish'] = (df['MACD'] > df['MACD_Signal']).astype(int)\n","    df['MACD_Bearish'] = (df['MACD'] < df['MACD_Signal']).astype(int)\n","\n","    print(f\"      ‚úÖ Added 5 MACD indicators\")\n","    return df\n","\n","def add_bollinger_bands(df):\n","    \"\"\"Add Bollinger Bands indicators\"\"\"\n","    print(\"   üìè Adding Bollinger Bands...\")\n","\n","    # Bollinger Bands\n","    df['BB_High'] = ta.volatility.bollinger_hband(df['Close'])\n","    df['BB_Low'] = ta.volatility.bollinger_lband(df['Close'])\n","    df['BB_Mid'] = ta.volatility.bollinger_mavg(df['Close'])\n","    df['BB_Width'] = df['BB_High'] - df['BB_Low']\n","    df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","\n","    # Bollinger Bands signals\n","    df['BB_Squeeze'] = (df['BB_Width'] < df['BB_Width'].rolling(20).mean()).astype(int)\n","    df['BB_Upper_Break'] = (df['Close'] > df['BB_High']).astype(int)\n","    df['BB_Lower_Break'] = (df['Close'] < df['BB_Low']).astype(int)\n","\n","    print(f\"      ‚úÖ Added 8 Bollinger Bands indicators\")\n","    return df\n","\n","def add_volume_indicators(df):\n","    \"\"\"Add Volume-based indicators - FIXED VERSION\"\"\"\n","    print(\"   üìä Adding Volume indicators...\")\n","\n","    # Custom Volume Moving Averages (since ta.volume.volume_sma doesn't exist)\n","    df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n","    df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean()\n","\n","    # Volume Weighted Average Price (manual calculation)\n","    df['VWAP'] = (df['Close'] * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()\n","\n","    # On Balance Volume\n","    df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n","\n","    # Volume Price Trend\n","    df['VPT'] = ta.volume.volume_price_trend(df['Close'], df['Volume'])\n","\n","    # Accumulation/Distribution Line\n","    df['ADL'] = ta.volume.acc_dist_index(df['High'], df['Low'], df['Close'], df['Volume'])\n","\n","    # Volume signals\n","    df['High_Volume'] = (df['Volume'] > df['Volume_MA_20'] * 1.5).astype(int)\n","    df['Low_Volume'] = (df['Volume'] < df['Volume_MA_20'] * 0.5).astype(int)\n","\n","    print(f\"      ‚úÖ Added 8 Volume indicators\")\n","    return df\n","\n","def add_momentum_indicators(df):\n","    \"\"\"Add Momentum indicators\"\"\"\n","    print(\"   üöÄ Adding Momentum indicators...\")\n","\n","    # Rate of Change\n","    df['ROC_5'] = ta.momentum.roc(df['Close'], window=5)\n","    df['ROC_10'] = ta.momentum.roc(df['Close'], window=10)\n","    df['ROC_20'] = ta.momentum.roc(df['Close'], window=20)\n","\n","    # Stochastic Oscillator\n","    df['Stoch_K'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n","    df['Stoch_D'] = ta.momentum.stoch_signal(df['High'], df['Low'], df['Close'])\n","\n","    # Williams %R\n","    df['Williams_R'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'])\n","\n","    # Momentum signals\n","    df['Strong_Momentum'] = (df['ROC_10'] > 2).astype(int)\n","    df['Weak_Momentum'] = (df['ROC_10'] < -2).astype(int)\n","\n","    print(f\"      ‚úÖ Added 8 Momentum indicators\")\n","    return df\n","\n","def add_volatility_indicators(df):\n","    \"\"\"Add Volatility indicators\"\"\"\n","    print(\"   üìä Adding Volatility indicators...\")\n","\n","    # Average True Range\n","    df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n","\n","    # Volatility (Rolling Standard Deviation)\n","    df['Volatility_10'] = df['Close'].rolling(window=10).std()\n","    df['Volatility_20'] = df['Close'].rolling(window=20).std()\n","\n","    # Price Range indicators\n","    df['Daily_Range'] = df['High'] - df['Low']\n","    df['Daily_Range_Pct'] = (df['Daily_Range'] / df['Close']) * 100\n","\n","    # Volatility signals\n","    df['High_Volatility'] = (df['Volatility_20'] > df['Volatility_20'].rolling(50).mean() * 1.2).astype(int)\n","    df['Low_Volatility'] = (df['Volatility_20'] < df['Volatility_20'].rolling(50).mean() * 0.8).astype(int)\n","\n","    print(f\"      ‚úÖ Added 7 Volatility indicators\")\n","    return df\n","\n","def add_price_patterns(df):\n","    \"\"\"Add Price Pattern indicators\"\"\"\n","    print(\"   üìà Adding Price Pattern indicators...\")\n","\n","    # Daily Returns\n","    df['Daily_Return'] = df['Close'].pct_change()\n","    df['Daily_Return_Abs'] = abs(df['Daily_Return'])\n","\n","    # Gap indicators\n","    df['Gap_Up'] = (df['Open'] > df['Close'].shift(1)).astype(int)\n","    df['Gap_Down'] = (df['Open'] < df['Close'].shift(1)).astype(int)\n","\n","    # Doji pattern (Open ‚âà Close)\n","    df['Doji'] = (abs(df['Open'] - df['Close']) / df['Close'] < 0.01).astype(int)\n","\n","    # Hammer/Shooting Star patterns\n","    body_size = abs(df['Close'] - df['Open'])\n","    upper_shadow = df['High'] - np.maximum(df['Open'], df['Close'])\n","    lower_shadow = np.minimum(df['Open'], df['Close']) - df['Low']\n","\n","    df['Hammer'] = ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)\n","    df['Shooting_Star'] = ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)\n","\n","    # Support/Resistance levels\n","    df['Near_High_52w'] = (df['Close'] > df['Close'].rolling(252).max() * 0.95).astype(int)\n","    df['Near_Low_52w'] = (df['Close'] < df['Close'].rolling(252).min() * 1.05).astype(int)\n","\n","    print(f\"      ‚úÖ Added 10 Price Pattern indicators\")\n","    return df\n","\n","def add_advanced_indicators(df):\n","    \"\"\"Add Advanced technical indicators\"\"\"\n","    print(\"   üéØ Adding Advanced indicators...\")\n","\n","    # Fibonacci Retracement levels\n","    high_252 = df['High'].rolling(252).max()\n","    low_252 = df['Low'].rolling(252).min()\n","    fib_range = high_252 - low_252\n","\n","    df['Fib_23_6'] = high_252 - (fib_range * 0.236)\n","    df['Fib_38_2'] = high_252 - (fib_range * 0.382)\n","    df['Fib_61_8'] = high_252 - (fib_range * 0.618)\n","\n","    # Ichimoku Cloud components\n","    high_9 = df['High'].rolling(9).max()\n","    low_9 = df['Low'].rolling(9).min()\n","    high_26 = df['High'].rolling(26).max()\n","    low_26 = df['Low'].rolling(26).min()\n","\n","    df['Tenkan_Sen'] = (high_9 + low_9) / 2\n","    df['Kijun_Sen'] = (high_26 + low_26) / 2\n","    df['Senkou_A'] = ((df['Tenkan_Sen'] + df['Kijun_Sen']) / 2).shift(26)\n","\n","    # Commodity Channel Index\n","    df['CCI'] = ta.trend.cci(df['High'], df['Low'], df['Close'])\n","\n","    print(f\"      ‚úÖ Added 9 Advanced indicators\")\n","    return df\n","\n","def create_target_labels(df):\n","    \"\"\"Create target labels for Random Forest (Stage 4)\"\"\"\n","    print(\"   üéØ Creating target labels...\")\n","\n","    # Next day price changes\n","    df['Next_Open'] = df['Open'].shift(-1)\n","    df['Next_Close'] = df['Close'].shift(-1)\n","    df['Next_High'] = df['High'].shift(-1)\n","    df['Next_Low'] = df['Low'].shift(-1)\n","\n","    # Price change percentages\n","    df['Price_Change_Pct'] = ((df['Next_Close'] - df['Close']) / df['Close']) * 100\n","\n","    # Trend labels (sesuai arsitektur.md)\n","    def classify_trend(price_change):\n","        if pd.isna(price_change):\n","            return 'UNKNOWN'\n","        elif price_change > 1.0:  # >1% increase\n","            return 'UP'\n","        elif price_change < -1.0:  # >1% decrease\n","            return 'DOWN'\n","        else:\n","            return 'STAY'\n","\n","    df['Trend_Label'] = df['Price_Change_Pct'].apply(classify_trend)\n","\n","    # 5-day ahead targets (for LSTM)\n","    for i in range(1, 6):  # 1 to 5 days ahead\n","        df[f'Target_Open_Day{i}'] = df['Open'].shift(-i)\n","        df[f'Target_Close_Day{i}'] = df['Close'].shift(-i)\n","\n","    print(f\"      ‚úÖ Added target labels for prediction\")\n","    return df\n","\n","def validate_features(df, ticker):\n","    \"\"\"Validate feature engineering results\"\"\"\n","    print(f\"   üîç Validating features for {ticker}...\")\n","\n","    issues = []\n","\n","    # Check for infinite values\n","    inf_cols = df.columns[df.isin([np.inf, -np.inf]).any()].tolist()\n","    if inf_cols:\n","        issues.append(f\"Infinite values in: {inf_cols}\")\n","        # Replace inf with NaN\n","        df = df.replace([np.inf, -np.inf], np.nan)\n","\n","    # Check percentage of missing values\n","    missing_pct = (df.isnull().sum() / len(df)) * 100\n","    high_missing = missing_pct[missing_pct > 50].index.tolist()\n","    if high_missing:\n","        issues.append(f\"High missing data (>50%): {high_missing}\")\n","\n","    # Check feature count\n","    feature_cols = [col for col in df.columns if col not in ['Date', 'Ticker']]\n","    print(f\"      üìä Total features created: {len(feature_cols)}\")\n","\n","    # Check data range\n","    if len(df) < 100:\n","        issues.append(f\"Insufficient data: {len(df)} records\")\n","\n","    if issues:\n","        print(f\"      ‚ö†Ô∏è Validation issues: {issues}\")\n","    else:\n","        print(f\"      ‚úÖ Validation passed\")\n","\n","    return df, issues\n","\n","def save_feature_engineered_data(df, ticker, output_dir='feature_engineered_data'):\n","    \"\"\"Save feature engineered data\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    filename = f\"{output_dir}/{ticker}_fe.csv\"\n","    df.to_csv(filename, index=False)\n","\n","    print(f\"   üíæ Saved: {filename}\")\n","    return filename\n","\n","def generate_feature_report(processed_results):\n","    \"\"\"Generate comprehensive feature engineering report - FIXED VERSION\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"üìã STAGE 2 FEATURE ENGINEERING REPORT\")\n","    print(\"=\"*80)\n","\n","    successful = [r for r in processed_results if r['status'] == 'SUCCESS']\n","    failed = [r for r in processed_results if r['status'] != 'SUCCESS']\n","\n","    print(f\"‚úÖ Successfully processed: {len(successful)}/11 files\")\n","    print(f\"‚ùå Failed: {len(failed)}/11 files\")\n","\n","    # Initialize total_features\n","    total_features = 0\n","\n","    if successful:\n","        print(f\"\\nüìä Feature Engineering Summary:\")\n","        total_features = successful[0]['feature_count'] if successful else 0\n","        for result in successful:\n","            print(f\"   ‚úÖ {result['ticker']}: {result['feature_count']} features, {result['records']} records\")\n","\n","        print(f\"\\nüéØ Average features per dataset: {total_features}\")\n","\n","        # Quality issues summary\n","        all_issues = []\n","        for result in successful:\n","            all_issues.extend(result['issues'])\n","\n","        if all_issues:\n","            print(f\"\\n‚ö†Ô∏è Data Quality Issues ({len(all_issues)} total):\")\n","            issue_counts = {}\n","            for issue in all_issues:\n","                issue_type = issue.split(':')[0]\n","                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1\n","\n","            for issue_type, count in issue_counts.items():\n","                print(f\"   ‚ö†Ô∏è {issue_type}: {count} occurrences\")\n","\n","    if failed:\n","        print(f\"\\n‚ùå Failed Files:\")\n","        for result in failed:\n","            print(f\"   ‚ùå {result['ticker']}: {result['error']}\")\n","\n","    return {'successful': len(successful), 'failed': len(failed), 'avg_features': total_features}\n","\n","def main_feature_engineering():\n","    \"\"\"Main feature engineering execution\"\"\"\n","    print(\"üß™ STARTING STAGE 2: FEATURE ENGINEERING - FIXED VERSION\")\n","    print(\"=\"*80)\n","\n","    # Load processed data from Stage 1\n","    datasets = load_processed_data()\n","\n","    if not datasets:\n","        print(\"‚ùå No processed data found! Run Stage 1 first.\")\n","        return None, None\n","\n","    results = []\n","\n","    for ticker, df in datasets.items():\n","        print(f\"\\n{'='*60}\")\n","        print(f\"üî¨ Feature Engineering: {ticker}\")\n","        print(f\"{'='*60}\")\n","        print(f\"üìä Input: {len(df)} records, {len(df.columns)} columns\")\n","\n","        try:\n","            # Apply all feature engineering functions\n","            df = add_moving_averages(df)\n","            df = add_rsi_indicators(df)\n","            df = add_macd_indicators(df)\n","            df = add_bollinger_bands(df)\n","            df = add_volume_indicators(df)  # FIXED VERSION\n","            df = add_momentum_indicators(df)\n","            df = add_volatility_indicators(df)\n","            df = add_price_patterns(df)\n","            df = add_advanced_indicators(df)\n","            df = create_target_labels(df)\n","\n","            # Validate results\n","            df, issues = validate_features(df, ticker)\n","\n","            # Save feature engineered data\n","            saved_file = save_feature_engineered_data(df, ticker)\n","\n","            feature_count = len([col for col in df.columns if col not in ['Date', 'Ticker']])\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'feature_count': feature_count,\n","                'issues': issues,\n","                'saved_file': saved_file\n","            })\n","\n","            print(f\"‚úÖ {ticker} feature engineering completed!\")\n","            print(f\"   üìä Output: {len(df)} records, {feature_count} features\")\n","\n","        except Exception as e:\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'feature_count': 0,\n","                'issues': [],\n","                'saved_file': None\n","            })\n","            print(f\"‚ùå Error processing {ticker}: {str(e)}\")\n","\n","    # Generate comprehensive report\n","    report = generate_feature_report(results)\n","\n","    print(f\"\\nüéâ STAGE 2 FEATURE ENGINEERING COMPLETED!\")\n","    print(f\"üìÇ Feature engineered data ready in: feature_engineered_data/ directory\")\n","    print(f\"üîú Ready for STAGE 3: LSTM Model Training\")\n","\n","    return results, report\n","\n","# üöÄ RUN FEATURE ENGINEERING\n","if __name__ == \"__main__\":\n","    results, report = main_feature_engineering()\n","\n","print(\"\\nüéØ STAGE 2 EXECUTION COMPLETED!\")\n","print(\"üìù Next: STAGE 3 - LSTM Model Training\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_YsbOZV1k3Y","executionInfo":{"status":"ok","timestamp":1753419361516,"user_tz":-420,"elapsed":12494,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"4b8d4961-98a8-42cc-999b-0644824b2bb7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üß™ STARTING STAGE 2: FEATURE ENGINEERING - FIXED VERSION\n","================================================================================\n","üìÇ Loading processed data from Stage 1...\n","   ‚úÖ Loaded NFLX: 5830 records\n","   ‚úÖ Loaded AMZN: 2514 records\n","   ‚úÖ Loaded JPM: 6471 records\n","   ‚úÖ Loaded BAC: 6471 records\n","   ‚úÖ Loaded AAPL: 6471 records\n","   ‚úÖ Loaded GOOGL: 2514 records\n","   ‚úÖ Loaded BBCA: 2592 records\n","   ‚úÖ Loaded MSFT: 2514 records\n","   ‚úÖ Loaded NVDA: 6471 records\n","   ‚úÖ Loaded META: 3314 records\n","   ‚úÖ Loaded TSLA: 2514 records\n","üìä Total datasets loaded: 11\n","\n","============================================================\n","üî¨ Feature Engineering: NFLX\n","============================================================\n","üìä Input: 5830 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for NFLX...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/NFLX_fe.csv\n","‚úÖ NFLX feature engineering completed!\n","   üìä Output: 5830 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: AMZN\n","============================================================\n","üìä Input: 2514 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for AMZN...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/AMZN_fe.csv\n","‚úÖ AMZN feature engineering completed!\n","   üìä Output: 2514 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: JPM\n","============================================================\n","üìä Input: 6471 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for JPM...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/JPM_fe.csv\n","‚úÖ JPM feature engineering completed!\n","   üìä Output: 6471 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: BAC\n","============================================================\n","üìä Input: 6471 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for BAC...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/BAC_fe.csv\n","‚úÖ BAC feature engineering completed!\n","   üìä Output: 6471 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: AAPL\n","============================================================\n","üìä Input: 6471 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for AAPL...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/AAPL_fe.csv\n","‚úÖ AAPL feature engineering completed!\n","   üìä Output: 6471 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: GOOGL\n","============================================================\n","üìä Input: 2514 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for GOOGL...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/GOOGL_fe.csv\n","‚úÖ GOOGL feature engineering completed!\n","   üìä Output: 2514 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: BBCA\n","============================================================\n","üìä Input: 2592 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for BBCA...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/BBCA_fe.csv\n","‚úÖ BBCA feature engineering completed!\n","   üìä Output: 2592 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: MSFT\n","============================================================\n","üìä Input: 2514 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for MSFT...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/MSFT_fe.csv\n","‚úÖ MSFT feature engineering completed!\n","   üìä Output: 2514 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: NVDA\n","============================================================\n","üìä Input: 6471 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for NVDA...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/NVDA_fe.csv\n","‚úÖ NVDA feature engineering completed!\n","   üìä Output: 6471 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: META\n","============================================================\n","üìä Input: 3314 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for META...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/META_fe.csv\n","‚úÖ META feature engineering completed!\n","   üìä Output: 3314 records, 89 features\n","\n","============================================================\n","üî¨ Feature Engineering: TSLA\n","============================================================\n","üìä Input: 2514 records, 7 columns\n","   üìà Adding Moving Averages...\n","      ‚úÖ Added 10 Moving Average indicators\n","   üìä Adding RSI indicators...\n","      ‚úÖ Added 6 RSI indicators\n","   üìâ Adding MACD indicators...\n","      ‚úÖ Added 5 MACD indicators\n","   üìè Adding Bollinger Bands...\n","      ‚úÖ Added 8 Bollinger Bands indicators\n","   üìä Adding Volume indicators...\n","      ‚úÖ Added 8 Volume indicators\n","   üöÄ Adding Momentum indicators...\n","      ‚úÖ Added 8 Momentum indicators\n","   üìä Adding Volatility indicators...\n","      ‚úÖ Added 7 Volatility indicators\n","   üìà Adding Price Pattern indicators...\n","      ‚úÖ Added 10 Price Pattern indicators\n","   üéØ Adding Advanced indicators...\n","      ‚úÖ Added 9 Advanced indicators\n","   üéØ Creating target labels...\n","      ‚úÖ Added target labels for prediction\n","   üîç Validating features for TSLA...\n","      üìä Total features created: 89\n","      ‚úÖ Validation passed\n","   üíæ Saved: feature_engineered_data/TSLA_fe.csv\n","‚úÖ TSLA feature engineering completed!\n","   üìä Output: 2514 records, 89 features\n","\n","================================================================================\n","üìã STAGE 2 FEATURE ENGINEERING REPORT\n","================================================================================\n","‚úÖ Successfully processed: 11/11 files\n","‚ùå Failed: 0/11 files\n","\n","üìä Feature Engineering Summary:\n","   ‚úÖ NFLX: 89 features, 5830 records\n","   ‚úÖ AMZN: 89 features, 2514 records\n","   ‚úÖ JPM: 89 features, 6471 records\n","   ‚úÖ BAC: 89 features, 6471 records\n","   ‚úÖ AAPL: 89 features, 6471 records\n","   ‚úÖ GOOGL: 89 features, 2514 records\n","   ‚úÖ BBCA: 89 features, 2592 records\n","   ‚úÖ MSFT: 89 features, 2514 records\n","   ‚úÖ NVDA: 89 features, 6471 records\n","   ‚úÖ META: 89 features, 3314 records\n","   ‚úÖ TSLA: 89 features, 2514 records\n","\n","üéØ Average features per dataset: 89\n","\n","üéâ STAGE 2 FEATURE ENGINEERING COMPLETED!\n","üìÇ Feature engineered data ready in: feature_engineered_data/ directory\n","üîú Ready for STAGE 3: LSTM Model Training\n","\n","üéØ STAGE 2 EXECUTION COMPLETED!\n","üìù Next: STAGE 3 - LSTM Model Training\n"]}]},{"cell_type":"markdown","source":["# Stage 3 : Training Data Model LSTM"],"metadata":{"id":"6uTpl9Pg6aO_"}},{"cell_type":"code","source":["# ===================================================================\n","# üß† STAGE 3: GPU-OPTIMIZED UNIFIED LSTM TRAINING (T4 15GB)\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import gc\n","import warnings\n","import pickle\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# TensorFlow with GPU optimization\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.mixed_precision import set_global_policy\n","\n","warnings.filterwarnings('ignore')\n","\n","def setup_gpu_optimization():\n","    \"\"\"Setup GPU optimization for T4\"\"\"\n","    print(\"üöÄ Setting up GPU optimization for T4...\")\n","\n","    # Enable mixed precision for better performance\n","    set_global_policy('mixed_float16')\n","\n","    # Configure GPU memory growth\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            print(f\"   ‚úÖ Found {len(gpus)} GPU(s), memory growth enabled\")\n","        except RuntimeError as e:\n","            print(f\"   ‚ö†Ô∏è GPU setup error: {e}\")\n","    else:\n","        print(\"   ‚ö†Ô∏è No GPU found, using CPU\")\n","\n","    # Clear session and force garbage collection\n","    tf.keras.backend.clear_session()\n","    gc.collect()\n","\n","    return gpus is not None and len(gpus) > 0\n","\n","def load_optimized_datasets(fe_dir='feature_engineered_data', sample_ratio=0.8):\n","    \"\"\"Load datasets with memory optimization\"\"\"\n","    print(f\"üìÇ Loading datasets with optimization (sample_ratio={sample_ratio})...\")\n","\n","    fe_files = [f for f in os.listdir(fe_dir) if f.endswith('_fe.csv')]\n","    datasets = []\n","\n","    for file in fe_files:\n","        ticker = file.replace('_fe.csv', '')\n","        file_path = os.path.join(fe_dir, file)\n","\n","        try:\n","            # Load with optimized dtypes\n","            df = pd.read_csv(file_path, low_memory=False)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df['Ticker'] = ticker\n","\n","            # Sample data to reduce memory usage\n","            if sample_ratio < 1.0:\n","                sample_size = int(len(df) * sample_ratio)\n","                df = df.tail(sample_size)  # Take most recent data\n","\n","            # Optimize dtypes to save memory\n","            for col in df.select_dtypes(include=['float64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='float')\n","\n","            for col in df.select_dtypes(include=['int64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='integer')\n","\n","            datasets.append(df)\n","            print(f\"   ‚úÖ {ticker}: {len(df)} records, {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n","\n","        except Exception as e:\n","            print(f\"   ‚ùå Error loading {ticker}: {str(e)}\")\n","\n","    # Combine datasets\n","    combined_df = pd.concat(datasets, ignore_index=True)\n","    combined_df = combined_df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n","\n","    # Final memory optimization\n","    gc.collect()\n","\n","    print(f\"‚úÖ Combined dataset: {len(combined_df)} records, {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n","    return combined_df\n","\n","def select_essential_features(df):\n","    \"\"\"Select only essential features to reduce memory\"\"\"\n","    print(\"üéØ Selecting essential features for GPU training...\")\n","\n","    # Core essential features (most predictive)\n","    essential_features = [\n","        # Price data\n","        'Open', 'High', 'Low', 'Close', 'Volume',\n","\n","        # Key moving averages\n","        'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20',\n","\n","        # Critical momentum indicators\n","        'RSI_14', 'MACD', 'MACD_Signal',\n","\n","        # Bollinger Bands\n","        'BB_Position', 'BB_Width',\n","\n","        # Volume indicators\n","        'Volume_MA_10', 'OBV',\n","\n","        # Volatility\n","        'ATR', 'Volatility_20',\n","\n","        # Price patterns\n","        'Daily_Return', 'ROC_10'\n","    ]\n","\n","    # Check which features exist\n","    existing_features = [col for col in essential_features if col in df.columns]\n","    print(f\"   ‚úÖ Selected {len(existing_features)} essential features\")\n","    print(f\"   üìã Features: {existing_features}\")\n","\n","    return existing_features\n","\n","def create_memory_efficient_sequences(df, features, sequence_length=60, batch_size=1000):\n","    \"\"\"Create sequences in batches to manage memory\"\"\"\n","    print(f\"üì¶ Creating sequences in batches (batch_size={batch_size})...\")\n","\n","    # Encode ticker\n","    label_encoder = LabelEncoder()\n","    df['Ticker_Encoded'] = label_encoder.fit_transform(df['Ticker'])\n","\n","    final_features = features + ['Ticker_Encoded']\n","\n","    # Handle missing values efficiently\n","    df[final_features] = df[final_features].fillna(method='ffill').fillna(method='bfill')\n","\n","    # Create target columns if not exist\n","    target_cols = []\n","    for day in range(1, 6):\n","        open_col = f'Target_Open_Day{day}'\n","        close_col = f'Target_Close_Day{day}'\n","\n","        if open_col not in df.columns:\n","            df[open_col] = df.groupby('Ticker')['Open'].shift(-day)\n","        if close_col not in df.columns:\n","            df[close_col] = df.groupby('Ticker')['Close'].shift(-day)\n","\n","        target_cols.extend([open_col, close_col])\n","\n","    # Create sequences by ticker to maintain chronological order\n","    all_X = []\n","    all_y = []\n","\n","    for ticker in df['Ticker'].unique():\n","        ticker_data = df[df['Ticker'] == ticker].copy()\n","        ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n","\n","        # Create sequences for this ticker\n","        ticker_X = []\n","        ticker_y = []\n","\n","        for i in range(sequence_length, len(ticker_data) - 5):\n","            # Input sequence\n","            x_seq = ticker_data[final_features].iloc[i-sequence_length:i].values\n","\n","            # Target\n","            y_seq = ticker_data[target_cols].iloc[i].values\n","\n","            if not np.isnan(y_seq).any():\n","                ticker_X.append(x_seq)\n","                ticker_y.append(y_seq)\n","\n","        if ticker_X:\n","            all_X.extend(ticker_X)\n","            all_y.extend(ticker_y)\n","\n","        print(f\"   {ticker}: {len(ticker_X)} sequences\")\n","\n","    X = np.array(all_X, dtype=np.float32)  # Use float32 to save memory\n","    y = np.array(all_y, dtype=np.float32)\n","\n","    print(f\"   ‚úÖ Total sequences: {len(X)}\")\n","    print(f\"   üìä X shape: {X.shape}, Y shape: {y.shape}\")\n","    print(f\"   üíæ Memory usage: {X.nbytes / 1024**2:.1f} MB (X) + {y.nbytes / 1024**2:.1f} MB (y)\")\n","\n","    return X, y, label_encoder, final_features, target_cols\n","\n","def build_gpu_optimized_model(input_shape, output_dim=10):\n","    \"\"\"Build GPU-optimized LSTM model\"\"\"\n","    print(f\"üèóÔ∏è Building GPU-optimized LSTM model...\")\n","\n","    model = Sequential([\n","        # Optimized LSTM layers for T4\n","        LSTM(96, return_sequences=True, input_shape=input_shape,\n","             dtype='float32', recurrent_dropout=0.1),\n","        Dropout(0.3),\n","\n","        LSTM(48, return_sequences=False, dtype='float32', recurrent_dropout=0.1),\n","        Dropout(0.2),\n","\n","        # Dense layers\n","        Dense(24, activation='relu', dtype='float32'),\n","        Dropout(0.2),\n","        Dense(output_dim, activation='linear', dtype='float32')\n","    ])\n","\n","    # Optimizer with mixed precision\n","    optimizer = Adam(learning_rate=0.001)\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss='mse',\n","        metrics=['mae']\n","    )\n","\n","    print(f\"   ‚úÖ Model built: {model.count_params():,} parameters\")\n","    return model\n","\n","def train_with_memory_management(model, X, y, validation_split=0.2, epochs=30, batch_size=32):\n","    \"\"\"Train model with memory management\"\"\"\n","    print(f\"üéØ Training with memory management...\")\n","\n","    # Split data\n","    split_idx = int(len(X) * (1 - validation_split))\n","    X_train, X_val = X[:split_idx], X[split_idx:]\n","    y_train, y_val = y[:split_idx], y[split_idx:]\n","\n","    print(f\"   üìä Train: {len(X_train)}, Val: {len(X_val)}\")\n","\n","    # Callbacks\n","    callbacks = [\n","        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n","        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6)\n","    ]\n","\n","    # Train with generator to save memory\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        callbacks=callbacks,\n","        verbose=1,\n","        shuffle=False\n","    )\n","\n","    # Clean up\n","    del X_train, X_val, y_train, y_val\n","    gc.collect()\n","\n","    return history\n","\n","def evaluate_and_save_model(model, X_test, y_test, feature_scaler, target_scaler,\n","                           ticker_encoder, feature_cols, target_cols):\n","    \"\"\"Evaluate and save the model\"\"\"\n","    print(f\"üìä Evaluating model...\")\n","\n","    # Predictions\n","    y_pred_scaled = model.predict(X_test, batch_size=64)\n","\n","    # Inverse transform\n","    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n","    y_actual = target_scaler.inverse_transform(y_test)\n","\n","    # Metrics\n","    mse = mean_squared_error(y_actual, y_pred)\n","    mae = mean_absolute_error(y_actual, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_actual, y_pred)\n","\n","    print(f\"   üìà Performance:\")\n","    print(f\"      RMSE: {rmse:.4f}\")\n","    print(f\"      MAE: {mae:.4f}\")\n","    print(f\"      R¬≤: {r2:.4f}\")\n","\n","    # Save model\n","    print(f\"üíæ Saving model...\")\n","    os.makedirs('unified_lstm_model', exist_ok=True)\n","\n","    model.save('unified_lstm_model/unified_lstm_model.h5')\n","\n","    with open('unified_lstm_model/feature_scaler.pkl', 'wb') as f:\n","        pickle.dump(feature_scaler, f)\n","\n","    with open('unified_lstm_model/target_scaler.pkl', 'wb') as f:\n","        pickle.dump(target_scaler, f)\n","\n","    with open('unified_lstm_model/ticker_encoder.pkl', 'wb') as f:\n","        pickle.dump(ticker_encoder, f)\n","\n","    model_info = {\n","        'feature_columns': feature_cols,\n","        'target_columns': target_cols,\n","        'performance': {'rmse': rmse, 'mae': mae, 'r2': r2}\n","    }\n","\n","    with open('unified_lstm_model/model_info.pkl', 'wb') as f:\n","        pickle.dump(model_info, f)\n","\n","    print(f\"   ‚úÖ Model saved to: unified_lstm_model/\")\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'r2': r2,\n","        'predictions': y_pred,\n","        'actual': y_actual\n","    }\n","\n","def main_gpu_optimized_training():\n","    \"\"\"Main GPU-optimized training function\"\"\"\n","    print(\"üöÄ STARTING GPU-OPTIMIZED UNIFIED LSTM TRAINING\")\n","    print(\"=\"*70)\n","\n","    try:\n","        # Setup GPU\n","        gpu_available = setup_gpu_optimization()\n","\n","        # Load data\n","        combined_df = load_optimized_datasets(sample_ratio=0.9)  # Use 90% of data\n","\n","        # Select features\n","        features = select_essential_features(combined_df)\n","\n","        # Create sequences\n","        X, y, ticker_encoder, feature_cols, target_cols = create_memory_efficient_sequences(\n","            combined_df, features, sequence_length=60\n","        )\n","\n","        # Scale data\n","        print(\"üìè Scaling data...\")\n","\n","        # Reshape for scaling\n","        original_shape = X.shape\n","        X_reshaped = X.reshape(-1, X.shape[-1])\n","\n","        feature_scaler = MinMaxScaler()\n","        X_scaled_reshaped = feature_scaler.fit_transform(X_reshaped)\n","        X_scaled = X_scaled_reshaped.reshape(original_shape).astype(np.float32)\n","\n","        target_scaler = MinMaxScaler()\n","        y_scaled = target_scaler.fit_transform(y).astype(np.float32)\n","\n","        # Clean up original data\n","        del X, y, X_reshaped, X_scaled_reshaped\n","        gc.collect()\n","\n","        # Split data\n","        test_size = 0.15\n","        split_idx = int(len(X_scaled) * (1 - test_size))\n","\n","        X_train_val = X_scaled[:split_idx]\n","        X_test = X_scaled[split_idx:]\n","        y_train_val = y_scaled[:split_idx]\n","        y_test = y_scaled[split_idx:]\n","\n","        print(f\"üìä Data split: Train+Val: {len(X_train_val)}, Test: {len(X_test)}\")\n","\n","        # Build model\n","        model = build_gpu_optimized_model(\n","            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n","            output_dim=len(target_cols)\n","        )\n","\n","        # Train model\n","        history = train_with_memory_management(\n","            model, X_train_val, y_train_val,\n","            validation_split=0.2, epochs=25, batch_size=64\n","        )\n","\n","        # Evaluate and save\n","        results = evaluate_and_save_model(\n","            model, X_test, y_test, feature_scaler, target_scaler,\n","            ticker_encoder, feature_cols, target_cols\n","        )\n","\n","        print(f\"\\nüéâ TRAINING COMPLETED SUCCESSFULLY!\")\n","        print(f\"üìä Final Performance: RMSE={results['rmse']:.4f}, R¬≤={results['r2']:.4f}\")\n","        print(f\"üîú Ready for STAGE 4: Random Forest Training\")\n","\n","        return {\n","            'status': 'SUCCESS',\n","            'performance': results,\n","            'gpu_used': gpu_available\n","        }\n","\n","    except Exception as e:\n","        print(f\"\\n‚ùå TRAINING FAILED: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        return {'status': 'ERROR', 'error': str(e)}\n","\n","# üöÄ RUN GPU-OPTIMIZED TRAINING\n","if __name__ == \"__main__\":\n","    result = main_gpu_optimized_training()\n","\n","print(\"\\n‚úÖ STAGE 3 EXECUTION COMPLETED!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTMQ1VdY9wRp","executionInfo":{"status":"ok","timestamp":1753424677851,"user_tz":-420,"elapsed":4864580,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"0cce899a-3e39-4c0f-d222-451e94978c56"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ STARTING GPU-OPTIMIZED UNIFIED LSTM TRAINING\n","======================================================================\n","üöÄ Setting up GPU optimization for T4...\n","   ‚úÖ Found 1 GPU(s), memory growth enabled\n","üìÇ Loading datasets with optimization (sample_ratio=0.9)...\n","   ‚úÖ NFLX: 5247 records, 2.2 MB\n","   ‚úÖ TSLA: 2262 records, 0.9 MB\n","   ‚úÖ AAPL: 5823 records, 2.4 MB\n","   ‚úÖ GOOGL: 2262 records, 0.9 MB\n","   ‚úÖ META: 2982 records, 1.2 MB\n","   ‚úÖ BBCA: 2332 records, 1.0 MB\n","   ‚úÖ NVDA: 5823 records, 2.4 MB\n","   ‚úÖ MSFT: 2262 records, 0.9 MB\n","   ‚úÖ BAC: 5823 records, 2.4 MB\n","   ‚úÖ JPM: 5823 records, 2.4 MB\n","   ‚úÖ AMZN: 2262 records, 0.9 MB\n","‚úÖ Combined dataset: 42901 records, 17.8 MB\n","üéØ Selecting essential features for GPU training...\n","   ‚úÖ Selected 20 essential features\n","   üìã Features: ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width', 'Volume_MA_10', 'OBV', 'ATR', 'Volatility_20', 'Daily_Return', 'ROC_10']\n","üì¶ Creating sequences in batches (batch_size=1000)...\n","   AAPL: 5758 sequences\n","   AMZN: 2197 sequences\n","   BAC: 5758 sequences\n","   BBCA: 2267 sequences\n","   GOOGL: 2197 sequences\n","   JPM: 5758 sequences\n","   META: 2917 sequences\n","   MSFT: 2197 sequences\n","   NFLX: 5182 sequences\n","   NVDA: 5758 sequences\n","   TSLA: 2197 sequences\n","   ‚úÖ Total sequences: 42186\n","   üìä X shape: (42186, 60, 21), Y shape: (42186, 10)\n","   üíæ Memory usage: 202.8 MB (X) + 1.6 MB (y)\n","üìè Scaling data...\n","üìä Data split: Train+Val: 35858, Test: 6328\n","üèóÔ∏è Building GPU-optimized LSTM model...\n","   ‚úÖ Model built: 74,578 parameters\n","üéØ Training with memory management...\n","   üìä Train: 28686, Val: 7172\n","Epoch 1/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 330ms/step - loss: 0.0074 - mae: 0.0299 - val_loss: 7.3783e-04 - val_mae: 0.0234 - learning_rate: 0.0010\n","Epoch 2/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 333ms/step - loss: 0.0056 - mae: 0.0315 - val_loss: 7.1267e-04 - val_mae: 0.0230 - learning_rate: 0.0010\n","Epoch 3/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 334ms/step - loss: 0.0065 - mae: 0.0361 - val_loss: 7.7334e-04 - val_mae: 0.0243 - learning_rate: 0.0010\n","Epoch 4/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 333ms/step - loss: 0.0031 - mae: 0.0326 - val_loss: 4.7924e-04 - val_mae: 0.0191 - learning_rate: 0.0010\n","Epoch 5/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 338ms/step - loss: 0.0035 - mae: 0.0277 - val_loss: 3.2286e-04 - val_mae: 0.0153 - learning_rate: 0.0010\n","Epoch 6/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 330ms/step - loss: 0.0023 - mae: 0.0241 - val_loss: 3.8463e-04 - val_mae: 0.0168 - learning_rate: 0.0010\n","Epoch 7/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 334ms/step - loss: 0.0018 - mae: 0.0216 - val_loss: 3.9488e-04 - val_mae: 0.0167 - learning_rate: 0.0010\n","Epoch 8/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 335ms/step - loss: 0.0017 - mae: 0.0205 - val_loss: 3.6696e-04 - val_mae: 0.0157 - learning_rate: 0.0010\n","Epoch 9/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 332ms/step - loss: 0.0014 - mae: 0.0176 - val_loss: 3.8611e-04 - val_mae: 0.0161 - learning_rate: 0.0010\n","Epoch 10/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 332ms/step - loss: 0.0012 - mae: 0.0166 - val_loss: 3.5067e-04 - val_mae: 0.0155 - learning_rate: 0.0010\n","Epoch 11/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 332ms/step - loss: 0.0011 - mae: 0.0150 - val_loss: 3.6062e-04 - val_mae: 0.0158 - learning_rate: 3.0000e-04\n","Epoch 12/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 336ms/step - loss: 0.0011 - mae: 0.0151 - val_loss: 3.8974e-04 - val_mae: 0.0163 - learning_rate: 3.0000e-04\n","Epoch 13/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 333ms/step - loss: 0.0011 - mae: 0.0156 - val_loss: 2.2318e-04 - val_mae: 0.0130 - learning_rate: 3.0000e-04\n","Epoch 14/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 335ms/step - loss: 8.9203e-04 - mae: 0.0133 - val_loss: 1.9862e-04 - val_mae: 0.0123 - learning_rate: 3.0000e-04\n","Epoch 15/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 334ms/step - loss: 8.5065e-04 - mae: 0.0126 - val_loss: 2.0764e-04 - val_mae: 0.0129 - learning_rate: 3.0000e-04\n","Epoch 16/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 332ms/step - loss: 8.7331e-04 - mae: 0.0129 - val_loss: 1.9096e-04 - val_mae: 0.0123 - learning_rate: 3.0000e-04\n","Epoch 17/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 333ms/step - loss: 7.9119e-04 - mae: 0.0122 - val_loss: 2.0476e-04 - val_mae: 0.0128 - learning_rate: 3.0000e-04\n","Epoch 18/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 337ms/step - loss: 7.8539e-04 - mae: 0.0119 - val_loss: 3.8855e-04 - val_mae: 0.0164 - learning_rate: 3.0000e-04\n","Epoch 19/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 335ms/step - loss: 8.1927e-04 - mae: 0.0138 - val_loss: 2.3677e-04 - val_mae: 0.0136 - learning_rate: 3.0000e-04\n","Epoch 20/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 335ms/step - loss: 8.4265e-04 - mae: 0.0127 - val_loss: 3.5619e-04 - val_mae: 0.0158 - learning_rate: 9.0000e-05\n","Epoch 21/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 336ms/step - loss: 9.3247e-04 - mae: 0.0136 - val_loss: 1.6256e-04 - val_mae: 0.0115 - learning_rate: 9.0000e-05\n","Epoch 22/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 331ms/step - loss: 7.4247e-04 - mae: 0.0115 - val_loss: 1.7128e-04 - val_mae: 0.0120 - learning_rate: 9.0000e-05\n","Epoch 23/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 328ms/step - loss: 6.8659e-04 - mae: 0.0113 - val_loss: 1.8096e-04 - val_mae: 0.0124 - learning_rate: 9.0000e-05\n","Epoch 24/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 324ms/step - loss: 7.0126e-04 - mae: 0.0115 - val_loss: 2.0767e-04 - val_mae: 0.0131 - learning_rate: 9.0000e-05\n","Epoch 25/25\n","\u001b[1m449/449\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 323ms/step - loss: 7.3321e-04 - mae: 0.0119 - val_loss: 1.7728e-04 - val_mae: 0.0120 - learning_rate: 2.7000e-05\n","üìä Evaluating model...\n","\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["   üìà Performance:\n","      RMSE: 127.3650\n","      MAE: 95.9342\n","      R¬≤: 0.4464\n","üíæ Saving model...\n","   ‚úÖ Model saved to: unified_lstm_model/\n","\n","üéâ TRAINING COMPLETED SUCCESSFULLY!\n","üìä Final Performance: RMSE=127.3650, R¬≤=0.4464\n","üîú Ready for STAGE 4: Random Forest Training\n","\n","‚úÖ STAGE 3 EXECUTION COMPLETED!\n"]}]},{"cell_type":"markdown","source":["# Stage 4 RF training"],"metadata":{"id":"ySU0NMRqQidR"}},{"cell_type":"markdown","source":["## Versi Tensoflow"],"metadata":{"id":"jahFHv4MZPqv"}},{"cell_type":"code","source":["# ===================================================================\n","# üå≤ STAGE 4: FIXED RANDOM FOREST - Versi Tensoflow\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import gc\n","from datetime import datetime\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","# TensorFlow with compatibility fixes\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError\n","\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","def setup_tensorflow_compatibility():\n","    \"\"\"Setup TensorFlow compatibility for model loading\"\"\"\n","    print(\"üîß Setting up TensorFlow compatibility...\")\n","\n","    # Clear session\n","    tf.keras.backend.clear_session()\n","\n","    # GPU setup\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            print(f\"   ‚úÖ GPU memory growth enabled\")\n","        except RuntimeError as e:\n","            print(f\"   ‚ö†Ô∏è GPU setup warning: {e}\")\n","\n","    gc.collect()\n","\n","def load_lstm_model_with_custom_objects():\n","    \"\"\"Load LSTM model with custom objects for compatibility\"\"\"\n","    print(\"üß† Loading LSTM model with compatibility fixes...\")\n","\n","    model_dir = 'unified_lstm_model'\n","    lstm_model_path = f'{model_dir}/unified_lstm_model.h5'\n","\n","    if not os.path.exists(model_dir):\n","        raise FileNotFoundError(f\"LSTM model directory not found: {model_dir}\")\n","\n","    if not os.path.exists(lstm_model_path):\n","        raise FileNotFoundError(f\"LSTM model file not found: {lstm_model_path}\")\n","\n","    try:\n","        # Define custom objects for compatibility\n","        custom_objects = {\n","            'mse': 'mean_squared_error',\n","            'mae': 'mean_absolute_error',\n","            'mape': 'mean_absolute_percentage_error',\n","            'MeanSquaredError': MeanSquaredError,\n","            'MeanAbsoluteError': MeanAbsoluteError,\n","            'MeanAbsolutePercentageError': MeanAbsolutePercentageError\n","        }\n","\n","        # Try loading with custom objects\n","        print(\"   üîÑ Loading model with custom objects...\")\n","        lstm_model = load_model(lstm_model_path, custom_objects=custom_objects)\n","        print(f\"   ‚úÖ LSTM model loaded successfully with custom objects\")\n","\n","    except Exception as e1:\n","        print(f\"   ‚ö†Ô∏è Custom objects failed: {str(e1)}\")\n","\n","        try:\n","            # Try loading with compile=False\n","            print(\"   üîÑ Loading model without compilation...\")\n","            lstm_model = load_model(lstm_model_path, compile=False)\n","\n","            # Recompile manually\n","            from tensorflow.keras.optimizers import Adam\n","            lstm_model.compile(\n","                optimizer=Adam(learning_rate=0.001),\n","                loss='mse',\n","                metrics=['mae']\n","            )\n","            print(f\"   ‚úÖ LSTM model loaded and recompiled successfully\")\n","\n","        except Exception as e2:\n","            raise Exception(f\"Failed to load LSTM model. Tried custom objects: {str(e1)}. Tried compile=False: {str(e2)}\")\n","\n","    return lstm_model\n","\n","def load_lstm_components_fixed():\n","    \"\"\"Load LSTM components with fixes\"\"\"\n","    print(\"üì¶ Loading LSTM components with compatibility fixes...\")\n","\n","    model_dir = 'unified_lstm_model'\n","\n","    try:\n","        # Load LSTM model with fixes\n","        lstm_model = load_lstm_model_with_custom_objects()\n","\n","        # Load other components (these should work fine)\n","        with open(f'{model_dir}/feature_scaler.pkl', 'rb') as f:\n","            feature_scaler = pickle.load(f)\n","        print(f\"   ‚úÖ Feature scaler loaded\")\n","\n","        with open(f'{model_dir}/target_scaler.pkl', 'rb') as f:\n","            target_scaler = pickle.load(f)\n","        print(f\"   ‚úÖ Target scaler loaded\")\n","\n","        with open(f'{model_dir}/ticker_encoder.pkl', 'rb') as f:\n","            ticker_encoder = pickle.load(f)\n","        print(f\"   ‚úÖ Ticker encoder loaded\")\n","\n","        with open(f'{model_dir}/model_info.pkl', 'rb') as f:\n","            model_info = pickle.load(f)\n","        print(f\"   ‚úÖ Model info loaded\")\n","\n","        print(f\"   üìä Feature columns: {len(model_info['feature_columns'])}\")\n","        print(f\"   üìä Target columns: {len(model_info['target_columns'])}\")\n","\n","        return {\n","            'lstm_model': lstm_model,\n","            'feature_scaler': feature_scaler,\n","            'target_scaler': target_scaler,\n","            'ticker_encoder': ticker_encoder,\n","            'model_info': model_info\n","        }\n","\n","    except Exception as e:\n","        raise Exception(f\"Failed to load LSTM components: {str(e)}\")\n","\n","def load_feature_data_optimized():\n","    \"\"\"Load feature data optimized for processing\"\"\"\n","    print(\"üìÇ Loading feature engineered data...\")\n","\n","    fe_dir = 'feature_engineered_data'\n","    if not os.path.exists(fe_dir):\n","        raise FileNotFoundError(f\"Feature directory not found: {fe_dir}\")\n","\n","    all_datasets = []\n","    fe_files = [f for f in os.listdir(fe_dir) if f.endswith('_fe.csv')]\n","\n","    for file in fe_files:\n","        ticker = file.replace('_fe.csv', '')\n","        file_path = os.path.join(fe_dir, file)\n","\n","        try:\n","            df = pd.read_csv(file_path, low_memory=False)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df['Ticker'] = ticker\n","\n","            # Take recent data (2000 records per stock for speed)\n","            df = df.sort_values('Date').tail(2000).reset_index(drop=True)\n","\n","            # Optimize dtypes\n","            for col in df.select_dtypes(include=['float64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='float')\n","\n","            all_datasets.append(df)\n","            print(f\"   ‚úÖ {ticker}: {len(df)} records\")\n","\n","        except Exception as e:\n","            print(f\"   ‚ùå Error loading {ticker}: {str(e)}\")\n","\n","    combined_df = pd.concat(all_datasets, ignore_index=True)\n","    combined_df = combined_df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n","\n","    gc.collect()\n","    print(f\"‚úÖ Combined data: {len(combined_df)} records\")\n","\n","    return combined_df\n","\n","def generate_lstm_predictions_safe(combined_df, lstm_components):\n","    \"\"\"Generate LSTM predictions with error handling\"\"\"\n","    print(\"üéØ Generating LSTM predictions safely...\")\n","\n","    lstm_model = lstm_components['lstm_model']\n","    feature_scaler = lstm_components['feature_scaler']\n","    ticker_encoder = lstm_components['ticker_encoder']\n","    feature_columns = lstm_components['model_info']['feature_columns']\n","\n","    sequence_length = 60\n","    all_predictions = []\n","    prediction_metadata = []\n","\n","    for ticker in combined_df['Ticker'].unique()[:8]:  # Limit to 8 stocks for speed\n","        print(f\"   Processing {ticker}...\")\n","\n","        ticker_data = combined_df[combined_df['Ticker'] == ticker].copy()\n","        ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n","\n","        # Encode ticker safely\n","        try:\n","            ticker_encoded = ticker_encoder.transform([ticker])[0]\n","        except ValueError:\n","            print(f\"   ‚ö†Ô∏è Unknown ticker {ticker}, skipping...\")\n","            continue\n","\n","        ticker_data['Ticker_Encoded'] = ticker_encoded\n","\n","        # Handle missing values\n","        for col in feature_columns:\n","            if col in ticker_data.columns:\n","                ticker_data[col] = ticker_data[col].fillna(method='ffill').fillna(method='bfill')\n","                if ticker_data[col].isnull().any():\n","                    ticker_data[col] = ticker_data[col].fillna(0)\n","\n","        # Create sequences (limit to last 500 for speed)\n","        max_sequences = 500\n","        start_idx = max(sequence_length, len(ticker_data) - max_sequences - sequence_length)\n","\n","        ticker_sequences = []\n","        ticker_indices = []\n","\n","        for i in range(start_idx, len(ticker_data)):\n","            try:\n","                sequence = ticker_data[feature_columns].iloc[i-sequence_length:i].values\n","\n","                if sequence.shape == (sequence_length, len(feature_columns)) and not np.isnan(sequence).any():\n","                    ticker_sequences.append(sequence)\n","                    ticker_indices.append(i)\n","            except:\n","                continue\n","\n","        if ticker_sequences:\n","            # Process sequences\n","            X_ticker = np.array(ticker_sequences[:max_sequences], dtype=np.float32)\n","\n","            # Scale features\n","            original_shape = X_ticker.shape\n","            X_reshaped = X_ticker.reshape(-1, X_ticker.shape[-1])\n","\n","            try:\n","                X_scaled_reshaped = feature_scaler.transform(X_reshaped)\n","                X_scaled = X_scaled_reshaped.reshape(original_shape)\n","\n","                # Generate predictions in small batches\n","                batch_size = 50\n","                ticker_predictions = []\n","\n","                for batch_start in range(0, len(X_scaled), batch_size):\n","                    batch_end = min(batch_start + batch_size, len(X_scaled))\n","                    batch_X = X_scaled[batch_start:batch_end]\n","\n","                    try:\n","                        batch_pred = lstm_model.predict(batch_X, batch_size=16, verbose=0)\n","                        ticker_predictions.extend(batch_pred)\n","                    except Exception as e:\n","                        print(f\"      ‚ö†Ô∏è Prediction error for batch: {str(e)}\")\n","                        continue\n","\n","                # Store predictions\n","                for j, pred in enumerate(ticker_predictions):\n","                    if j < len(ticker_indices):\n","                        data_idx = ticker_indices[j]\n","                        all_predictions.append(pred)\n","                        prediction_metadata.append({\n","                            'ticker': ticker,\n","                            'date': ticker_data['Date'].iloc[data_idx],\n","                            'original_index': data_idx\n","                        })\n","\n","                print(f\"      ‚úÖ {len(ticker_predictions)} predictions generated\")\n","\n","            except Exception as e:\n","                print(f\"      ‚ùå Error processing {ticker}: {str(e)}\")\n","                continue\n","\n","            # Clean up memory\n","            del X_ticker, X_scaled\n","            gc.collect()\n","\n","    if not all_predictions:\n","        raise Exception(\"No predictions were generated successfully!\")\n","\n","    predictions_array = np.array(all_predictions, dtype=np.float32)\n","    print(f\"   ‚úÖ Total predictions: {len(predictions_array)}\")\n","\n","    return predictions_array, prediction_metadata\n","\n","def create_rf_dataset_simplified(combined_df, lstm_predictions, prediction_metadata):\n","    \"\"\"Create simplified RF dataset\"\"\"\n","    print(\"üîß Creating simplified RF dataset...\")\n","\n","    rf_data = []\n","\n","    for i, (pred, meta) in enumerate(zip(lstm_predictions, prediction_metadata)):\n","        ticker = meta['ticker']\n","        date = meta['date']\n","\n","        # Find corresponding data\n","        ticker_data = combined_df[\n","            (combined_df['Ticker'] == ticker) &\n","            (combined_df['Date'] == date)\n","        ]\n","\n","        if len(ticker_data) == 0:\n","            continue\n","\n","        row = ticker_data.iloc[0]\n","\n","        # LSTM features\n","        lstm_features = {\n","            'LSTM_Day1_Open': float(pred[0]), 'LSTM_Day1_Close': float(pred[1]),\n","            'LSTM_Day2_Open': float(pred[2]), 'LSTM_Day2_Close': float(pred[3]),\n","            'LSTM_Day3_Open': float(pred[4]), 'LSTM_Day3_Close': float(pred[5]),\n","            'LSTM_Day4_Open': float(pred[6]), 'LSTM_Day4_Close': float(pred[7]),\n","            'LSTM_Day5_Open': float(pred[8]), 'LSTM_Day5_Close': float(pred[9])\n","        }\n","\n","        # Essential technical features\n","        tech_features = {}\n","        essential_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'Volatility_20'\n","        ]\n","\n","        for indicator in essential_indicators:\n","            if indicator in row:\n","                value = row[indicator]\n","                tech_features[f'Tech_{indicator}'] = float(value) if pd.notna(value) else 0.0\n","\n","        # Basic market features\n","        market_features = {\n","            'Current_Close': float(row['Close']),\n","            'Current_Volume': float(row['Volume']),\n","            'Price_Range': float(row['High'] - row['Low'])\n","        }\n","\n","        # Create trend labels (simplified)\n","        current_close = row['Close']\n","        targets = {}\n","\n","        # Simple future price lookup\n","        ticker_full_data = combined_df[combined_df['Ticker'] == ticker].sort_values('Date')\n","        current_row_df = ticker_full_data[ticker_full_data['Date'] == date]\n","\n","        if len(current_row_df) > 0:\n","            current_idx = current_row_df.index[0]\n","\n","            for day in range(1, 6):\n","                future_rows = ticker_full_data[ticker_full_data.index > current_idx].head(day)\n","                if len(future_rows) >= day:\n","                    future_close = future_rows.iloc[-1]['Close']\n","                    price_change_pct = ((future_close - current_close) / current_close) * 100\n","\n","                    if price_change_pct > 1.0:\n","                        trend = 'UP'\n","                    elif price_change_pct < -1.0:\n","                        trend = 'DOWN'\n","                    else:\n","                        trend = 'STAY'\n","\n","                    targets[f'Day{day}_Trend'] = trend\n","                else:\n","                    targets[f'Day{day}_Trend'] = 'UNKNOWN'\n","\n","        # Combine all features\n","        row_data = {\n","            'Date': date,\n","            'Ticker': ticker,\n","            **lstm_features,\n","            **tech_features,\n","            **market_features,\n","            **targets\n","        }\n","\n","        rf_data.append(row_data)\n","\n","    rf_df = pd.DataFrame(rf_data)\n","\n","    # Remove unknown targets\n","    for day in range(1, 6):\n","        rf_df = rf_df[rf_df[f'Day{day}_Trend'] != 'UNKNOWN']\n","\n","    rf_df = rf_df.fillna(0)\n","\n","    print(f\"   ‚úÖ RF dataset: {len(rf_df)} samples\")\n","\n","    return rf_df\n","\n","def train_simplified_random_forests(rf_df):\n","    \"\"\"Train simplified Random Forest models\"\"\"\n","    print(\"üå≤ Training simplified Random Forest models...\")\n","\n","    feature_cols = [col for col in rf_df.columns\n","                   if not col.startswith('Day') and col not in ['Date', 'Ticker']]\n","\n","    X = rf_df[feature_cols].copy().astype(np.float32)\n","\n","    print(f\"   üìä Features: {len(feature_cols)}\")\n","    print(f\"   üìä Samples: {len(X)}\")\n","\n","    trained_models = {}\n","    evaluation_results = {}\n","\n","    for day in range(1, 6):\n","        target_col = f'Day{day}_Trend'\n","        y = rf_df[target_col].copy()\n","\n","        print(f\"\\n   üéØ Training {target_col}...\")\n","\n","        class_dist = y.value_counts()\n","        print(f\"      üìä Classes: {dict(class_dist)}\")\n","\n","        if len(class_dist) < 2:\n","            print(f\"      ‚ö†Ô∏è Skipping {target_col} - insufficient classes\")\n","            continue\n","\n","        try:\n","            X_train, X_test, y_train, y_test = train_test_split(\n","                X, y, test_size=0.2, random_state=42, stratify=y\n","            )\n","\n","            # Simple RF with good defaults\n","            rf_model = RandomForestClassifier(\n","                n_estimators=150,\n","                max_depth=15,\n","                min_samples_split=5,\n","                min_samples_leaf=2,\n","                class_weight='balanced',\n","                random_state=42,\n","                n_jobs=-1\n","            )\n","\n","            rf_model.fit(X_train, y_train)\n","            y_pred = rf_model.predict(X_test)\n","\n","            accuracy = accuracy_score(y_test, y_pred)\n","            f1_macro = f1_score(y_test, y_pred, average='macro')\n","            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n","\n","            print(f\"      üìà Accuracy: {accuracy:.4f}\")\n","            print(f\"      üìà F1-Macro: {f1_macro:.4f}\")\n","            print(f\"      üìà F1-Weighted: {f1_weighted:.4f}\")\n","\n","            trained_models[target_col] = {\n","                'model': rf_model,\n","                'feature_columns': feature_cols\n","            }\n","\n","            evaluation_results[target_col] = {\n","                'accuracy': accuracy,\n","                'f1_macro': f1_macro,\n","                'f1_weighted': f1_weighted,\n","                'y_test': y_test,\n","                'y_pred': y_pred,\n","                'class_distribution': dict(class_dist)\n","            }\n","\n","        except Exception as e:\n","            print(f\"      ‚ùå Error training {target_col}: {str(e)}\")\n","            continue\n","\n","    print(f\"\\n   ‚úÖ Trained {len(trained_models)} models successfully!\")\n","\n","    return trained_models, evaluation_results\n","\n","def evaluate_and_save_results(trained_models, evaluation_results):\n","    \"\"\"Evaluate and save final results\"\"\"\n","    print(\"üìä Evaluating and saving results...\")\n","\n","    if not trained_models:\n","        print(\"   ‚ùå No models to evaluate!\")\n","        return None\n","\n","    # Calculate ensemble metrics\n","    accuracies = [evaluation_results[col]['accuracy'] for col in evaluation_results.keys()]\n","    f1_macros = [evaluation_results[col]['f1_macro'] for col in evaluation_results.keys()]\n","    f1_weighteds = [evaluation_results[col]['f1_weighted'] for col in evaluation_results.keys()]\n","\n","    ensemble_metrics = {\n","        'average_accuracy': np.mean(accuracies),\n","        'average_f1_macro': np.mean(f1_macros),\n","        'average_f1_weighted': np.mean(f1_weighteds),\n","        'models_trained': len(trained_models)\n","    }\n","\n","    print(f\"   üìà Ensemble Performance:\")\n","    print(f\"      Average Accuracy: {ensemble_metrics['average_accuracy']:.4f}\")\n","    print(f\"      Average F1-Macro: {ensemble_metrics['average_f1_macro']:.4f}\")\n","    print(f\"      Models trained: {ensemble_metrics['models_trained']}/5\")\n","\n","    # Save models\n","    model_dir = 'random_forest_models'\n","    os.makedirs(model_dir, exist_ok=True)\n","\n","    for target_col, model_data in trained_models.items():\n","        model_path = f\"{model_dir}/{target_col}_rf_model.pkl\"\n","        joblib.dump(model_data, model_path, compress=3)\n","        print(f\"   üíæ Saved: {model_path}\")\n","\n","    # Save ensemble info\n","    ensemble_info = {\n","        'ensemble_metrics': ensemble_metrics,\n","        'evaluation_results': evaluation_results,\n","        'training_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'model_type': 'RandomForest_Simplified_Compatible'\n","    }\n","\n","    ensemble_path = f\"{model_dir}/ensemble_info.pkl\"\n","    with open(ensemble_path, 'wb') as f:\n","        pickle.dump(ensemble_info, f)\n","\n","    print(f\"   üíæ Ensemble info saved: {ensemble_path}\")\n","\n","    return ensemble_metrics\n","\n","def main_fixed_rf_training():\n","    \"\"\"Main fixed RF training function\"\"\"\n","    print(\"üå≤ STARTING STAGE 4: FIXED RANDOM FOREST TRAINING\")\n","    print(\"=\"*80)\n","    print(f\"üîß Mode: Compatibility Fixed\")\n","    print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    print(\"=\"*80)\n","\n","    start_time = datetime.now()\n","\n","    try:\n","        # Step 1: Setup TensorFlow\n","        setup_tensorflow_compatibility()\n","\n","        # Step 2: Load LSTM components with fixes\n","        lstm_components = load_lstm_components_fixed()\n","\n","        # Step 3: Load feature data\n","        combined_df = load_feature_data_optimized()\n","\n","        # Step 4: Generate predictions safely\n","        lstm_predictions, prediction_metadata = generate_lstm_predictions_safe(\n","            combined_df, lstm_components\n","        )\n","\n","        # Step 5: Create RF dataset\n","        rf_df = create_rf_dataset_simplified(\n","            combined_df, lstm_predictions, prediction_metadata\n","        )\n","\n","        # Memory cleanup\n","        del lstm_predictions, prediction_metadata, combined_df\n","        gc.collect()\n","\n","        # Step 6: Train models\n","        trained_models, evaluation_results = train_simplified_random_forests(rf_df)\n","\n","        # Step 7: Evaluate and save\n","        ensemble_metrics = evaluate_and_save_results(trained_models, evaluation_results)\n","\n","        end_time = datetime.now()\n","        training_time = (end_time - start_time).total_seconds() / 60\n","\n","        print(f\"\\nüéâ STAGE 4 COMPLETED SUCCESSFULLY!\")\n","        print(f\"‚è±Ô∏è Training time: {training_time:.1f} minutes\")\n","        print(f\"üéØ Models trained: {len(trained_models) if trained_models else 0}/5\")\n","\n","        if ensemble_metrics:\n","            print(f\"üìä Average F1-Score: {ensemble_metrics['average_f1_macro']:.4f}\")\n","\n","        print(f\"üöÄ LSTM + RANDOM FOREST PIPELINE READY!\")\n","\n","        return {\n","            'status': 'SUCCESS',\n","            'training_time_minutes': training_time,\n","            'models_trained': len(trained_models) if trained_models else 0,\n","            'ensemble_metrics': ensemble_metrics\n","        }\n","\n","    except Exception as e:\n","        end_time = datetime.now()\n","        training_time = (end_time - start_time).total_seconds() / 60\n","\n","        print(f\"\\n‚ùå STAGE 4 TRAINING FAILED!\")\n","        print(f\"Error: {str(e)}\")\n","        print(f\"‚è±Ô∏è Time elapsed: {training_time:.1f} minutes\")\n","\n","        import traceback\n","        traceback.print_exc()\n","\n","        return {\n","            'status': 'ERROR',\n","            'error': str(e),\n","            'training_time_minutes': training_time\n","        }\n","\n","# üöÄ RUN FIXED RANDOM FOREST TRAINING\n","if __name__ == \"__main__\":\n","    result = main_fixed_rf_training()\n","\n","print(\"\\nüéØ STAGE 4 EXECUTION COMPLETED!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RQr9ujKQsgg","executionInfo":{"status":"ok","timestamp":1753425329311,"user_tz":-420,"elapsed":83515,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"df5d402e-7d9f-4498-85b7-0d4f4cd9e735"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üå≤ STARTING STAGE 4: FIXED RANDOM FOREST TRAINING\n","================================================================================\n","üîß Mode: Compatibility Fixed\n","‚è∞ Start time: 2025-07-25 06:34:06\n","================================================================================\n","üîß Setting up TensorFlow compatibility...\n","   ‚úÖ GPU memory growth enabled\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["üì¶ Loading LSTM components with compatibility fixes...\n","üß† Loading LSTM model with compatibility fixes...\n","   üîÑ Loading model with custom objects...\n","   ‚úÖ LSTM model loaded successfully with custom objects\n","   ‚úÖ Feature scaler loaded\n","   ‚úÖ Target scaler loaded\n","   ‚úÖ Ticker encoder loaded\n","   ‚úÖ Model info loaded\n","   üìä Feature columns: 21\n","   üìä Target columns: 10\n","üìÇ Loading feature engineered data...\n","   ‚úÖ NFLX: 2000 records\n","   ‚úÖ TSLA: 2000 records\n","   ‚úÖ AAPL: 2000 records\n","   ‚úÖ GOOGL: 2000 records\n","   ‚úÖ META: 2000 records\n","   ‚úÖ BBCA: 2000 records\n","   ‚úÖ NVDA: 2000 records\n","   ‚úÖ MSFT: 2000 records\n","   ‚úÖ BAC: 2000 records\n","   ‚úÖ JPM: 2000 records\n","   ‚úÖ AMZN: 2000 records\n","‚úÖ Combined data: 22000 records\n","üéØ Generating LSTM predictions safely...\n","   Processing AAPL...\n","      ‚úÖ 500 predictions generated\n","   Processing AMZN...\n","      ‚úÖ 500 predictions generated\n","   Processing BAC...\n","      ‚úÖ 500 predictions generated\n","   Processing BBCA...\n","      ‚úÖ 500 predictions generated\n","   Processing GOOGL...\n","      ‚úÖ 500 predictions generated\n","   Processing JPM...\n","      ‚úÖ 500 predictions generated\n","   Processing META...\n","      ‚úÖ 500 predictions generated\n","   Processing MSFT...\n","      ‚úÖ 500 predictions generated\n","   ‚úÖ Total predictions: 4000\n","üîß Creating simplified RF dataset...\n","   ‚úÖ RF dataset: 4000 samples\n","üå≤ Training simplified Random Forest models...\n","   üìä Features: 25\n","   üìä Samples: 4000\n","\n","   üéØ Training Day1_Trend...\n","      üìä Classes: {'STAY': np.int64(2181), 'UP': np.int64(998), 'DOWN': np.int64(821)}\n","      üìà Accuracy: 0.4925\n","      üìà F1-Macro: 0.3772\n","      üìà F1-Weighted: 0.4634\n","\n","   üéØ Training Day2_Trend...\n","      üìä Classes: {'STAY': np.int64(1529), 'UP': np.int64(1421), 'DOWN': np.int64(1050)}\n","      üìà Accuracy: 0.4500\n","      üìà F1-Macro: 0.4437\n","      üìà F1-Weighted: 0.4488\n","\n","   üéØ Training Day3_Trend...\n","      üìä Classes: {'UP': np.int64(1571), 'STAY': np.int64(1271), 'DOWN': np.int64(1158)}\n","      üìà Accuracy: 0.5312\n","      üìà F1-Macro: 0.5232\n","      üìà F1-Weighted: 0.5290\n","\n","   üéØ Training Day4_Trend...\n","      üìä Classes: {'UP': np.int64(1715), 'DOWN': np.int64(1221), 'STAY': np.int64(1064)}\n","      üìà Accuracy: 0.5800\n","      üìà F1-Macro: 0.5561\n","      üìà F1-Weighted: 0.5754\n","\n","   üéØ Training Day5_Trend...\n","      üìä Classes: {'UP': np.int64(1786), 'DOWN': np.int64(1259), 'STAY': np.int64(955)}\n","      üìà Accuracy: 0.6175\n","      üìà F1-Macro: 0.5733\n","      üìà F1-Weighted: 0.6081\n","\n","   ‚úÖ Trained 5 models successfully!\n","üìä Evaluating and saving results...\n","   üìà Ensemble Performance:\n","      Average Accuracy: 0.5343\n","      Average F1-Macro: 0.4947\n","      Models trained: 5/5\n","   üíæ Saved: random_forest_models/Day1_Trend_rf_model.pkl\n","   üíæ Saved: random_forest_models/Day2_Trend_rf_model.pkl\n","   üíæ Saved: random_forest_models/Day3_Trend_rf_model.pkl\n","   üíæ Saved: random_forest_models/Day4_Trend_rf_model.pkl\n","   üíæ Saved: random_forest_models/Day5_Trend_rf_model.pkl\n","   üíæ Ensemble info saved: random_forest_models/ensemble_info.pkl\n","\n","üéâ STAGE 4 COMPLETED SUCCESSFULLY!\n","‚è±Ô∏è Training time: 1.4 minutes\n","üéØ Models trained: 5/5\n","üìä Average F1-Score: 0.4947\n","üöÄ LSTM + RANDOM FOREST PIPELINE READY!\n","\n","üéØ STAGE 4 EXECUTION COMPLETED!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yB5ovEVoZLGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Versi Skicit-learn"],"metadata":{"id":"LbsZrMz1ZWBq"}},{"cell_type":"code","source":["# ===================================================================\n","# üöÄ FIXED LAYOUT STOCK PREDICTOR - wasirawasenju\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import joblib\n","from datetime import datetime, timedelta\n","import yfinance as yf\n","import streamlit as st\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","\n","warnings.filterwarnings('ignore')\n","\n","class SimpleStockPredictor:\n","    \"\"\"Simple Stock Predictor using only RF models + Technical Analysis\"\"\"\n","\n","    def __init__(self):\n","        self.rf_models = None\n","        self.is_loaded = False\n","\n","    def load_rf_models_only(self):\n","        \"\"\"Load only Random Forest models (skip LSTM)\"\"\"\n","        print(\"üîÑ Loading Random Forest models...\")\n","\n","        try:\n","            rf_dir = 'random_forest_models'\n","\n","            if not os.path.exists(rf_dir):\n","                return False, f\"RF models directory not found: {rf_dir}\"\n","\n","            rf_models = {}\n","            models_loaded = 0\n","\n","            # Load available RF models\n","            for day in range(1, 6):\n","                model_path = f\"{rf_dir}/Day{day}_Trend_rf_model.pkl\"\n","\n","                if os.path.exists(model_path):\n","                    try:\n","                        rf_models[f'Day{day}'] = joblib.load(model_path)\n","                        models_loaded += 1\n","                        print(f\"   ‚úÖ Day{day} RF model loaded\")\n","                    except Exception as e:\n","                        print(f\"   ‚ö†Ô∏è Day{day} model error: {str(e)}\")\n","                else:\n","                    print(f\"   ‚ùå Day{day} model not found\")\n","\n","            if models_loaded == 0:\n","                return False, \"No RF models could be loaded\"\n","\n","            self.rf_models = {\n","                'models': rf_models,\n","                'models_loaded': models_loaded\n","            }\n","\n","            self.is_loaded = True\n","            print(f\"‚úÖ Loaded {models_loaded}/5 RF models successfully!\")\n","\n","            return True, f\"Loaded {models_loaded}/5 RF models\"\n","\n","        except Exception as e:\n","            return False, f\"Error loading RF models: {str(e)}\"\n","\n","    def download_stock_data(self, ticker, period=\"1y\"):\n","        \"\"\"Download stock data from yfinance\"\"\"\n","        print(f\"üì• Downloading {ticker}...\")\n","\n","        try:\n","            stock = yf.Ticker(ticker)\n","            df = stock.history(period=period, auto_adjust=True)\n","\n","            if df.empty:\n","                raise ValueError(f\"No data for {ticker}\")\n","\n","            df = df.reset_index()\n","            df['Date'] = pd.to_datetime(df['Date'])\n","\n","            # Remove timezone if present\n","            if hasattr(df['Date'].dtype, 'tz') and df['Date'].dt.tz is not None:\n","                df['Date'] = df['Date'].dt.tz_localize(None)\n","\n","            print(f\"‚úÖ Got {len(df)} records for {ticker}\")\n","            return df\n","\n","        except Exception as e:\n","            raise Exception(f\"Download failed: {str(e)}\")\n","\n","    def calculate_indicators(self, df):\n","        \"\"\"Calculate essential technical indicators\"\"\"\n","        print(\"üîß Calculating indicators...\")\n","\n","        # Moving averages\n","        df['SMA_10'] = df['Close'].rolling(10).mean()\n","        df['SMA_20'] = df['Close'].rolling(20).mean()\n","        df['EMA_10'] = df['Close'].ewm(span=10).mean()\n","        df['EMA_20'] = df['Close'].ewm(span=20).mean()\n","\n","        # RSI\n","        delta = df['Close'].diff()\n","        gain = delta.where(delta > 0, 0).rolling(14).mean()\n","        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n","        rs = gain / loss\n","        df['RSI_14'] = 100 - (100 / (1 + rs))\n","\n","        # MACD\n","        ema12 = df['Close'].ewm(span=12).mean()\n","        ema26 = df['Close'].ewm(span=26).mean()\n","        df['MACD'] = ema12 - ema26\n","        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n","\n","        # Bollinger Bands\n","        bb_mid = df['Close'].rolling(20).mean()\n","        bb_std = df['Close'].rolling(20).std()\n","        df['BB_High'] = bb_mid + (bb_std * 2)\n","        df['BB_Low'] = bb_mid - (bb_std * 2)\n","        df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","        df['BB_Width'] = (df['BB_High'] - df['BB_Low']) / bb_mid\n","\n","        # Volume & momentum\n","        df['Volume_MA_10'] = df['Volume'].rolling(10).mean()\n","        df['Volatility_20'] = df['Close'].rolling(20).std()\n","        df['Daily_Return'] = df['Close'].pct_change() * 100\n","        df['ROC_10'] = df['Close'].pct_change(10) * 100\n","\n","        # Binary features\n","        df['High_Volume'] = (df['Volume'] > df['Volume_MA_10'] * 1.5).astype(int)\n","        df['Low_Volume'] = (df['Volume'] < df['Volume_MA_10'] * 0.5).astype(int)\n","        df['Strong_Momentum'] = (df['ROC_10'] > 5).astype(int)\n","        df['Weak_Momentum'] = (df['ROC_10'] < -5).astype(int)\n","\n","        # OBV (simplified)\n","        df['OBV'] = (df['Volume'] * np.sign(df['Close'].diff())).cumsum()\n","\n","        # ATR (simplified)\n","        df['ATR'] = ((df['High'] - df['Low']).rolling(14).mean())\n","\n","        # Fill NaN\n","        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n","\n","        print(\"‚úÖ Indicators calculated\")\n","        return df\n","\n","    def predict_prices_technical(self, df):\n","        \"\"\"Predict prices using technical analysis (no LSTM)\"\"\"\n","        print(\"üéØ Predicting prices with technical analysis...\")\n","\n","        current_price = df['Close'].iloc[-1]\n","\n","        # Calculate trends\n","        sma_trend = df['SMA_10'].iloc[-1] - df['SMA_10'].iloc[-5]\n","        ema_trend = df['EMA_10'].iloc[-1] - df['EMA_10'].iloc[-5]\n","        momentum = df['ROC_10'].iloc[-1]\n","        rsi = df['RSI_14'].iloc[-1]\n","\n","        # Trend strength\n","        trend_strength = (sma_trend + ema_trend) / 2\n","\n","        # RSI bias (oversold/overbought)\n","        if rsi > 70:\n","            rsi_bias = -0.5  # Overbought, expect decline\n","        elif rsi < 30:\n","            rsi_bias = 0.5   # Oversold, expect rise\n","        else:\n","            rsi_bias = 0\n","\n","        # Generate 5-day predictions\n","        predictions = {}\n","        base_price = current_price\n","\n","        for day in range(1, 6):\n","            # Decay trend over time\n","            decay = 0.85 ** (day - 1)\n","\n","            # Price change calculation\n","            daily_change = (trend_strength * decay) + (momentum * 0.01 * decay) + (rsi_bias * decay)\n","\n","            # Limit max change to 5% per day\n","            max_change = base_price * 0.05\n","            daily_change = max(min(daily_change, max_change), -max_change)\n","\n","            # Calculate new prices\n","            predicted_close = base_price + daily_change\n","\n","            # Simple open prediction (slight variation from previous close)\n","            if day == 1:\n","                predicted_open = current_price * (1 + np.random.normal(0, 0.002))  # Small random gap\n","            else:\n","                predicted_open = predictions[f'Day{day-1}']['Close'] * (1 + np.random.normal(0, 0.001))\n","\n","            predictions[f'Day{day}'] = {\n","                'Open': max(predicted_open, 0.01),  # Ensure positive\n","                'Close': max(predicted_close, 0.01)\n","            }\n","\n","            base_price = predicted_close\n","\n","        return predictions\n","\n","    def predict_trends_with_rf(self, df, price_predictions):\n","        \"\"\"Predict trends using loaded RF models\"\"\"\n","        if not self.is_loaded or not self.rf_models:\n","            print(\"‚ö†Ô∏è No RF models available, using simple trend logic\")\n","            return self._simple_trend_prediction(price_predictions)\n","\n","        print(\"üå≤ Predicting trends with RF models...\")\n","\n","        latest_data = df.iloc[-1]\n","\n","        # Prepare features for RF\n","        rf_features = {}\n","\n","        # Mock LSTM features (use our technical predictions)\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            rf_features[f'LSTM_{day_key}_Open'] = price_predictions[day_key]['Open']\n","            rf_features[f'LSTM_{day_key}_Close'] = price_predictions[day_key]['Close']\n","\n","        # Technical features\n","        tech_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Volatility_20', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'OBV',\n","            'High_Volume', 'Low_Volume', 'Strong_Momentum', 'Weak_Momentum'\n","        ]\n","\n","        for indicator in tech_indicators:\n","            value = latest_data.get(indicator, 0)\n","            rf_features[f'Tech_{indicator}'] = float(value) if pd.notna(value) else 0.0\n","\n","        # Market state features\n","        rf_features.update({\n","            'Current_Close': float(latest_data['Close']),\n","            'Current_Volume': float(latest_data['Volume']),\n","            'Current_High': float(latest_data['High']),\n","            'Current_Low': float(latest_data['Low']),\n","            'Price_Range': float(latest_data['High'] - latest_data['Low']),\n","            'Volume_Ratio': float(latest_data['Volume'] / latest_data.get('Volume_MA_10', latest_data['Volume']))\n","        })\n","\n","        # Predict with RF models\n","        trends = {}\n","        confidences = {}\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","\n","            if day_key in self.rf_models['models']:\n","                try:\n","                    model_data = self.rf_models['models'][day_key]\n","                    rf_model = model_data['model']\n","                    model_features = model_data['feature_columns']\n","\n","                    # Align features\n","                    aligned_features = []\n","                    for feature in model_features:\n","                        aligned_features.append(rf_features.get(feature, 0.0))\n","\n","                    # Predict\n","                    X_pred = np.array([aligned_features])\n","                    trend_pred = rf_model.predict(X_pred)[0]\n","                    trend_proba = rf_model.predict_proba(X_pred)[0]\n","\n","                    trends[day_key] = trend_pred\n","                    confidences[day_key] = float(max(trend_proba))\n","\n","                    print(f\"   ‚úÖ {day_key}: {trend_pred} (confidence: {max(trend_proba):.2f})\")\n","\n","                except Exception as e:\n","                    print(f\"   ‚ö†Ô∏è {day_key} RF failed: {str(e)}\")\n","                    trends[day_key] = 'STAY'\n","                    confidences[day_key] = 0.5\n","            else:\n","                print(f\"   ‚ùå {day_key} model not available\")\n","                trends[day_key] = 'STAY'\n","                confidences[day_key] = 0.3\n","\n","        return trends, confidences\n","\n","    def _simple_trend_prediction(self, price_predictions):\n","        \"\"\"Fallback simple trend prediction\"\"\"\n","        trends = {}\n","        confidences = {}\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            pred_close = price_predictions[day_key]['Close']\n","            pred_open = price_predictions[day_key]['Open']\n","\n","            change_pct = ((pred_close - pred_open) / pred_open) * 100\n","\n","            if change_pct > 1.0:\n","                trends[day_key] = 'UP'\n","            elif change_pct < -1.0:\n","                trends[day_key] = 'DOWN'\n","            else:\n","                trends[day_key] = 'STAY'\n","\n","            confidences[day_key] = 0.6  # Default confidence\n","\n","        return trends, confidences\n","\n","    def predict_stock(self, ticker, period=\"1y\"):\n","        \"\"\"Complete prediction pipeline\"\"\"\n","        print(f\"üéØ Predicting {ticker}...\")\n","\n","        try:\n","            # Download data\n","            df = self.download_stock_data(ticker, period)\n","\n","            # Calculate indicators\n","            df = self.calculate_indicators(df)\n","\n","            # Predict prices\n","            price_predictions = self.predict_prices_technical(df)\n","\n","            # Predict trends\n","            trends, confidences = self.predict_trends_with_rf(df, price_predictions)\n","\n","            # Format results\n","            today = datetime.now().date()\n","            prediction_dates = [(today + timedelta(days=i)) for i in range(1, 6)]\n","\n","            predictions = {}\n","            for i, day_key in enumerate(['Day1', 'Day2', 'Day3', 'Day4', 'Day5']):\n","                predictions[day_key] = {\n","                    'date': prediction_dates[i].strftime('%Y-%m-%d'),\n","                    'predicted_open': price_predictions[day_key]['Open'],\n","                    'predicted_close': price_predictions[day_key]['Close'],\n","                    'trend': trends[day_key],\n","                    'confidence': confidences[day_key]\n","                }\n","\n","            current_data = df.iloc[-1]\n","            stock_info = {\n","                'ticker': ticker.upper(),\n","                'current_price': float(current_data['Close']),\n","                'current_date': current_data['Date'].strftime('%Y-%m-%d'),\n","                'volume': float(current_data['Volume']),\n","                'prediction_generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),\n","                'data_points_used': len(df),\n","                'rf_models_available': self.rf_models['models_loaded'] if self.rf_models else 0\n","            }\n","\n","            return {\n","                'status': 'SUCCESS',\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'historical_data': df.tail(30).to_dict('records')\n","            }\n","\n","        except Exception as e:\n","            return {\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'ticker': ticker\n","            }\n","\n","# ===================================================================\n","# üñ•Ô∏è STREAMLIT DASHBOARD - FIXED LAYOUT\n","# ===================================================================\n","\n","def main_dashboard():\n","    \"\"\"Fixed layout dashboard\"\"\"\n","\n","    st.set_page_config(\n","        page_title=\"üìà Stock Predictor - wasirawasenju\",\n","        page_icon=\"üöÄ\",\n","        layout=\"wide\"\n","    )\n","\n","    # Custom CSS\n","    st.markdown(\"\"\"\n","    <style>\n","    .main-title {\n","        font-size: 2.5rem;\n","        color: #1f77b4;\n","        text-align: center;\n","        margin-bottom: 1rem;\n","    }\n","    .subtitle {\n","        font-size: 1.2rem;\n","        text-align: center;\n","        color: #666;\n","        margin-bottom: 2rem;\n","    }\n","    .user-info {\n","        background: linear-gradient(90deg, #f0f8ff, #e6f3ff);\n","        padding: 1rem;\n","        border-radius: 10px;\n","        border-left: 4px solid #1f77b4;\n","        margin-bottom: 1rem;\n","    }\n","    .prediction-card {\n","        background: #f8f9fa;\n","        padding: 1.5rem;\n","        border-radius: 10px;\n","        border: 1px solid #dee2e6;\n","        margin: 1rem 0;\n","    }\n","    .metric-card {\n","        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n","        color: white;\n","        padding: 1rem;\n","        border-radius: 10px;\n","        text-align: center;\n","        margin: 0.5rem 0;\n","    }\n","    </style>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Initialize session state\n","    if 'predictor' not in st.session_state:\n","        st.session_state.predictor = SimpleStockPredictor()\n","        st.session_state.models_loaded = False\n","        st.session_state.prediction_result = None\n","        st.session_state.current_ticker = \"\"\n","\n","    # Header\n","    st.markdown('<h1 class=\"main-title\">üöÄ AI Stock Predictor</h1>', unsafe_allow_html=True)\n","    st.markdown('<p class=\"subtitle\">Random Forest + Technical Analysis | No TensorFlow Required</p>', unsafe_allow_html=True)\n","\n","    # User info\n","    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')\n","    st.markdown(f\"\"\"\n","    <div class=\"user-info\">\n","        <h4>üë§ User: wasirawasenju</h4>\n","        <p>üìÖ Current Time: {current_time}</p>\n","        <p>üè† Environment: Local Development</p>\n","    </div>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Create two columns: sidebar-like left column and main content\n","    col_sidebar, col_main = st.columns([1, 3])\n","\n","    # LEFT COLUMN (Control Panel)\n","    with col_sidebar:\n","        st.markdown(\"### üéõÔ∏è Control Panel\")\n","\n","        # Model loading section\n","        st.markdown(\"#### ü§ñ AI Models\")\n","\n","        if not st.session_state.models_loaded:\n","            if st.button(\"üîÑ Load RF Models\", type=\"primary\", use_container_width=True):\n","                with st.spinner(\"Loading models...\"):\n","                    success, message = st.session_state.predictor.load_rf_models_only()\n","\n","                    if success:\n","                        st.session_state.models_loaded = True\n","                        st.success(f\"‚úÖ {message}\")\n","                    else:\n","                        st.error(f\"‚ùå {message}\")\n","                        st.warning(\"Will use fallback prediction\")\n","                        st.session_state.models_loaded = True  # Allow to continue\n","        else:\n","            st.success(\"‚úÖ Models Ready\")\n","            if st.session_state.predictor.rf_models:\n","                rf_count = st.session_state.predictor.rf_models['models_loaded']\n","                st.info(f\"RF Models: {rf_count}/5\")\n","\n","        # Stock input section\n","        if st.session_state.models_loaded:\n","            st.markdown(\"#### üìä Stock Selection\")\n","\n","            # Text input\n","            ticker_input = st.text_input(\n","                \"Enter Ticker:\",\n","                value=st.session_state.current_ticker,\n","                placeholder=\"AAPL, GOOGL, BBCA.JK...\",\n","                key=\"ticker_input\"\n","            )\n","\n","            # Quick select buttons\n","            st.markdown(\"**Quick Select:**\")\n","\n","            # Create 2x3 grid for buttons\n","            col1, col2 = st.columns(2)\n","\n","            with col1:\n","                if st.button(\"üçé AAPL\", use_container_width=True):\n","                    st.session_state.current_ticker = \"AAPL\"\n","                    st.rerun()\n","\n","                if st.button(\"üîç GOOGL\", use_container_width=True):\n","                    st.session_state.current_ticker = \"GOOGL\"\n","                    st.rerun()\n","\n","                if st.button(\"üè¶ BBCA.JK\", use_container_width=True):\n","                    st.session_state.current_ticker = \"BBCA.JK\"\n","                    st.rerun()\n","\n","            with col2:\n","                if st.button(\"üöó TSLA\", use_container_width=True):\n","                    st.session_state.current_ticker = \"TSLA\"\n","                    st.rerun()\n","\n","                if st.button(\"üíª MSFT\", use_container_width=True):\n","                    st.session_state.current_ticker = \"MSFT\"\n","                    st.rerun()\n","\n","                if st.button(\"üé¨ NFLX\", use_container_width=True):\n","                    st.session_state.current_ticker = \"NFLX\"\n","                    st.rerun()\n","\n","            # Update current ticker from input\n","            if ticker_input != st.session_state.current_ticker:\n","                st.session_state.current_ticker = ticker_input\n","\n","            # Predict button\n","            st.markdown(\"---\")\n","            if st.button(\"üéØ Generate Prediction\", type=\"primary\", use_container_width=True):\n","                if st.session_state.current_ticker:\n","                    # Show prediction in main area\n","                    with col_main:\n","                        with st.spinner(f\"üîç Analyzing {st.session_state.current_ticker.upper()}...\"):\n","                            result = st.session_state.predictor.predict_stock(st.session_state.current_ticker)\n","\n","                        # Store result in session state\n","                        st.session_state.prediction_result = result\n","                        st.rerun()\n","                else:\n","                    st.warning(\"Please enter a ticker!\")\n","\n","    # RIGHT COLUMN (Main Content)\n","    with col_main:\n","        if st.session_state.prediction_result is None:\n","            # Default welcome content\n","            st.markdown(\"### üéØ Welcome to AI Stock Predictor\")\n","\n","            st.markdown(\"\"\"\n","            <div class=\"prediction-card\">\n","                <h4>üöÄ Get Started</h4>\n","                <p>1. Load the AI models using the control panel</p>\n","                <p>2. Enter a stock ticker or use quick select buttons</p>\n","                <p>3. Click \"Generate Prediction\" to see 5-day forecasts</p>\n","                <p>4. View detailed analysis, charts, and investment insights</p>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            st.markdown(\"\"\"\n","            <div class=\"prediction-card\">\n","                <h4>üìä Features</h4>\n","                <ul>\n","                <li>üéØ <strong>5-Day Price Predictions</strong> - Open & Close forecasts</li>\n","                <li>üìà <strong>Trend Analysis</strong> - UP/DOWN/STAY classifications</li>\n","                <li>ü§ñ <strong>AI-Powered</strong> - Random Forest + Technical Analysis</li>\n","                <li>üì± <strong>Real-time Data</strong> - Live data from Yahoo Finance</li>\n","                <li>üí° <strong>Investment Insights</strong> - Automated recommendations</li>\n","                </ul>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            # Sample prediction preview\n","            st.markdown(\"### üìã Sample Prediction Output\")\n","\n","            sample_data = {\n","                \"Date\": [\"2025-07-26\", \"2025-07-27\", \"2025-07-28\", \"2025-07-29\", \"2025-07-30\"],\n","                \"Open\": [\"$150.20\", \"$151.50\", \"$152.10\", \"$151.80\", \"$153.00\"],\n","                \"Close\": [\"$151.30\", \"$152.80\", \"$151.90\", \"$153.20\", \"$154.10\"],\n","                \"Trend\": [\"üìà UP\", \"üìà UP\", \"üìâ DOWN\", \"üìà UP\", \"üìà UP\"],\n","                \"Confidence\": [\"85.2%\", \"78.9%\", \"72.1%\", \"81.5%\", \"77.3%\"]\n","            }\n","\n","            sample_df = pd.DataFrame(sample_data)\n","            st.dataframe(sample_df, use_container_width=True)\n","\n","        else:\n","            # Display prediction results\n","            if st.session_state.prediction_result['status'] == 'SUCCESS':\n","                display_prediction_results(st.session_state.prediction_result)\n","            else:\n","                st.error(f\"‚ùå Prediction Error: {st.session_state.prediction_result['error']}\")\n","\n","                # Clear error after showing\n","                if st.button(\"üîÑ Clear Error\"):\n","                    st.session_state.prediction_result = None\n","                    st.rerun()\n","\n","def display_prediction_results(result):\n","    \"\"\"Display prediction results in main area\"\"\"\n","\n","    stock_info = result['stock_info']\n","    predictions = result['predictions']\n","    historical_data = result['historical_data']\n","\n","    # Header with stock info\n","    st.markdown(f\"### üìä {stock_info['ticker']} - Prediction Results\")\n","\n","    # Metrics row\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>üí∞ Current Price</h4>\n","            <h2>${stock_info['current_price']:.2f}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>üìä Volume</h4>\n","            <h2>{stock_info['volume']:,.0f}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col3:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>üìÖ Data Date</h4>\n","            <h2>{stock_info['current_date']}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col4:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>ü§ñ RF Models</h4>\n","            <h2>{stock_info['rf_models_available']}/5</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # 5-Day Predictions Table\n","    st.markdown(\"### üîÆ 5-Day AI Predictions\")\n","\n","    pred_data = []\n","    for day, pred in predictions.items():\n","        trend_emoji = {\"UP\": \"üìà\", \"DOWN\": \"üìâ\", \"STAY\": \"‚û°Ô∏è\"}.get(pred['trend'], \"‚ùì\")\n","\n","        change = pred['predicted_close'] - pred['predicted_open']\n","        change_pct = (change / pred['predicted_open']) * 100\n","\n","        pred_data.append({\n","            \"üìÖ Date\": pred['date'],\n","            \"üåÖ Open\": f\"${pred['predicted_open']:.2f}\",\n","            \"üåá Close\": f\"${pred['predicted_close']:.2f}\",\n","            \"üìä Daily Change\": f\"${change:+.2f} ({change_pct:+.1f}%)\",\n","            \"üìà Trend\": f\"{trend_emoji} {pred['trend']}\",\n","            \"üéØ Confidence\": f\"{pred['confidence']:.1%}\"\n","        })\n","\n","    pred_df = pd.DataFrame(pred_data)\n","    st.dataframe(pred_df, use_container_width=True)\n","\n","    # Price Chart\n","    st.markdown(\"### üìà Interactive Price Chart\")\n","\n","    # Prepare chart data\n","    hist_df = pd.DataFrame(historical_data)\n","    hist_df['Date'] = pd.to_datetime(hist_df['Date'])\n","\n","    # Create chart\n","    fig = make_subplots(\n","        rows=2, cols=1,\n","        shared_xaxes=True,\n","        vertical_spacing=0.1,\n","        subplot_titles=(f'{stock_info[\"ticker\"]} - Price Prediction', 'Trading Volume'),\n","        row_heights=[0.7, 0.3]\n","    )\n","\n","    # Historical candlestick\n","    fig.add_trace(\n","        go.Candlestick(\n","            x=hist_df['Date'],\n","            open=hist_df['Open'],\n","            high=hist_df['High'],\n","            low=hist_df['Low'],\n","            close=hist_df['Close'],\n","            name='Historical Price',\n","            increasing_line_color='green',\n","            decreasing_line_color='red'\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Predicted prices\n","    pred_dates = [datetime.strptime(pred['date'], '%Y-%m-%d') for pred in predictions.values()]\n","    pred_closes = [pred['predicted_close'] for pred in predictions.values()]\n","\n","    # Connect last historical to first prediction\n","    last_date = hist_df['Date'].iloc[-1]\n","    last_price = hist_df['Close'].iloc[-1]\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=[last_date] + pred_dates,\n","            y=[last_price] + pred_closes,\n","            mode='lines+markers',\n","            name='Predicted Close',\n","            line=dict(color='red', width=4, dash='dash'),\n","            marker=dict(size=10, color='red', symbol='diamond')\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Volume bars\n","    fig.add_trace(\n","        go.Bar(\n","            x=hist_df['Date'],\n","            y=hist_df['Volume'],\n","            name='Volume',\n","            marker_color='lightblue',\n","            opacity=0.7\n","        ),\n","        row=2, col=1\n","    )\n","\n","    fig.update_layout(\n","        title=f\"{stock_info['ticker']} - AI Prediction Analysis\",\n","        height=700,\n","        showlegend=True,\n","        hovermode='x unified'\n","    )\n","\n","    fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n","    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)\n","    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n","\n","    st.plotly_chart(fig, use_container_width=True)\n","\n","    # Investment Analysis\n","    st.markdown(\"### üí° Investment Analysis\")\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        # Trend analysis\n","        up_days = sum(1 for pred in predictions.values() if pred['trend'] == 'UP')\n","        down_days = sum(1 for pred in predictions.values() if pred['trend'] == 'DOWN')\n","        stay_days = sum(1 for pred in predictions.values() if pred['trend'] == 'STAY')\n","        avg_confidence = sum(pred['confidence'] for pred in predictions.values()) / len(predictions)\n","\n","        if up_days > down_days:\n","            recommendation = \"üü¢ BULLISH OUTLOOK\"\n","            rec_color = \"#28a745\"\n","            advice = f\"Model predicts {up_days} bullish days. Consider buying opportunities.\"\n","        elif down_days > up_days:\n","            recommendation = \"üî¥ BEARISH OUTLOOK\"\n","            rec_color = \"#dc3545\"\n","            advice = f\"Model predicts {down_days} bearish days. Exercise caution.\"\n","        else:\n","            recommendation = \"üü° NEUTRAL OUTLOOK\"\n","            rec_color = \"#ffc107\"\n","            advice = \"Mixed signals detected. Wait for clearer trends.\"\n","\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","            <h4 style=\"color: {rec_color};\">{recommendation}</h4>\n","            <p>{advice}</p>\n","            <p><strong>Average Confidence:</strong> {avg_confidence:.1%}</p>\n","            <p><strong>Trend Distribution:</strong> {up_days} UP, {down_days} DOWN, {stay_days} STAY</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        # Price range analysis\n","        current_price = stock_info['current_price']\n","        min_pred = min(pred['predicted_close'] for pred in predictions.values())\n","        max_pred = max(pred['predicted_close'] for pred in predictions.values())\n","\n","        price_range = max_pred - min_pred\n","        upside = ((max_pred - current_price) / current_price) * 100\n","        downside = ((current_price - min_pred) / current_price) * 100\n","\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","            <h4>üíπ Price Range Analysis</h4>\n","            <p><strong>Current:</strong> ${current_price:.2f}</p>\n","            <p><strong>Predicted Range:</strong> ${min_pred:.2f} - ${max_pred:.2f}</p>\n","            <p><strong>Volatility:</strong> ${price_range:.2f} ({(price_range/current_price)*100:.1f}%)</p>\n","            <p><strong>Upside Potential:</strong> +{upside:.1f}%</p>\n","            <p><strong>Downside Risk:</strong> -{downside:.1f}%</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Action buttons\n","    col1, col2, col3 = st.columns(3)\n","\n","    with col1:\n","        if st.button(\"üì• Export JSON\", use_container_width=True):\n","            import json\n","            export_data = {\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'recommendation': recommendation\n","            }\n","\n","            st.download_button(\n","                label=\"Download Prediction Report\",\n","                data=json.dumps(export_data, indent=2),\n","                file_name=f\"{stock_info['ticker']}_prediction_{datetime.now().strftime('%Y%m%d_%H%M')}.json\",\n","                mime=\"application/json\"\n","            )\n","\n","    with col2:\n","        if st.button(\"üîÑ New Prediction\", use_container_width=True):\n","            st.session_state.prediction_result = None\n","            st.session_state.current_ticker = \"\"\n","            st.rerun()\n","\n","    with col3:\n","        if st.button(\"üìä Analyze Another\", use_container_width=True):\n","            st.session_state.prediction_result = None\n","            st.rerun()\n","\n","    # Disclaimer\n","    st.markdown(\"---\")\n","    st.warning(\"\"\"\n","    ‚ö†Ô∏è **Disclaimer:** This AI prediction is for educational purposes only.\n","    Always conduct your own research and consult financial advisors before making investment decisions.\n","    Past performance does not guarantee future results.\n","    \"\"\")\n","\n","if __name__ == \"__main__\":\n","    main_dashboard()"],"metadata":{"id":"jrOVIYZVZZ6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 5 : Dashboard dan real time prediction"],"metadata":{"id":"wOGLzqt4UdL9"}},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"zm_5eoFFc-hS"}},{"cell_type":"code","source":["# Install dependencies first\n","pip install streamlit plotly yfinance\n","\n","# Run Streamlit dashboard\n","streamlit run app.py"],"metadata":{"id":"LwVOVgThVwkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================================================================\n","# üöÄ STAGE 5: REAL-TIME PREDICTION API & DASHBOARD\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import joblib\n","from datetime import datetime, timedelta\n","import yfinance as yf\n","import streamlit as st\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","\n","# TensorFlow for LSTM predictions\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","class StockPredictionSystem:\n","    \"\"\"Complete Stock Prediction System with LSTM + Random Forest\"\"\"\n","\n","    def __init__(self):\n","        self.lstm_components = None\n","        self.rf_models = None\n","        self.is_loaded = False\n","\n","    def load_models(self):\n","        \"\"\"Load all trained models and components\"\"\"\n","        print(\"üîÑ Loading trained models...\")\n","\n","        try:\n","            # Load LSTM components\n","            self.lstm_components = self._load_lstm_components()\n","\n","            # Load Random Forest models\n","            self.rf_models = self._load_rf_models()\n","\n","            self.is_loaded = True\n","            print(\"‚úÖ All models loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading models: {str(e)}\")\n","            raise e\n","\n","    def _load_lstm_components(self):\n","        \"\"\"Load LSTM model and components\"\"\"\n","        model_dir = 'unified_lstm_model'\n","\n","        # Custom objects for compatibility\n","        custom_objects = {\n","            'mse': 'mean_squared_error',\n","            'mae': 'mean_absolute_error'\n","        }\n","\n","        # Load LSTM model\n","        lstm_model = load_model(\n","            f'{model_dir}/unified_lstm_model.h5',\n","            custom_objects=custom_objects,\n","            compile=False\n","        )\n","\n","        # Recompile\n","        from tensorflow.keras.optimizers import Adam\n","        lstm_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n","\n","        # Load scalers and encoders\n","        with open(f'{model_dir}/feature_scaler.pkl', 'rb') as f:\n","            feature_scaler = pickle.load(f)\n","\n","        with open(f'{model_dir}/target_scaler.pkl', 'rb') as f:\n","            target_scaler = pickle.load(f)\n","\n","        with open(f'{model_dir}/ticker_encoder.pkl', 'rb') as f:\n","            ticker_encoder = pickle.load(f)\n","\n","        with open(f'{model_dir}/model_info.pkl', 'rb') as f:\n","            model_info = pickle.load(f)\n","\n","        return {\n","            'model': lstm_model,\n","            'feature_scaler': feature_scaler,\n","            'target_scaler': target_scaler,\n","            'ticker_encoder': ticker_encoder,\n","            'model_info': model_info\n","        }\n","\n","    def _load_rf_models(self):\n","        \"\"\"Load Random Forest models\"\"\"\n","        model_dir = 'random_forest_models'\n","\n","        rf_models = {}\n","        for day in range(1, 6):\n","            model_path = f\"{model_dir}/Day{day}_Trend_rf_model.pkl\"\n","            if os.path.exists(model_path):\n","                rf_models[f'Day{day}'] = joblib.load(model_path)\n","\n","        # Load ensemble info\n","        with open(f'{model_dir}/ensemble_info.pkl', 'rb') as f:\n","            ensemble_info = pickle.load(f)\n","\n","        return {\n","            'models': rf_models,\n","            'ensemble_info': ensemble_info\n","        }\n","\n","    def download_stock_data(self, ticker, period=\"2y\"):\n","        \"\"\"Download stock data from yfinance with optimal period\"\"\"\n","        print(f\"üì• Downloading {ticker} data from yfinance...\")\n","\n","        try:\n","            # Download data\n","            stock = yf.Ticker(ticker)\n","            df = stock.history(period=period)\n","\n","            if df.empty:\n","                raise ValueError(f\"No data found for ticker: {ticker}\")\n","\n","            # Reset index to make Date a column\n","            df = df.reset_index()\n","            df['Date'] = pd.to_datetime(df['Date'])\n","\n","            # Ensure we have enough data (minimum 100 days for 60-day sequences)\n","            if len(df) < 100:\n","                print(f\"‚ö†Ô∏è Limited data for {ticker}: {len(df)} days\")\n","\n","            print(f\"‚úÖ Downloaded {ticker}: {len(df)} records from {df['Date'].min().date()} to {df['Date'].max().date()}\")\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error downloading {ticker}: {str(e)}\")\n","            raise e\n","\n","    def calculate_technical_indicators(self, df):\n","        \"\"\"Calculate essential technical indicators\"\"\"\n","        print(\"üîß Calculating technical indicators...\")\n","\n","        try:\n","            # Moving Averages\n","            df['SMA_10'] = df['Close'].rolling(window=10).mean()\n","            df['SMA_20'] = df['Close'].rolling(window=20).mean()\n","            df['EMA_10'] = df['Close'].ewm(span=10).mean()\n","            df['EMA_20'] = df['Close'].ewm(span=20).mean()\n","\n","            # RSI\n","            delta = df['Close'].diff()\n","            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n","            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n","            rs = gain / loss\n","            df['RSI_14'] = 100 - (100 / (1 + rs))\n","\n","            # MACD\n","            ema_12 = df['Close'].ewm(span=12).mean()\n","            ema_26 = df['Close'].ewm(span=26).mean()\n","            df['MACD'] = ema_12 - ema_26\n","            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n","\n","            # Bollinger Bands\n","            bb_period = 20\n","            bb_std = 2\n","            df['BB_Mid'] = df['Close'].rolling(window=bb_period).mean()\n","            bb_std_dev = df['Close'].rolling(window=bb_period).std()\n","            df['BB_High'] = df['BB_Mid'] + (bb_std_dev * bb_std)\n","            df['BB_Low'] = df['BB_Mid'] - (bb_std_dev * bb_std)\n","            df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","            df['BB_Width'] = (df['BB_High'] - df['BB_Low']) / df['BB_Mid']\n","\n","            # Volume indicators\n","            df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n","            df['OBV'] = (df['Volume'] * np.where(df['Close'] > df['Close'].shift(1), 1,\n","                                               np.where(df['Close'] < df['Close'].shift(1), -1, 0))).cumsum()\n","\n","            # Volatility and momentum\n","            df['ATR'] = df[['High', 'Low', 'Close']].apply(\n","                lambda x: max(x['High'] - x['Low'],\n","                            abs(x['High'] - df['Close'].shift(1).iloc[x.name] if pd.notna(df['Close'].shift(1).iloc[x.name]) else x['High']),\n","                            abs(x['Low'] - df['Close'].shift(1).iloc[x.name] if pd.notna(df['Close'].shift(1).iloc[x.name]) else x['Low'])), axis=1\n","            ).rolling(window=14).mean()\n","\n","            df['Volatility_20'] = df['Close'].rolling(window=20).std()\n","            df['Daily_Return'] = df['Close'].pct_change() * 100\n","            df['ROC_10'] = ((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10)) * 100\n","\n","            # Additional features\n","            df['High_Volume'] = (df['Volume'] > df['Volume_MA_10'] * 1.5).astype(int)\n","            df['Low_Volume'] = (df['Volume'] < df['Volume_MA_10'] * 0.5).astype(int)\n","            df['Strong_Momentum'] = (df['ROC_10'] > 5).astype(int)\n","            df['Weak_Momentum'] = (df['ROC_10'] < -5).astype(int)\n","\n","            # Fill NaN values\n","            df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","            print(f\"‚úÖ Technical indicators calculated: {len(df.columns)} total columns\")\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error calculating indicators: {str(e)}\")\n","            raise e\n","\n","    def prepare_lstm_sequence(self, df, ticker):\n","        \"\"\"Prepare 60-day sequence for LSTM prediction\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded! Call load_models() first.\")\n","\n","        feature_columns = self.lstm_components['model_info']['feature_columns']\n","\n","        # Add ticker encoding\n","        try:\n","            ticker_encoded = self.lstm_components['ticker_encoder'].transform([ticker.upper()])[0]\n","        except ValueError:\n","            # If ticker not in training data, use a default or map to similar\n","            print(f\"‚ö†Ô∏è Ticker {ticker} not in training data, using default encoding\")\n","            ticker_encoded = 0\n","\n","        df['Ticker_Encoded'] = ticker_encoded\n","\n","        # Get the most recent 60 days\n","        sequence_length = 60\n","        if len(df) < sequence_length:\n","            raise ValueError(f\"Insufficient data: {len(df)} days, need at least {sequence_length}\")\n","\n","        # Get features that exist in the dataframe\n","        available_features = [col for col in feature_columns if col in df.columns]\n","        missing_features = [col for col in feature_columns if col not in df.columns]\n","\n","        if missing_features:\n","            print(f\"‚ö†Ô∏è Missing features: {missing_features}\")\n","            # Fill missing features with zeros or reasonable defaults\n","            for feature in missing_features:\n","                df[feature] = 0\n","\n","        # Get the last 60 days\n","        recent_data = df[feature_columns].tail(sequence_length)\n","\n","        # Scale the sequence\n","        sequence_scaled = self.lstm_components['feature_scaler'].transform(recent_data.values)\n","\n","        return sequence_scaled.reshape(1, sequence_length, -1)\n","\n","    def generate_lstm_predictions(self, sequence):\n","        \"\"\"Generate 5-day price predictions using LSTM\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded!\")\n","\n","        # Get LSTM prediction (scaled)\n","        lstm_pred_scaled = self.lstm_components['model'].predict(sequence, verbose=0)\n","\n","        # Denormalize to actual prices\n","        lstm_pred_actual = self.lstm_components['target_scaler'].inverse_transform(lstm_pred_scaled)\n","\n","        # Format predictions\n","        predictions = {\n","            'Day1': {'Open': float(lstm_pred_actual[0][0]), 'Close': float(lstm_pred_actual[0][1])},\n","            'Day2': {'Open': float(lstm_pred_actual[0][2]), 'Close': float(lstm_pred_actual[0][3])},\n","            'Day3': {'Open': float(lstm_pred_actual[0][4]), 'Close': float(lstm_pred_actual[0][5])},\n","            'Day4': {'Open': float(lstm_pred_actual[0][6]), 'Close': float(lstm_pred_actual[0][7])},\n","            'Day5': {'Open': float(lstm_pred_actual[0][8]), 'Close': float(lstm_pred_actual[0][9])}\n","        }\n","\n","        return predictions\n","\n","    def prepare_rf_features(self, df, lstm_predictions):\n","        \"\"\"Prepare features for Random Forest trend prediction\"\"\"\n","        latest_data = df.iloc[-1]\n","\n","        # LSTM features\n","        rf_features = {}\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            rf_features[f'LSTM_{day_key}_Open'] = lstm_predictions[day_key]['Open']\n","            rf_features[f'LSTM_{day_key}_Close'] = lstm_predictions[day_key]['Close']\n","\n","        # Technical indicators (current state)\n","        tech_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Volatility_20', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'OBV',\n","            'High_Volume', 'Low_Volume', 'Strong_Momentum', 'Weak_Momentum'\n","        ]\n","\n","        for indicator in tech_indicators:\n","            if indicator in latest_data:\n","                rf_features[f'Tech_{indicator}'] = float(latest_data[indicator])\n","            else:\n","                rf_features[f'Tech_{indicator}'] = 0.0\n","\n","        # Market state features\n","        rf_features.update({\n","            'Current_Close': float(latest_data['Close']),\n","            'Current_Volume': float(latest_data['Volume']),\n","            'Current_High': float(latest_data['High']),\n","            'Current_Low': float(latest_data['Low']),\n","            'Price_Range': float(latest_data['High'] - latest_data['Low']),\n","            'Volume_Ratio': float(latest_data['Volume'] / latest_data.get('Volume_MA_10', latest_data['Volume']))\n","        })\n","\n","        return rf_features\n","\n","    def predict_trends(self, rf_features):\n","        \"\"\"Predict trends using Random Forest models\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded!\")\n","\n","        trends = {}\n","        confidence_scores = {}\n","\n","        # Convert to DataFrame for prediction\n","        feature_df = pd.DataFrame([rf_features])\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","\n","            if day_key in self.rf_models['models']:\n","                model_data = self.rf_models['models'][day_key]\n","                rf_model = model_data['model']\n","\n","                # Ensure feature alignment\n","                model_features = model_data['feature_columns']\n","\n","                # Align features (fill missing with 0)\n","                aligned_features = []\n","                for feature in model_features:\n","                    aligned_features.append(rf_features.get(feature, 0.0))\n","\n","                # Predict\n","                X_pred = np.array([aligned_features])\n","                trend_pred = rf_model.predict(X_pred)[0]\n","                trend_proba = rf_model.predict_proba(X_pred)[0]\n","\n","                trends[day_key] = trend_pred\n","                confidence_scores[day_key] = float(max(trend_proba))\n","            else:\n","                trends[day_key] = 'UNKNOWN'\n","                confidence_scores[day_key] = 0.0\n","\n","        return trends, confidence_scores\n","\n","    def predict_stock(self, ticker, period=\"2y\"):\n","        \"\"\"Complete stock prediction pipeline\"\"\"\n","        print(f\"üéØ Starting prediction for {ticker}...\")\n","\n","        try:\n","            # Download data\n","            df = self.download_stock_data(ticker, period)\n","\n","            # Calculate technical indicators\n","            df = self.calculate_technical_indicators(df)\n","\n","            # Prepare LSTM sequence\n","            lstm_sequence = self.prepare_lstm_sequence(df, ticker)\n","\n","            # Generate LSTM predictions\n","            lstm_predictions = self.generate_lstm_predictions(lstm_sequence)\n","\n","            # Prepare RF features\n","            rf_features = self.prepare_rf_features(df, lstm_predictions)\n","\n","            # Predict trends\n","            trends, confidence_scores = self.predict_trends(rf_features)\n","\n","            # Generate prediction dates (starting from tomorrow)\n","            today = datetime.now().date()\n","            prediction_dates = [(today + timedelta(days=i)) for i in range(1, 6)]\n","\n","            # Combine results\n","            predictions = {}\n","            for i, day_key in enumerate(['Day1', 'Day2', 'Day3', 'Day4', 'Day5']):\n","                predictions[day_key] = {\n","                    'date': prediction_dates[i].strftime('%Y-%m-%d'),\n","                    'predicted_open': lstm_predictions[day_key]['Open'],\n","                    'predicted_close': lstm_predictions[day_key]['Close'],\n","                    'trend': trends[day_key],\n","                    'confidence': confidence_scores[day_key]\n","                }\n","\n","            # Add current stock info\n","            current_data = df.iloc[-1]\n","            stock_info = {\n","                'ticker': ticker.upper(),\n","                'current_price': float(current_data['Close']),\n","                'current_date': current_data['Date'].strftime('%Y-%m-%d'),\n","                'volume': float(current_data['Volume']),\n","                'market_cap': 'N/A',  # Would need additional API for this\n","                'prediction_generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')\n","            }\n","\n","            result = {\n","                'status': 'SUCCESS',\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'historical_data': df.tail(30).to_dict('records')  # Last 30 days for charts\n","            }\n","\n","            print(f\"‚úÖ Prediction completed for {ticker}\")\n","            return result\n","\n","        except Exception as e:\n","            print(f\"‚ùå Prediction failed for {ticker}: {str(e)}\")\n","            return {\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'ticker': ticker\n","            }\n","\n","# ===================================================================\n","# üñ•Ô∏è STREAMLIT DASHBOARD\n","# ===================================================================\n","\n","def create_streamlit_dashboard():\n","    \"\"\"Create Streamlit dashboard for stock predictions\"\"\"\n","\n","    st.set_page_config(\n","        page_title=\"üöÄ AI Stock Predictor\",\n","        page_icon=\"üìà\",\n","        layout=\"wide\",\n","        initial_sidebar_state=\"expanded\"\n","    )\n","\n","    # Custom CSS\n","    st.markdown(\"\"\"\n","    <style>\n","    .main-header {\n","        font-size: 2.5rem;\n","        color: #1f77b4;\n","        text-align: center;\n","        margin-bottom: 2rem;\n","    }\n","    .prediction-card {\n","        background-color: #f0f2f6;\n","        padding: 1rem;\n","        border-radius: 0.5rem;\n","        margin: 0.5rem 0;\n","    }\n","    .trend-up {\n","        color: #00ff00;\n","        font-weight: bold;\n","    }\n","    .trend-down {\n","        color: #ff0000;\n","        font-weight: bold;\n","    }\n","    .trend-stay {\n","        color: #ffa500;\n","        font-weight: bold;\n","    }\n","    </style>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Header\n","    st.markdown('<h1 class=\"main-header\">üöÄ AI Stock Predictor</h1>', unsafe_allow_html=True)\n","    st.markdown('<p style=\"text-align: center; font-size: 1.2rem;\">LSTM + Random Forest | 5-Day Stock Price & Trend Prediction</p>', unsafe_allow_html=True)\n","\n","    # Sidebar\n","    st.sidebar.header(\"üìä Prediction Settings\")\n","\n","    # Initialize session state\n","    if 'prediction_system' not in st.session_state:\n","        st.session_state.prediction_system = StockPredictionSystem()\n","        st.session_state.models_loaded = False\n","\n","    # Load models button\n","    if not st.session_state.models_loaded:\n","        if st.sidebar.button(\"üîÑ Load AI Models\", type=\"primary\"):\n","            with st.spinner(\"Loading trained models...\"):\n","                try:\n","                    st.session_state.prediction_system.load_models()\n","                    st.session_state.models_loaded = True\n","                    st.sidebar.success(\"‚úÖ Models loaded successfully!\")\n","                except Exception as e:\n","                    st.sidebar.error(f\"‚ùå Error loading models: {str(e)}\")\n","                    return\n","\n","    if not st.session_state.models_loaded:\n","        st.warning(\"üîÑ Please load the AI models first using the sidebar button.\")\n","        return\n","\n","    # Stock input\n","    ticker_input = st.sidebar.text_input(\n","        \"üìà Enter Stock Ticker (yfinance format)\",\n","        placeholder=\"e.g., AAPL, GOOGL, TSLA, BBCA.JK\",\n","        help=\"Use Yahoo Finance ticker format. For Indonesian stocks, add .JK (e.g., BBCA.JK)\"\n","    )\n","\n","    # Data period selection\n","    period_options = {\n","        \"2 Years\": \"2y\",\n","        \"1 Year\": \"1y\",\n","        \"6 Months\": \"6mo\",\n","        \"3 Months\": \"3mo\"\n","    }\n","\n","    selected_period = st.sidebar.selectbox(\n","        \"üìÖ Historical Data Period\",\n","        options=list(period_options.keys()),\n","        index=0,\n","        help=\"More data = better predictions, but slower processing\"\n","    )\n","\n","    # Predict button\n","    if st.sidebar.button(\"üéØ Generate Prediction\", type=\"primary\"):\n","        if not ticker_input:\n","            st.sidebar.error(\"Please enter a stock ticker!\")\n","            return\n","\n","        # Main prediction area\n","        with st.container():\n","            with st.spinner(f\"üîç Analyzing {ticker_input.upper()}...\"):\n","                result = st.session_state.prediction_system.predict_stock(\n","                    ticker_input,\n","                    period_options[selected_period]\n","                )\n","\n","            if result['status'] == 'SUCCESS':\n","                display_prediction_results(result)\n","            else:\n","                st.error(f\"‚ùå Prediction failed: {result.get('error', 'Unknown error')}\")\n","\n","def display_prediction_results(result):\n","    \"\"\"Display prediction results in Streamlit\"\"\"\n","\n","    stock_info = result['stock_info']\n","    predictions = result['predictions']\n","    historical_data = result['historical_data']\n","\n","    # Stock info header\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\n","            label=\"üè¢ Stock\",\n","            value=stock_info['ticker']\n","        )\n","\n","    with col2:\n","        st.metric(\n","            label=\"üí∞ Current Price\",\n","            value=f\"${stock_info['current_price']:.2f}\"\n","        )\n","\n","    with col3:\n","        st.metric(\n","            label=\"üìä Volume\",\n","            value=f\"{stock_info['volume']:,.0f}\"\n","        )\n","\n","    with col4:\n","        st.metric(\n","            label=\"üìÖ Last Update\",\n","            value=stock_info['current_date']\n","        )\n","\n","    st.markdown(\"---\")\n","\n","    # 5-Day Predictions\n","    st.subheader(\"üîÆ 5-Day AI Predictions\")\n","\n","    # Create prediction table\n","    pred_data = []\n","    for day_key, pred in predictions.items():\n","        trend_emoji = {\"UP\": \"üìà\", \"DOWN\": \"üìâ\", \"STAY\": \"‚û°Ô∏è\"}.get(pred['trend'], \"‚ùì\")\n","        trend_color = {\"UP\": \"trend-up\", \"DOWN\": \"trend-down\", \"STAY\": \"trend-stay\"}.get(pred['trend'], \"\")\n","\n","        pred_data.append({\n","            \"Date\": pred['date'],\n","            \"Predicted Open\": f\"${pred['predicted_open']:.2f}\",\n","            \"Predicted Close\": f\"${pred['predicted_close']:.2f}\",\n","            \"Daily Change\": f\"${pred['predicted_close'] - pred['predicted_open']:.2f}\",\n","            \"Trend\": f\"{trend_emoji} {pred['trend']}\",\n","            \"Confidence\": f\"{pred['confidence']:.1%}\"\n","        })\n","\n","    pred_df = pd.DataFrame(pred_data)\n","    st.dataframe(pred_df, use_container_width=True)\n","\n","    # Price chart\n","    st.subheader(\"üìà Price Prediction Chart\")\n","\n","    # Prepare chart data\n","    historical_df = pd.DataFrame(historical_data)\n","    historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n","\n","    # Create price chart\n","    fig = make_subplots(\n","        rows=2, cols=1,\n","        shared_xaxes=True,\n","        vertical_spacing=0.1,\n","        subplot_titles=('Price Prediction', 'Volume'),\n","        row_heights=[0.7, 0.3]\n","    )\n","\n","    # Historical prices\n","    fig.add_trace(\n","        go.Scatter(\n","            x=historical_df['Date'],\n","            y=historical_df['Close'],\n","            mode='lines',\n","            name='Historical Close',\n","            line=dict(color='blue', width=2)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Predicted prices\n","    pred_dates = [datetime.strptime(pred['date'], '%Y-%m-%d') for pred in predictions.values()]\n","    pred_opens = [pred['predicted_open'] for pred in predictions.values()]\n","    pred_closes = [pred['predicted_close'] for pred in predictions.values()]\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=pred_dates,\n","            y=pred_opens,\n","            mode='lines+markers',\n","            name='Predicted Open',\n","            line=dict(color='orange', width=2, dash='dash'),\n","            marker=dict(size=8)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=pred_dates,\n","            y=pred_closes,\n","            mode='lines+markers',\n","            name='Predicted Close',\n","            line=dict(color='red', width=2, dash='dash'),\n","            marker=dict(size=8)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Volume\n","    fig.add_trace(\n","        go.Bar(\n","            x=historical_df['Date'],\n","            y=historical_df['Volume'],\n","            name='Volume',\n","            marker_color='lightblue'\n","        ),\n","        row=2, col=1\n","    )\n","\n","    fig.update_layout(\n","        title=f\"{stock_info['ticker']} - 5-Day AI Prediction\",\n","        xaxis_title=\"Date\",\n","        yaxis_title=\"Price ($)\",\n","        height=600,\n","        showlegend=True\n","    )\n","\n","    st.plotly_chart(fig, use_container_width=True)\n","\n","    # Trend analysis\n","    st.subheader(\"üìä Trend Analysis\")\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        # Trend distribution\n","        trend_counts = {}\n","        for pred in predictions.values():\n","            trend = pred['trend']\n","            trend_counts[trend] = trend_counts.get(trend, 0) + 1\n","\n","        fig_pie = px.pie(\n","            values=list(trend_counts.values()),\n","            names=list(trend_counts.keys()),\n","            title=\"5-Day Trend Distribution\",\n","            color_discrete_map={\n","                'UP': '#00ff00',\n","                'DOWN': '#ff0000',\n","                'STAY': '#ffa500'\n","            }\n","        )\n","\n","        st.plotly_chart(fig_pie, use_container_width=True)\n","\n","    with col2:\n","        # Confidence scores\n","        confidence_data = [\n","            {\"Day\": day_key, \"Confidence\": pred['confidence']}\n","            for day_key, pred in predictions.items()\n","        ]\n","\n","        fig_conf = px.bar(\n","            pd.DataFrame(confidence_data),\n","            x='Day',\n","            y='Confidence',\n","            title=\"Prediction Confidence by Day\",\n","            color='Confidence',\n","            color_continuous_scale='viridis'\n","        )\n","\n","        fig_conf.update_layout(yaxis_title=\"Confidence Score\")\n","        st.plotly_chart(fig_conf, use_container_width=True)\n","\n","    # Investment recommendations\n","    st.subheader(\"üí° AI Investment Insights\")\n","\n","    # Calculate overall trend\n","    up_days = sum(1 for pred in predictions.values() if pred['trend'] == 'UP')\n","    down_days = sum(1 for pred in predictions.values() if pred['trend'] == 'DOWN')\n","    stay_days = sum(1 for pred in predictions.values() if pred['trend'] == 'STAY')\n","\n","    avg_confidence = sum(pred['confidence'] for pred in predictions.values()) / len(predictions)\n","\n","    if up_days > down_days:\n","        recommendation = \"üü¢ BULLISH\"\n","        recommendation_text = f\"Model predicts {up_days} UP days vs {down_days} DOWN days. Consider BUYING.\"\n","    elif down_days > up_days:\n","        recommendation = \"üî¥ BEARISH\"\n","        recommendation_text = f\"Model predicts {down_days} DOWN days vs {up_days} UP days. Consider SELLING or avoiding.\"\n","    else:\n","        recommendation = \"üü° NEUTRAL\"\n","        recommendation_text = f\"Mixed signals. {up_days} UP, {down_days} DOWN, {stay_days} STAY days. HOLD or wait for clearer signals.\"\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","        <h3>{recommendation}</h3>\n","        <p>{recommendation_text}</p>\n","        <p><strong>Average Confidence:</strong> {avg_confidence:.1%}</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","        <h3>üìã Key Metrics</h3>\n","        <ul>\n","        <li><strong>Bullish Days:</strong> {up_days}/5</li>\n","        <li><strong>Bearish Days:</strong> {down_days}/5</li>\n","        <li><strong>Neutral Days:</strong> {stay_days}/5</li>\n","        <li><strong>Model Confidence:</strong> {avg_confidence:.1%}</li>\n","        </ul>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Disclaimer\n","    st.markdown(\"---\")\n","    st.markdown(\"\"\"\n","    **‚ö†Ô∏è Disclaimer:** This AI prediction is for educational purposes only.\n","    Always conduct your own research and consider consulting with financial advisors before making investment decisions.\n","    Past performance does not guarantee future results.\n","    \"\"\")\n","\n","    # Export option\n","    if st.button(\"üì• Export Predictions as JSON\"):\n","        st.download_button(\n","            label=\"Download JSON\",\n","            data=pd.Series(result).to_json(),\n","            file_name=f\"{stock_info['ticker']}_prediction_{datetime.now().strftime('%Y%m%d')}.json\",\n","            mime=\"application/json\"\n","        )\n","\n","# ===================================================================\n","# üöÄ MAIN EXECUTION\n","# ===================================================================\n","\n","if __name__ == \"__main__\":\n","    # Check if running in Streamlit\n","    try:\n","        import streamlit as st\n","        create_streamlit_dashboard()\n","    except ImportError:\n","        print(\"Streamlit not found. Running as standalone API...\")\n","\n","        # Standalone API example\n","        predictor = StockPredictionSystem()\n","        predictor.load_models()\n","\n","        # Example prediction\n","        test_ticker = \"AAPL\"\n","        result = predictor.predict_stock(test_ticker)\n","\n","        if result['status'] == 'SUCCESS':\n","            print(f\"\\nüéâ Prediction successful for {test_ticker}!\")\n","            for day_key, pred in result['predictions'].items():\n","                print(f\"{day_key} ({pred['date']}): {pred['trend']} - Close: ${pred['predicted_close']:.2f} (Confidence: {pred['confidence']:.1%})\")\n","        else:\n","            print(f\"‚ùå Prediction failed: {result['error']}\")\n","\n","print(\"\\nüéØ STAGE 5 PREDICTION SYSTEM READY!\")"],"metadata":{"id":"I26AiVSAU2CU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 6 : Evaluasi Model"],"metadata":{"id":"4WJaKowEdDz5"}},{"cell_type":"markdown","source":["## LSTM Evaluasi"],"metadata":{"id":"9oFTsHHLdYgx"}},{"cell_type":"code","source":["\"\"\"\n","===================================================================\n","üß† SIMPLE LSTM MAE EVALUATION\n","===================================================================\n","User: wasirawasenju\n","Date: 2025-07-25 07:28:21 UTC\n","Environment: Google Colab\n","Focus: LSTM MAE + Visualization ONLY\n","===================================================================\n","\"\"\"\n","\n","# ===================================================================\n","# üì¶ SIMPLE SETUP\n","# ===================================================================\n","\n","!pip install -q plotly tensorflow pandas numpy scikit-learn\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import warnings\n","import pickle\n","import os\n","from datetime import datetime\n","from sklearn.metrics import mean_absolute_error\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","warnings.filterwarnings('ignore')\n","tf.get_logger().setLevel('ERROR')\n","\n","print(\"üß† SIMPLE LSTM MAE EVALUATION\")\n","print(\"=\"*60)\n","print(f\"üë§ User: wasirawasenju\")\n","print(f\"üìÖ Date: 2025-07-25 07:28:21 UTC\")\n","print(f\"üéØ Focus: MAE + Visualization\")\n","print(\"=\"*60)\n","\n","# ===================================================================\n","# üîç QUICK FILE CHECK\n","# ===================================================================\n","\n","def quick_check():\n","    \"\"\"Quick check of LSTM files\"\"\"\n","    print(\"üìÅ Quick file check...\")\n","\n","    lstm_dir = 'unified_lstm_model'\n","    if os.path.exists(lstm_dir):\n","        files = os.listdir(lstm_dir)\n","        print(f\"   ‚úÖ Found {len(files)} files in {lstm_dir}/\")\n","\n","        key_files = ['unified_lstm_model.h5', 'feature_scaler.pkl', 'target_scaler.pkl']\n","        for file in key_files:\n","            if file in files:\n","                print(f\"      ‚úÖ {file}\")\n","            else:\n","                print(f\"      ‚ùå {file}\")\n","        return True\n","    else:\n","        print(f\"   ‚ùå {lstm_dir}/ not found\")\n","        return False\n","\n","quick_check()\n","\n","# ===================================================================\n","# üß† SIMPLE LSTM EVALUATOR\n","# ===================================================================\n","\n","class SimpleLSTMEvaluator:\n","    \"\"\"Super simple LSTM MAE evaluator\"\"\"\n","\n","    def __init__(self):\n","        self.model = None\n","        self.feature_scaler = None\n","        self.target_scaler = None\n","        self.model_info = None\n","        self.mae_results = {}\n","\n","    def load_lstm_simple(self):\n","        \"\"\"Load LSTM components - simplified\"\"\"\n","        print(\"üîÑ Loading LSTM...\")\n","\n","        try:\n","            # Load model\n","            print(\"   üì¶ Loading model...\")\n","            self.model = load_model('unified_lstm_model/unified_lstm_model.h5', compile=False)\n","\n","            # Recompile\n","            from tensorflow.keras.optimizers import Adam\n","            self.model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n","            print(\"      ‚úÖ Model loaded & compiled\")\n","\n","            # Load scalers\n","            print(\"   üì¶ Loading scalers...\")\n","            with open('unified_lstm_model/feature_scaler.pkl', 'rb') as f:\n","                self.feature_scaler = pickle.load(f)\n","\n","            with open('unified_lstm_model/target_scaler.pkl', 'rb') as f:\n","                self.target_scaler = pickle.load(f)\n","\n","            with open('unified_lstm_model/model_info.pkl', 'rb') as f:\n","                self.model_info = pickle.load(f)\n","\n","            print(\"      ‚úÖ Scalers loaded\")\n","            print(f\"      üìä Feature count: {len(self.model_info['feature_columns'])}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"      ‚ùå Error: {str(e)}\")\n","            return False\n","\n","    def create_simple_test_data(self):\n","        \"\"\"Create simple test data\"\"\"\n","        print(\"üìä Creating simple test data...\")\n","\n","        # Simple synthetic data for testing\n","        np.random.seed(42)\n","\n","        # Create sample sequences\n","        test_sequences = []\n","        test_targets = []\n","\n","        feature_count = len(self.model_info['feature_columns'])\n","\n","        for i in range(10):  # Just 10 test cases\n","            # Random sequence data\n","            sequence = np.random.randn(60, feature_count)\n","            target = np.random.randn(10) * 100 + 150  # Price-like values\n","\n","            test_sequences.append(sequence)\n","            test_targets.append(target)\n","\n","        print(f\"   ‚úÖ Created {len(test_sequences)} test sequences\")\n","        return test_sequences, test_targets\n","\n","    def evaluate_mae_simple(self):\n","        \"\"\"Simple MAE evaluation\"\"\"\n","        print(\"\\nüéØ EVALUATING MAE...\")\n","\n","        if not self.model:\n","            print(\"‚ùå Model not loaded\")\n","            return False\n","\n","        # Get test data\n","        test_sequences, test_targets = self.create_simple_test_data()\n","\n","        # Scale test sequences\n","        scaled_sequences = []\n","        for seq in test_sequences:\n","            scaled_seq = self.feature_scaler.transform(seq)\n","            scaled_sequences.append(scaled_seq)\n","\n","        scaled_sequences = np.array(scaled_sequences)\n","\n","        print(f\"   üìä Input shape: {scaled_sequences.shape}\")\n","\n","        # Generate predictions\n","        print(\"   üîÑ Generating predictions...\")\n","        pred_scaled = self.model.predict(scaled_sequences, verbose=0)\n","\n","        # Denormalize predictions\n","        predictions = self.target_scaler.inverse_transform(pred_scaled)\n","\n","        print(f\"   üìä Predictions shape: {predictions.shape}\")\n","\n","        # Calculate MAE for each test case\n","        mae_values = []\n","        for i, (pred, actual) in enumerate(zip(predictions, test_targets)):\n","            mae = mean_absolute_error(actual, pred)\n","            mae_values.append(mae)\n","            print(f\"      Test {i+1}: MAE = {mae:.4f}\")\n","\n","        # Overall MAE\n","        all_predictions = predictions.flatten()\n","        all_actuals = np.array(test_targets).flatten()\n","        overall_mae = mean_absolute_error(all_actuals, all_predictions)\n","\n","        # Store results\n","        self.mae_results = {\n","            'individual_mae': mae_values,\n","            'overall_mae': overall_mae,\n","            'predictions': predictions,\n","            'actuals': test_targets,\n","            'mean_mae': np.mean(mae_values),\n","            'std_mae': np.std(mae_values),\n","            'min_mae': np.min(mae_values),\n","            'max_mae': np.max(mae_values)\n","        }\n","\n","        print(f\"\\nüìä MAE RESULTS:\")\n","        print(f\"   üéØ Overall MAE: {overall_mae:.4f}\")\n","        print(f\"   üìä Mean MAE: {np.mean(mae_values):.4f}\")\n","        print(f\"   üìä Std MAE: {np.std(mae_values):.4f}\")\n","        print(f\"   üìä Min MAE: {np.min(mae_values):.4f}\")\n","        print(f\"   üìä Max MAE: {np.max(mae_values):.4f}\")\n","\n","        return True\n","\n","    def create_mae_visualization(self):\n","        \"\"\"Create simple MAE visualization\"\"\"\n","        print(\"\\nüìä CREATING MAE VISUALIZATION...\")\n","\n","        if not self.mae_results:\n","            print(\"‚ùå No MAE results to visualize\")\n","            return\n","\n","        # Create 2x2 subplot\n","        fig = make_subplots(\n","            rows=2, cols=2,\n","            subplot_titles=(\n","                'MAE by Test Case',\n","                'Predictions vs Actuals (Sample)',\n","                'MAE Distribution',\n","                'Overall MAE Summary'\n","            ),\n","            specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n","                   [{\"type\": \"histogram\"}, {\"type\": \"indicator\"}]]\n","        )\n","\n","        # 1. MAE by test case\n","        test_cases = [f\"Test {i+1}\" for i in range(len(self.mae_results['individual_mae']))]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=test_cases,\n","                y=self.mae_results['individual_mae'],\n","                name='MAE',\n","                marker_color='blue'\n","            ),\n","            row=1, col=1\n","        )\n","\n","        # 2. Predictions vs Actuals (first test case)\n","        if len(self.mae_results['predictions']) > 0:\n","            sample_pred = self.mae_results['predictions'][0]\n","            sample_actual = self.mae_results['actuals'][0]\n","\n","            fig.add_trace(\n","                go.Scatter(\n","                    x=list(range(len(sample_actual))),\n","                    y=sample_actual,\n","                    mode='lines+markers',\n","                    name='Actual',\n","                    line=dict(color='red'),\n","                    marker=dict(size=8)\n","                ),\n","                row=1, col=2\n","            )\n","\n","            fig.add_trace(\n","                go.Scatter(\n","                    x=list(range(len(sample_pred))),\n","                    y=sample_pred,\n","                    mode='lines+markers',\n","                    name='Predicted',\n","                    line=dict(color='blue', dash='dash'),\n","                    marker=dict(size=8)\n","                ),\n","                row=1, col=2\n","            )\n","\n","        # 3. MAE distribution\n","        fig.add_trace(\n","            go.Histogram(\n","                x=self.mae_results['individual_mae'],\n","                name='MAE Distribution',\n","                marker_color='green',\n","                nbinsx=5\n","            ),\n","            row=2, col=1\n","        )\n","\n","        # 4. Overall MAE indicator\n","        fig.add_trace(\n","            go.Indicator(\n","                mode=\"gauge+number\",\n","                value=self.mae_results['overall_mae'],\n","                title={'text': \"Overall MAE\"},\n","                gauge={\n","                    'axis': {'range': [0, self.mae_results['max_mae'] * 1.2]},\n","                    'bar': {'color': \"darkblue\"},\n","                    'steps': [\n","                        {'range': [0, self.mae_results['overall_mae'] * 0.5], 'color': \"lightgray\"},\n","                        {'range': [self.mae_results['overall_mae'] * 0.5, self.mae_results['overall_mae']], 'color': \"gray\"}\n","                    ],\n","                    'threshold': {\n","                        'line': {'color': \"red\", 'width': 4},\n","                        'thickness': 0.75,\n","                        'value': self.mae_results['overall_mae']\n","                    }\n","                }\n","            ),\n","            row=2, col=2\n","        )\n","\n","        fig.update_layout(\n","            title=\"üß† LSTM MAE Evaluation Results\",\n","            height=800,\n","            showlegend=True\n","        )\n","\n","        fig.show()\n","\n","        # Simple bar chart for MAE summary\n","        print(\"   üìä Creating MAE summary chart...\")\n","\n","        fig2 = go.Figure()\n","\n","        metrics = ['Overall MAE', 'Mean MAE', 'Min MAE', 'Max MAE']\n","        values = [\n","            self.mae_results['overall_mae'],\n","            self.mae_results['mean_mae'],\n","            self.mae_results['min_mae'],\n","            self.mae_results['max_mae']\n","        ]\n","\n","        fig2.add_trace(\n","            go.Bar(\n","                x=metrics,\n","                y=values,\n","                marker_color=['red', 'blue', 'green', 'orange'],\n","                text=[f\"{val:.4f}\" for val in values],\n","                textposition='auto'\n","            )\n","        )\n","\n","        fig2.update_layout(\n","            title=\"üìä MAE Summary Statistics\",\n","            xaxis_title=\"Metrics\",\n","            yaxis_title=\"MAE Value\",\n","            height=400\n","        )\n","\n","        fig2.show()\n","\n","        print(\"   ‚úÖ Visualization created!\")\n","\n","# ===================================================================\n","# üöÄ SIMPLE MAIN EXECUTION\n","# ===================================================================\n","\n","def run_simple_evaluation():\n","    \"\"\"Run simple LSTM MAE evaluation\"\"\"\n","    print(\"\\nüöÄ RUNNING SIMPLE LSTM MAE EVALUATION\")\n","    print(\"=\"*50)\n","\n","    # Initialize\n","    evaluator = SimpleLSTMEvaluator()\n","\n","    # Step 1: Load LSTM\n","    print(\"üîÑ Step 1: Loading LSTM...\")\n","    if not evaluator.load_lstm_simple():\n","        print(\"‚ùå Cannot load LSTM model\")\n","        return\n","\n","    # Step 2: Evaluate MAE\n","    print(\"\\nüîÑ Step 2: Evaluating MAE...\")\n","    if not evaluator.evaluate_mae_simple():\n","        print(\"‚ùå MAE evaluation failed\")\n","        return\n","\n","    # Step 3: Create visualization\n","    print(\"\\nüîÑ Step 3: Creating visualization...\")\n","    evaluator.create_mae_visualization()\n","\n","    # Final summary\n","    print(f\"\\nüéâ EVALUATION COMPLETED!\")\n","    print(f\"üìä Final MAE: {evaluator.mae_results['overall_mae']:.4f}\")\n","    print(f\"üìä Test Cases: {len(evaluator.mae_results['individual_mae'])}\")\n","\n","    return evaluator.mae_results\n","\n","# ===================================================================\n","# üéØ RUN IT!\n","# ===================================================================\n","\n","# Execute the simple evaluation\n","results = run_simple_evaluation()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"üß† SIMPLE LSTM MAE EVALUATION COMPLETE!\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1KGEpZxVdHl6","executionInfo":{"status":"ok","timestamp":1753429396286,"user_tz":-420,"elapsed":11033,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"4c8aa0f5-9963-475a-de87-93283405b603"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["üß† SIMPLE LSTM MAE EVALUATION\n","============================================================\n","üë§ User: wasirawasenju\n","üìÖ Date: 2025-07-25 07:28:21 UTC\n","üéØ Focus: MAE + Visualization\n","============================================================\n","üìÅ Quick file check...\n","   ‚úÖ Found 5 files in unified_lstm_model/\n","      ‚úÖ unified_lstm_model.h5\n","      ‚úÖ feature_scaler.pkl\n","      ‚úÖ target_scaler.pkl\n","\n","üöÄ RUNNING SIMPLE LSTM MAE EVALUATION\n","==================================================\n","üîÑ Step 1: Loading LSTM...\n","üîÑ Loading LSTM...\n","   üì¶ Loading model...\n","      ‚úÖ Model loaded & compiled\n","   üì¶ Loading scalers...\n","      ‚úÖ Scalers loaded\n","      üìä Feature count: 21\n","\n","üîÑ Step 2: Evaluating MAE...\n","\n","üéØ EVALUATING MAE...\n","üìä Creating simple test data...\n","   ‚úÖ Created 10 test sequences\n","   üìä Input shape: (10, 60, 21)\n","   üîÑ Generating predictions...\n","   üìä Predictions shape: (10, 10)\n","      Test 1: MAE = 77.4268\n","      Test 2: MAE = 89.2965\n","      Test 3: MAE = 91.1387\n","      Test 4: MAE = 89.5054\n","      Test 5: MAE = 82.1781\n","      Test 6: MAE = 117.2152\n","      Test 7: MAE = 97.4072\n","      Test 8: MAE = 83.0177\n","      Test 9: MAE = 94.0276\n","      Test 10: MAE = 65.3021\n","\n","üìä MAE RESULTS:\n","   üéØ Overall MAE: 88.6515\n","   üìä Mean MAE: 88.6515\n","   üìä Std MAE: 12.9368\n","   üìä Min MAE: 65.3021\n","   üìä Max MAE: 117.2152\n","\n","üîÑ Step 3: Creating visualization...\n","\n","üìä CREATING MAE VISUALIZATION...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f38104e7-2eb5-4f51-94e6-a09196a24b28\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f38104e7-2eb5-4f51-94e6-a09196a24b28\")) {                    Plotly.newPlot(                        \"f38104e7-2eb5-4f51-94e6-a09196a24b28\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"MAE\",\"x\":[\"Test 1\",\"Test 2\",\"Test 3\",\"Test 4\",\"Test 5\",\"Test 6\",\"Test 7\",\"Test 8\",\"Test 9\",\"Test 10\"],\"y\":[77.4268327489437,89.29654361730451,91.13868678500589,89.50540772814817,82.17811677582716,117.21517469369928,97.40718812504872,83.0177223983969,94.02760165618679,65.30206595630938],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"red\"},\"marker\":{\"size\":8},\"mode\":\"lines+markers\",\"name\":\"Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[205.83269125217225,157.60053914124114,203.87559924463693,57.93264069206337,166.9360824036078,8.628550309501975,138.87739386512996,59.60923585352103,76.44700574102335,273.60931752104375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"marker\":{\"size\":8},\"mode\":\"lines+markers\",\"name\":\"Predicted\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[88.49317,87.75598,87.20838,86.45105,87.10689,85.75401,91.48683,88.75182,92.94491,81.6964],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"green\"},\"name\":\"MAE Distribution\",\"nbinsx\":5,\"x\":[77.4268327489437,89.29654361730451,91.13868678500589,89.50540772814817,82.17811677582716,117.21517469369928,97.40718812504872,83.0177223983969,94.02760165618679,65.30206595630938],\"type\":\"histogram\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"gauge\":{\"axis\":{\"range\":[0,140.65820963243914]},\"bar\":{\"color\":\"darkblue\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,44.325767024243525]},{\"color\":\"gray\",\"range\":[44.325767024243525,88.65153404848705]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":88.65153404848705}},\"mode\":\"gauge+number\",\"title\":{\"text\":\"Overall MAE\"},\"value\":88.65153404848705,\"type\":\"indicator\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,0.375]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"MAE by Test Case\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Predictions vs Actuals (Sample)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"MAE Distribution\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Overall MAE Summary\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"üß† LSTM MAE Evaluation Results\"},\"height\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f38104e7-2eb5-4f51-94e6-a09196a24b28');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   üìä Creating MAE summary chart...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\")) {                    Plotly.newPlot(                        \"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\",                        [{\"marker\":{\"color\":[\"red\",\"blue\",\"green\",\"orange\"]},\"text\":[\"88.6515\",\"88.6515\",\"65.3021\",\"117.2152\"],\"textposition\":\"auto\",\"x\":[\"Overall MAE\",\"Mean MAE\",\"Min MAE\",\"Max MAE\"],\"y\":[88.65153404848705,88.65153404848704,65.30206595630938,117.21517469369928],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"üìä MAE Summary Statistics\"},\"xaxis\":{\"title\":{\"text\":\"Metrics\"}},\"yaxis\":{\"title\":{\"text\":\"MAE Value\"}},\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('bb216dc0-9ad0-4fa5-99c8-768016fb8a8b');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   ‚úÖ Visualization created!\n","\n","üéâ EVALUATION COMPLETED!\n","üìä Final MAE: 88.6515\n","üìä Test Cases: 10\n","\n","============================================================\n","üß† SIMPLE LSTM MAE EVALUATION COMPLETE!\n","============================================================\n"]}]},{"cell_type":"markdown","source":["## RF evaluasi"],"metadata":{"id":"6LROE10kixmn"}},{"cell_type":"code","source":["\"\"\"\n","===================================================================\n","üå≤ SIMPLE RANDOM FOREST F1 SCORE EVALUATION\n","===================================================================\n","User: wasirawasenju\n","Date: 2025-07-25 07:43:56 UTC\n","Environment: Google Colab\n","Focus: RF F1 Score ONLY - Simple Version\n","===================================================================\n","\"\"\"\n","\n","# ===================================================================\n","# üì¶ SIMPLE SETUP\n","# ===================================================================\n","\n","!pip install -q plotly pandas numpy scikit-learn joblib\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import warnings\n","import pickle\n","import joblib\n","import os\n","from datetime import datetime\n","from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n","\n","warnings.filterwarnings('ignore')\n","\n","print(\"üå≤ SIMPLE RF F1 SCORE EVALUATION\")\n","print(\"=\"*60)\n","print(f\"üë§ User: wasirawasenju\")\n","print(f\"üìÖ Date: 2025-07-25 07:43:56 UTC\")\n","print(f\"üéØ Focus: F1 Score + Simple Visualization\")\n","print(\"=\"*60)\n","\n","# ===================================================================\n","# üîç QUICK RF FILE CHECK\n","# ===================================================================\n","\n","def quick_rf_check():\n","    \"\"\"Quick check of RF model files\"\"\"\n","    print(\"üìÅ Quick RF file check...\")\n","\n","    rf_dir = 'random_forest_models'\n","    if os.path.exists(rf_dir):\n","        files = os.listdir(rf_dir)\n","        print(f\"   ‚úÖ Found {len(files)} files in {rf_dir}/\")\n","\n","        # Check for Day models\n","        day_models = []\n","        for day in range(1, 6):\n","            day_file = f\"Day{day}_Trend_rf_model.pkl\"\n","            if day_file in files:\n","                day_models.append(f\"Day{day}\")\n","                print(f\"      ‚úÖ {day_file}\")\n","            else:\n","                print(f\"      ‚ùå {day_file}\")\n","\n","        return len(day_models) > 0, day_models\n","    else:\n","        print(f\"   ‚ùå {rf_dir}/ not found\")\n","        return False, []\n","\n","rf_available, available_days = quick_rf_check()\n","\n","# ===================================================================\n","# üå≤ SIMPLE RF EVALUATOR\n","# ===================================================================\n","\n","class SimpleRFEvaluator:\n","    \"\"\"Super simple RF F1 Score evaluator\"\"\"\n","\n","    def __init__(self):\n","        self.rf_models = {}\n","        self.f1_results = {}\n","\n","    def load_rf_models_simple(self):\n","        \"\"\"Load RF models - simplified\"\"\"\n","        print(\"üîÑ Loading RF models...\")\n","\n","        if not rf_available:\n","            print(\"‚ùå No RF models available\")\n","            return False\n","\n","        loaded_count = 0\n","\n","        for day in available_days:\n","            try:\n","                model_path = f\"random_forest_models/{day}_Trend_rf_model.pkl\"\n","                model_data = joblib.load(model_path)\n","                self.rf_models[day] = model_data\n","                loaded_count += 1\n","                print(f\"   ‚úÖ {day} model loaded\")\n","            except Exception as e:\n","                print(f\"   ‚ùå {day} failed: {str(e)}\")\n","\n","        print(f\"   üìä Loaded {loaded_count} RF models\")\n","        return loaded_count > 0\n","\n","    def create_simple_test_data(self):\n","        \"\"\"Create simple test data for RF evaluation\"\"\"\n","        print(\"üìä Creating simple RF test data...\")\n","\n","        # Simple synthetic classification data\n","        np.random.seed(42)\n","\n","        test_data = {}\n","\n","        for day_key in self.rf_models.keys():\n","            print(f\"   üîÑ Creating test data for {day_key}...\")\n","\n","            # Get model info\n","            model_data = self.rf_models[day_key]\n","            rf_model = model_data['model']\n","            feature_columns = model_data['feature_columns']\n","\n","            # Generate test features\n","            n_samples = 50  # Simple 50 test cases\n","            n_features = len(feature_columns)\n","\n","            # Random feature values (normalized-like)\n","            X_test = np.random.randn(n_samples, n_features)\n","\n","            # Generate realistic trend labels\n","            # Bias towards certain classes for realistic distribution\n","            trend_probs = np.random.rand(n_samples)\n","            y_test = []\n","\n","            for prob in trend_probs:\n","                if prob < 0.4:\n","                    y_test.append('STAY')  # 40% STAY\n","                elif prob < 0.7:\n","                    y_test.append('UP')    # 30% UP\n","                else:\n","                    y_test.append('DOWN')  # 30% DOWN\n","\n","            test_data[day_key] = {\n","                'X_test': X_test,\n","                'y_test': y_test,\n","                'feature_columns': feature_columns\n","            }\n","\n","            print(f\"      ‚úÖ {n_samples} test samples for {day_key}\")\n","\n","        return test_data\n","\n","    def evaluate_f1_simple(self):\n","        \"\"\"Simple F1 Score evaluation\"\"\"\n","        print(\"\\nüéØ EVALUATING F1 SCORES...\")\n","\n","        if not self.rf_models:\n","            print(\"‚ùå RF models not loaded\")\n","            return False\n","\n","        # Get test data\n","        test_data = self.create_simple_test_data()\n","\n","        day_results = {}\n","        all_predictions = []\n","        all_actuals = []\n","\n","        for day_key in self.rf_models.keys():\n","            print(f\"   üîÑ Evaluating {day_key}...\")\n","\n","            try:\n","                # Get model and test data\n","                model_data = self.rf_models[day_key]\n","                rf_model = model_data['model']\n","                test_info = test_data[day_key]\n","\n","                X_test = test_info['X_test']\n","                y_test = test_info['y_test']\n","\n","                # Generate predictions\n","                y_pred = rf_model.predict(X_test)\n","                y_proba = rf_model.predict_proba(X_test)\n","\n","                # Calculate F1 scores\n","                f1_macro = f1_score(y_test, y_pred, average='macro')\n","                f1_weighted = f1_score(y_test, y_pred, average='weighted')\n","                accuracy = accuracy_score(y_test, y_pred)\n","\n","                # Calculate per-class F1\n","                f1_per_class = f1_score(y_test, y_pred, average=None, labels=['UP', 'DOWN', 'STAY'])\n","\n","                # Average confidence\n","                max_probas = np.max(y_proba, axis=1)\n","                avg_confidence = np.mean(max_probas)\n","\n","                day_results[day_key] = {\n","                    'f1_macro': f1_macro,\n","                    'f1_weighted': f1_weighted,\n","                    'accuracy': accuracy,\n","                    'f1_up': f1_per_class[0] if len(f1_per_class) > 0 else 0,\n","                    'f1_down': f1_per_class[1] if len(f1_per_class) > 1 else 0,\n","                    'f1_stay': f1_per_class[2] if len(f1_per_class) > 2 else 0,\n","                    'avg_confidence': avg_confidence,\n","                    'predictions': y_pred.tolist(),\n","                    'actuals': y_test,\n","                    'test_count': len(y_test)\n","                }\n","\n","                # Add to overall collections\n","                all_predictions.extend(y_pred)\n","                all_actuals.extend(y_test)\n","\n","                print(f\"      üìä F1 Macro: {f1_macro:.3f}\")\n","                print(f\"      üìä F1 Weighted: {f1_weighted:.3f}\")\n","                print(f\"      üìä Accuracy: {accuracy:.3f}\")\n","                print(f\"      üìä Confidence: {avg_confidence:.3f}\")\n","\n","            except Exception as e:\n","                print(f\"      ‚ùå Error: {str(e)}\")\n","\n","        # Calculate overall metrics\n","        if all_predictions:\n","            overall_f1_macro = f1_score(all_actuals, all_predictions, average='macro')\n","            overall_f1_weighted = f1_score(all_actuals, all_predictions, average='weighted')\n","            overall_accuracy = accuracy_score(all_actuals, all_predictions)\n","\n","            overall_results = {\n","                'f1_macro': overall_f1_macro,\n","                'f1_weighted': overall_f1_weighted,\n","                'accuracy': overall_accuracy,\n","                'total_predictions': len(all_predictions),\n","                'models_evaluated': len(day_results)\n","            }\n","\n","            self.f1_results = {\n","                'day_results': day_results,\n","                'overall_results': overall_results,\n","                'all_predictions': all_predictions,\n","                'all_actuals': all_actuals\n","            }\n","\n","            print(f\"\\nüìä OVERALL F1 RESULTS:\")\n","            print(f\"   üéØ Overall F1 Macro: {overall_f1_macro:.3f}\")\n","            print(f\"   üéØ Overall F1 Weighted: {overall_f1_weighted:.3f}\")\n","            print(f\"   üéØ Overall Accuracy: {overall_accuracy:.3f}\")\n","            print(f\"   üìä Total Predictions: {len(all_predictions)}\")\n","            print(f\"   üìä Models Evaluated: {len(day_results)}\")\n","\n","            return True\n","        else:\n","            print(\"‚ùå No predictions generated\")\n","            return False\n","\n","    def create_f1_visualization(self):\n","        \"\"\"Create simple F1 Score visualization\"\"\"\n","        print(\"\\nüìä CREATING F1 VISUALIZATION...\")\n","\n","        if not self.f1_results:\n","            print(\"‚ùå No F1 results to visualize\")\n","            return\n","\n","        # Create 2x2 subplot\n","        fig = make_subplots(\n","            rows=2, cols=2,\n","            subplot_titles=(\n","                'F1 Macro Score by Day',\n","                'F1 Score by Class (Day1)',\n","                'Accuracy vs F1 Score',\n","                'Overall Performance Summary'\n","            ),\n","            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n","                   [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n","        )\n","\n","        day_results = self.f1_results['day_results']\n","\n","        # 1. F1 Macro by Day\n","        days = list(day_results.keys())\n","        f1_macros = [day_results[day]['f1_macro'] for day in days]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=days,\n","                y=f1_macros,\n","                name='F1 Macro',\n","                marker_color='blue',\n","                text=[f\"{val:.3f}\" for val in f1_macros],\n","                textposition='auto'\n","            ),\n","            row=1, col=1\n","        )\n","\n","        # 2. F1 by Class (Day1 example)\n","        if len(days) > 0:\n","            sample_day = days[0]\n","            sample_result = day_results[sample_day]\n","\n","            classes = ['UP', 'DOWN', 'STAY']\n","            class_f1s = [\n","                sample_result['f1_up'],\n","                sample_result['f1_down'],\n","                sample_result['f1_stay']\n","            ]\n","\n","            fig.add_trace(\n","                go.Bar(\n","                    x=classes,\n","                    y=class_f1s,\n","                    name=f'F1 by Class ({sample_day})',\n","                    marker_color=['green', 'red', 'orange'],\n","                    text=[f\"{val:.3f}\" for val in class_f1s],\n","                    textposition='auto'\n","                ),\n","                row=1, col=2\n","            )\n","\n","        # 3. Accuracy vs F1 Scatter\n","        accuracies = [day_results[day]['accuracy'] for day in days]\n","\n","        fig.add_trace(\n","            go.Scatter(\n","                x=accuracies,\n","                y=f1_macros,\n","                mode='markers+text',\n","                text=days,\n","                textposition='top center',\n","                name='Accuracy vs F1',\n","                marker=dict(size=12, color='purple')\n","            ),\n","            row=2, col=1\n","        )\n","\n","        # 4. Overall Performance Summary\n","        overall = self.f1_results['overall_results']\n","        metrics = ['F1 Macro', 'F1 Weighted', 'Accuracy']\n","        values = [\n","            overall['f1_macro'],\n","            overall['f1_weighted'],\n","            overall['accuracy']\n","        ]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=metrics,\n","                y=values,\n","                name='Overall Performance',\n","                marker_color=['blue', 'green', 'red'],\n","                text=[f\"{val:.3f}\" for val in values],\n","                textposition='auto'\n","            ),\n","            row=2, col=2\n","        )\n","\n","        fig.update_layout(\n","            title=\"üå≤ Random Forest F1 Score Evaluation\",\n","            height=800,\n","            showlegend=True\n","        )\n","\n","        fig.show()\n","\n","        # Simple F1 summary chart\n","        print(\"   üìä Creating F1 summary chart...\")\n","\n","        fig2 = go.Figure()\n","\n","        # F1 scores by day\n","        fig2.add_trace(\n","            go.Bar(\n","                x=days,\n","                y=f1_macros,\n","                name='F1 Macro Score',\n","                marker_color='blue',\n","                text=[f\"{val:.3f}\" for val in f1_macros],\n","                textposition='auto'\n","            )\n","        )\n","\n","        # Add benchmark line\n","        benchmark = 0.65  # Good F1 score benchmark\n","        fig2.add_hline(\n","            y=benchmark,\n","            line_dash=\"dash\",\n","            line_color=\"red\",\n","            annotation_text=f\"Good F1 Benchmark ({benchmark})\"\n","        )\n","\n","        fig2.update_layout(\n","            title=\"üìä F1 Macro Scores - RF Models Performance\",\n","            xaxis_title=\"RF Models\",\n","            yaxis_title=\"F1 Macro Score\",\n","            height=400,\n","            yaxis=dict(range=[0, 1])\n","        )\n","\n","        fig2.show()\n","\n","        # Confusion Matrix for overall performance\n","        self._create_confusion_matrix()\n","\n","        print(\"   ‚úÖ F1 visualization created!\")\n","\n","    def _create_confusion_matrix(self):\n","        \"\"\"Create simple confusion matrix visualization\"\"\"\n","        print(\"   üìä Creating confusion matrix...\")\n","\n","        # Calculate confusion matrix\n","        y_true = self.f1_results['all_actuals']\n","        y_pred = self.f1_results['all_predictions']\n","\n","        cm = confusion_matrix(y_true, y_pred, labels=['UP', 'DOWN', 'STAY'])\n","\n","        # Create heatmap\n","        fig = go.Figure()\n","\n","        fig.add_trace(\n","            go.Heatmap(\n","                z=cm,\n","                x=['UP', 'DOWN', 'STAY'],\n","                y=['UP', 'DOWN', 'STAY'],\n","                colorscale='Blues',\n","                text=cm,\n","                texttemplate=\"%{text}\",\n","                textfont={\"size\": 16},\n","                hoverongaps=False\n","            )\n","        )\n","\n","        fig.update_layout(\n","            title=\"üéØ Overall Confusion Matrix - RF Models\",\n","            xaxis_title=\"Predicted\",\n","            yaxis_title=\"Actual\",\n","            height=500\n","        )\n","\n","        fig.show()\n","\n","# ===================================================================\n","# üöÄ SIMPLE MAIN EXECUTION\n","# ===================================================================\n","\n","def run_simple_rf_evaluation():\n","    \"\"\"Run simple RF F1 evaluation\"\"\"\n","    print(\"\\nüöÄ RUNNING SIMPLE RF F1 EVALUATION\")\n","    print(\"=\"*50)\n","\n","    if not rf_available:\n","        print(\"‚ùå No RF models available\")\n","        return None\n","\n","    # Initialize\n","    evaluator = SimpleRFEvaluator()\n","\n","    # Step 1: Load RF models\n","    print(\"üîÑ Step 1: Loading RF models...\")\n","    if not evaluator.load_rf_models_simple():\n","        print(\"‚ùå Cannot load RF models\")\n","        return None\n","\n","    # Step 2: Evaluate F1\n","    print(\"\\nüîÑ Step 2: Evaluating F1 scores...\")\n","    if not evaluator.evaluate_f1_simple():\n","        print(\"‚ùå F1 evaluation failed\")\n","        return None\n","\n","    # Step 3: Create visualization\n","    print(\"\\nüîÑ Step 3: Creating F1 visualization...\")\n","    evaluator.create_f1_visualization()\n","\n","    # Final summary\n","    overall = evaluator.f1_results['overall_results']\n","    print(f\"\\nüéâ RF F1 EVALUATION COMPLETED!\")\n","    print(f\"üìä Overall F1 Macro: {overall['f1_macro']:.3f}\")\n","    print(f\"üìä Overall F1 Weighted: {overall['f1_weighted']:.3f}\")\n","    print(f\"üìä Overall Accuracy: {overall['accuracy']:.3f}\")\n","    print(f\"üìä Models Evaluated: {overall['models_evaluated']}\")\n","\n","    return evaluator.f1_results\n","\n","# ===================================================================\n","# üéØ EXECUTE\n","# ===================================================================\n","\n","if rf_available:\n","    results = run_simple_rf_evaluation()\n","else:\n","    print(\"‚ùå Cannot run evaluation - no RF models found\")\n","    results = None\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"üå≤ SIMPLE RF F1 EVALUATION COMPLETE!\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0aPMqAkxi1jn","executionInfo":{"status":"ok","timestamp":1753429756776,"user_tz":-420,"elapsed":8418,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"ff2e137a-8470-4f5a-8600-871133372c3f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["üå≤ SIMPLE RF F1 SCORE EVALUATION\n","============================================================\n","üë§ User: wasirawasenju\n","üìÖ Date: 2025-07-25 07:43:56 UTC\n","üéØ Focus: F1 Score + Simple Visualization\n","============================================================\n","üìÅ Quick RF file check...\n","   ‚úÖ Found 6 files in random_forest_models/\n","      ‚úÖ Day1_Trend_rf_model.pkl\n","      ‚úÖ Day2_Trend_rf_model.pkl\n","      ‚úÖ Day3_Trend_rf_model.pkl\n","      ‚úÖ Day4_Trend_rf_model.pkl\n","      ‚úÖ Day5_Trend_rf_model.pkl\n","\n","üöÄ RUNNING SIMPLE RF F1 EVALUATION\n","==================================================\n","üîÑ Step 1: Loading RF models...\n","üîÑ Loading RF models...\n","   ‚úÖ Day1 model loaded\n","   ‚úÖ Day2 model loaded\n","   ‚úÖ Day3 model loaded\n","   ‚úÖ Day4 model loaded\n","   ‚úÖ Day5 model loaded\n","   üìä Loaded 5 RF models\n","\n","üîÑ Step 2: Evaluating F1 scores...\n","\n","üéØ EVALUATING F1 SCORES...\n","üìä Creating simple RF test data...\n","   üîÑ Creating test data for Day1...\n","      ‚úÖ 50 test samples for Day1\n","   üîÑ Creating test data for Day2...\n","      ‚úÖ 50 test samples for Day2\n","   üîÑ Creating test data for Day3...\n","      ‚úÖ 50 test samples for Day3\n","   üîÑ Creating test data for Day4...\n","      ‚úÖ 50 test samples for Day4\n","   üîÑ Creating test data for Day5...\n","      ‚úÖ 50 test samples for Day5\n","   üîÑ Evaluating Day1...\n","      üìä F1 Macro: 0.169\n","      üìä F1 Weighted: 0.193\n","      üìä Accuracy: 0.340\n","      üìä Confidence: 0.407\n","   üîÑ Evaluating Day2...\n","      üìä F1 Macro: 0.230\n","      üìä F1 Weighted: 0.254\n","      üìä Accuracy: 0.320\n","      üìä Confidence: 0.381\n","   üîÑ Evaluating Day3...\n","      üìä F1 Macro: 0.120\n","      üìä F1 Weighted: 0.079\n","      üìä Accuracy: 0.220\n","      üìä Confidence: 0.453\n","   üîÑ Evaluating Day4...\n","      üìä F1 Macro: 0.092\n","      üìä F1 Weighted: 0.044\n","      üìä Accuracy: 0.160\n","      üìä Confidence: 0.487\n","   üîÑ Evaluating Day5...\n","      üìä F1 Macro: 0.120\n","      üìä F1 Weighted: 0.079\n","      üìä Accuracy: 0.220\n","      üìä Confidence: 0.518\n","\n","üìä OVERALL F1 RESULTS:\n","   üéØ Overall F1 Macro: 0.196\n","   üéØ Overall F1 Weighted: 0.200\n","   üéØ Overall Accuracy: 0.252\n","   üìä Total Predictions: 250\n","   üìä Models Evaluated: 5\n","\n","üîÑ Step 3: Creating F1 visualization...\n","\n","üìä CREATING F1 VISUALIZATION...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"68fa2e31-3be7-487d-9499-3f84f828e870\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"68fa2e31-3be7-487d-9499-3f84f828e870\")) {                    Plotly.newPlot(                        \"68fa2e31-3be7-487d-9499-3f84f828e870\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"F1 Macro\",\"text\":[\"0.169\",\"0.230\",\"0.120\",\"0.092\",\"0.120\"],\"textposition\":\"auto\",\"x\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":[\"green\",\"red\",\"orange\"]},\"name\":\"F1 by Class (Day1)\",\"text\":[\"0.000\",\"0.000\",\"0.507\"],\"textposition\":\"auto\",\"x\":[\"UP\",\"DOWN\",\"STAY\"],\"y\":[0.0,0.0,0.5074626865671642],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"purple\",\"size\":12},\"mode\":\"markers+text\",\"name\":\"Accuracy vs F1\",\"text\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"textposition\":\"top center\",\"x\":[0.34,0.32,0.22,0.16,0.22],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":[\"blue\",\"green\",\"red\"]},\"name\":\"Overall Performance\",\"text\":[\"0.196\",\"0.200\",\"0.252\"],\"textposition\":\"auto\",\"x\":[\"F1 Macro\",\"F1 Weighted\",\"Accuracy\"],\"y\":[0.1964616382869104,0.19993734681075348,0.252],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"F1 Macro Score by Day\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"F1 Score by Class (Day1)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Accuracy vs F1 Score\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Overall Performance Summary\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"üå≤ Random Forest F1 Score Evaluation\"},\"height\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('68fa2e31-3be7-487d-9499-3f84f828e870');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   üìä Creating F1 summary chart...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\")) {                    Plotly.newPlot(                        \"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"F1 Macro Score\",\"text\":[\"0.169\",\"0.230\",\"0.120\",\"0.092\",\"0.120\"],\"textposition\":\"auto\",\"x\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"shapes\":[{\"line\":{\"color\":\"red\",\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"xref\":\"x domain\",\"y0\":0.65,\"y1\":0.65,\"yref\":\"y\"}],\"annotations\":[{\"showarrow\":false,\"text\":\"Good F1 Benchmark (0.65)\",\"x\":1,\"xanchor\":\"right\",\"xref\":\"x domain\",\"y\":0.65,\"yanchor\":\"bottom\",\"yref\":\"y\"}],\"yaxis\":{\"title\":{\"text\":\"F1 Macro Score\"},\"range\":[0,1]},\"title\":{\"text\":\"üìä F1 Macro Scores - RF Models Performance\"},\"xaxis\":{\"title\":{\"text\":\"RF Models\"}},\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('804dbfe7-ff94-423b-8ec5-d71fde7ad8ae');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   üìä Creating confusion matrix...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"60e7b256-c14a-43b3-9166-db0bad5a8eeb\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"60e7b256-c14a-43b3-9166-db0bad5a8eeb\")) {                    Plotly.newPlot(                        \"60e7b256-c14a-43b3-9166-db0bad5a8eeb\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"hoverongaps\":false,\"text\":[[44,6,16],[51,1,15],[92,7,18]],\"textfont\":{\"size\":16},\"texttemplate\":\"%{text}\",\"x\":[\"UP\",\"DOWN\",\"STAY\"],\"y\":[\"UP\",\"DOWN\",\"STAY\"],\"z\":[[44,6,16],[51,1,15],[92,7,18]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"üéØ Overall Confusion Matrix - RF Models\"},\"xaxis\":{\"title\":{\"text\":\"Predicted\"}},\"yaxis\":{\"title\":{\"text\":\"Actual\"}},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('60e7b256-c14a-43b3-9166-db0bad5a8eeb');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   ‚úÖ F1 visualization created!\n","\n","üéâ RF F1 EVALUATION COMPLETED!\n","üìä Overall F1 Macro: 0.196\n","üìä Overall F1 Weighted: 0.200\n","üìä Overall Accuracy: 0.252\n","üìä Models Evaluated: 5\n","\n","============================================================\n","üå≤ SIMPLE RF F1 EVALUATION COMPLETE!\n","============================================================\n"]}]}]}