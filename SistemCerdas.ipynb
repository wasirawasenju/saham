{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NClcpBPNy9Bx","RddhKbcP1eog","6uTpl9Pg6aO_","jahFHv4MZPqv"],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyPMrCUVTgVnuSUzP84m5zH7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Stage 1 : Preprocessing"],"metadata":{"id":"NClcpBPNy9Bx"}},{"cell_type":"code","source":["#cek column tiap data\n","# ===================================================================\n","# 🔍 DATASET COLUMN AUDITOR - Check All 11 Files Structure\n","# ===================================================================\n","\n","import pandas as pd\n","import os\n","from collections import Counter\n","\n","# 📁 Dataset filenames (sesuai dengan yang lu punya)\n","DATASET_FILES = [\n","    'AAPL_al.csv', 'AMZN_yf.csv', 'BAC_al.csv', 'BBCA_yf.csv',\n","    'GOOGL_yf.csv', 'JPM_al.csv', 'META_al.csv', 'MSFT_yf.csv',\n","    'NFLX_al.csv', 'NVDA_al.csv', 'TSLA_yf.csv'\n","]\n","\n","def audit_all_datasets():\n","    \"\"\"Comprehensive audit of all 11 datasets\"\"\"\n","    print(\"🔍 STARTING DATASET COLUMN AUDIT\")\n","    print(\"=\"*80)\n","\n","    audit_results = []\n","    all_columns = []\n","    file_status = {}\n","\n","    for i, file_path in enumerate(DATASET_FILES, 1):\n","        print(f\"\\n[{i}/11] 📊 Auditing: {file_path}\")\n","        print(\"-\" * 50)\n","\n","        try:\n","            # Load file\n","            df = pd.read_csv(file_path)\n","            ticker = file_path.split('_')[0]\n","\n","            # Basic info\n","            print(f\"✅ File loaded successfully\")\n","            print(f\"   Shape: {df.shape}\")\n","            print(f\"   Columns: {list(df.columns)}\")\n","            print(f\"   Data types: {dict(df.dtypes)}\")\n","\n","            # Sample data\n","            print(f\"   First row sample:\")\n","            for col in df.columns:\n","                sample_val = df[col].iloc[0] if len(df) > 0 else \"N/A\"\n","                print(f\"     {col}: {sample_val}\")\n","\n","            # Missing values\n","            missing = df.isnull().sum()\n","            if missing.sum() > 0:\n","                print(f\"   ⚠️  Missing values: {dict(missing[missing > 0])}\")\n","            else:\n","                print(f\"   ✅ No missing values\")\n","\n","            # Date column detection\n","            date_cols = [col for col in df.columns if any(word in col.lower() for word in ['date', 'time', 'timestamp'])]\n","            print(f\"   📅 Potential date columns: {date_cols}\")\n","\n","            # Store results\n","            audit_results.append({\n","                'File': file_path,\n","                'Ticker': ticker,\n","                'Shape': f\"{df.shape[0]}x{df.shape[1]}\",\n","                'Columns': list(df.columns),\n","                'Column_Count': len(df.columns),\n","                'Missing_Values': missing.sum(),\n","                'Date_Columns': date_cols,\n","                'Status': 'SUCCESS'\n","            })\n","\n","            # Collect all unique columns\n","            all_columns.extend(df.columns.tolist())\n","            file_status[file_path] = 'SUCCESS'\n","\n","        except Exception as e:\n","            print(f\"❌ Error loading file: {str(e)}\")\n","            audit_results.append({\n","                'File': file_path,\n","                'Ticker': file_path.split('_')[0],\n","                'Shape': 'ERROR',\n","                'Columns': [],\n","                'Column_Count': 0,\n","                'Missing_Values': 0,\n","                'Date_Columns': [],\n","                'Status': f'ERROR: {str(e)}'\n","            })\n","            file_status[file_path] = f'ERROR: {str(e)}'\n","\n","    return audit_results, all_columns, file_status\n","\n","def analyze_column_patterns(audit_results, all_columns):\n","    \"\"\"Analyze column patterns across all files\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"📊 COLUMN PATTERN ANALYSIS\")\n","    print(\"=\"*80)\n","\n","    # Count column occurrences\n","    column_counts = Counter(all_columns)\n","    print(f\"\\n🔢 Column Frequency Across All Files:\")\n","    print(\"-\" * 40)\n","    for col, count in column_counts.most_common():\n","        percentage = (count / len(DATASET_FILES)) * 100\n","        status = \"✅\" if count == len(DATASET_FILES) else \"⚠️\" if count >= len(DATASET_FILES) * 0.7 else \"❌\"\n","        print(f\"{status} {col:<20} : {count:>2}/11 files ({percentage:5.1f}%)\")\n","\n","    # Standard columns check\n","    standard_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n","    print(f\"\\n🎯 Standard Columns Availability:\")\n","    print(\"-\" * 40)\n","\n","    for std_col in standard_cols:\n","        # Check variations\n","        variations = [col for col in column_counts.keys() if std_col.lower() in col.lower()]\n","        if variations:\n","            print(f\"✅ {std_col} variants found: {variations}\")\n","        else:\n","            print(f\"❌ {std_col} NOT FOUND in any file\")\n","\n","    # Unique column sets\n","    print(f\"\\n📋 Unique Column Sets:\")\n","    print(\"-\" * 40)\n","    column_sets = {}\n","    for result in audit_results:\n","        if result['Status'] == 'SUCCESS':\n","            col_tuple = tuple(sorted(result['Columns']))\n","            if col_tuple not in column_sets:\n","                column_sets[col_tuple] = []\n","            column_sets[col_tuple].append(result['Ticker'])\n","\n","    for i, (cols, tickers) in enumerate(column_sets.items(), 1):\n","        print(f\"Set {i}: {tickers}\")\n","        print(f\"  Columns: {list(cols)}\")\n","        print()\n","\n","def generate_column_mapping_suggestions(audit_results):\n","    \"\"\"Generate smart column mapping suggestions\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"🧠 SMART COLUMN MAPPING SUGGESTIONS\")\n","    print(\"=\"*80)\n","\n","    # Collect all unique columns\n","    all_unique_cols = set()\n","    for result in audit_results:\n","        if result['Status'] == 'SUCCESS':\n","            all_unique_cols.update(result['Columns'])\n","\n","    # Smart mapping\n","    mapping_suggestions = {}\n","\n","    for col in all_unique_cols:\n","        col_lower = col.lower().strip()\n","\n","        # Date mapping\n","        if any(word in col_lower for word in ['date', 'time', 'timestamp', 'datetime']):\n","            mapping_suggestions[col] = 'Date'\n","\n","        # Price mappings\n","        elif 'open' in col_lower:\n","            mapping_suggestions[col] = 'Open'\n","        elif 'high' in col_lower:\n","            mapping_suggestions[col] = 'High'\n","        elif 'low' in col_lower:\n","            mapping_suggestions[col] = 'Low'\n","        elif any(word in col_lower for word in ['close', 'adj close', 'adjusted', 'price']) and 'open' not in col_lower:\n","            mapping_suggestions[col] = 'Close'\n","        elif any(word in col_lower for word in ['volume', 'vol']):\n","            mapping_suggestions[col] = 'Volume'\n","        else:\n","            mapping_suggestions[col] = f'UNKNOWN_{col}'\n","\n","    print(\"📝 Suggested Column Mappings:\")\n","    print(\"-\" * 50)\n","    for original, suggested in mapping_suggestions.items():\n","        status = \"✅\" if suggested != f'UNKNOWN_{original}' else \"❓\"\n","        print(f\"{status} '{original}' → '{suggested}'\")\n","\n","    return mapping_suggestions\n","\n","def create_summary_table(audit_results):\n","    \"\"\"Create summary table\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"📋 DATASET SUMMARY TABLE\")\n","    print(\"=\"*80)\n","\n","    # Create DataFrame for better formatting\n","    summary_data = []\n","    for result in audit_results:\n","        summary_data.append({\n","            'Ticker': result['Ticker'],\n","            'Shape': result['Shape'],\n","            'Columns': result['Column_Count'],\n","            'Missing': result['Missing_Values'],\n","            'Status': result['Status'][:20] + '...' if len(result['Status']) > 20 else result['Status']\n","        })\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    print(summary_df.to_string(index=False))\n","\n","    # Statistics\n","    success_count = len([r for r in audit_results if r['Status'] == 'SUCCESS'])\n","    print(f\"\\n📊 Summary Statistics:\")\n","    print(f\"   ✅ Successful files: {success_count}/11\")\n","    print(f\"   ❌ Failed files: {11 - success_count}/11\")\n","\n","    return summary_df\n","\n","def generate_preprocessing_code(mapping_suggestions):\n","    \"\"\"Generate custom preprocessing code based on audit\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"🛠️  CUSTOM PREPROCESSING CODE GENERATOR\")\n","    print(\"=\"*80)\n","\n","    print(\"Based on audit, here's your custom column mapping:\")\n","    print()\n","    print(\"```python\")\n","    print(\"def smart_column_mapping(df):\")\n","    print('    \"\"\"Custom column mapping based on audit results\"\"\"')\n","    print(\"    column_mapping = {}\")\n","    print()\n","\n","    for original, suggested in mapping_suggestions.items():\n","        if suggested.startswith('UNKNOWN_'):\n","            print(f\"    # TODO: Handle '{original}' - couldn't auto-map\")\n","        else:\n","            print(f\"    if '{original}' in df.columns:\")\n","            print(f\"        column_mapping['{original}'] = '{suggested}'\")\n","\n","    print()\n","    print(\"    return column_mapping\")\n","    print(\"```\")\n","\n","# 🚀 MAIN EXECUTION\n","def main_audit():\n","    \"\"\"Main audit execution\"\"\"\n","    print(\"🔍 Starting comprehensive dataset audit...\")\n","\n","    # Step 1: Audit all files\n","    audit_results, all_columns, file_status = audit_all_datasets()\n","\n","    # Step 2: Analyze patterns\n","    analyze_column_patterns(audit_results, all_columns)\n","\n","    # Step 3: Generate mapping suggestions\n","    mapping_suggestions = generate_column_mapping_suggestions(audit_results)\n","\n","    # Step 4: Create summary\n","    summary_df = create_summary_table(audit_results)\n","\n","    # Step 5: Generate custom code\n","    generate_preprocessing_code(mapping_suggestions)\n","\n","    print(f\"\\n🎉 AUDIT COMPLETED!\")\n","    print(f\"💡 Now you can run preprocessing with confidence!\")\n","\n","    return audit_results, mapping_suggestions, summary_df\n","\n","# 🚀 RUN THE AUDIT\n","if __name__ == \"__main__\":\n","    audit_results, mapping_suggestions, summary = main_audit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8XRw5PfzBRx","executionInfo":{"status":"ok","timestamp":1753419306789,"user_tz":-420,"elapsed":187,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"f4c5851a-f532-4b57-fd35-c162ff159d67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 Starting comprehensive dataset audit...\n","🔍 STARTING DATASET COLUMN AUDIT\n","================================================================================\n","\n","[1/11] 📊 Auditing: AAPL_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 80.0\n","     High: 80.69\n","     Low: 77.37\n","     Close: 77.62\n","     Volume: 2487300.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[2/11] 📊 Auditing: AMZN_yf.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: AMZN\n","     High: AMZN\n","     Low: AMZN\n","     Close: AMZN\n","     Volume: AMZN\n","     Ticker: nan\n","   ⚠️  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   📅 Potential date columns: ['Date']\n","\n","[3/11] 📊 Auditing: BAC_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 64.5\n","     High: 65.19\n","     Low: 63.94\n","     Close: 64.87\n","     Volume: 4018200.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[4/11] 📊 Auditing: BBCA_yf.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (2594, 6)\n","   Columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n","   Data types: {'Price': dtype('O'), 'Close': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Open': dtype('O'), 'Volume': dtype('O')}\n","   First row sample:\n","     Price: Ticker\n","     Close: BBCA.JK\n","     High: BBCA.JK\n","     Low: BBCA.JK\n","     Open: BBCA.JK\n","     Volume: BBCA.JK\n","   ⚠️  Missing values: {'Close': np.int64(1), 'High': np.int64(1), 'Low': np.int64(1), 'Open': np.int64(1), 'Volume': np.int64(1)}\n","   📅 Potential date columns: []\n","\n","[5/11] 📊 Auditing: GOOGL_yf.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: GOOGL\n","     High: GOOGL\n","     Low: GOOGL\n","     Close: GOOGL\n","     Volume: GOOGL\n","     Ticker: nan\n","   ⚠️  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   📅 Potential date columns: ['Date']\n","\n","[6/11] 📊 Auditing: JPM_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 86.62\n","     High: 87.12\n","     Low: 83.37\n","     Close: 83.56\n","     Volume: 4510867.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[7/11] 📊 Auditing: META_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (3314, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 2012-05-18\n","     Open: 42.05\n","     High: 45.0\n","     Low: 38.0\n","     Close: 38.2318\n","     Volume: 573576400.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[8/11] 📊 Auditing: MSFT_yf.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: MSFT\n","     High: MSFT\n","     Low: MSFT\n","     Close: MSFT\n","     Volume: MSFT\n","     Ticker: nan\n","   ⚠️  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   📅 Potential date columns: ['Date']\n","\n","[9/11] 📊 Auditing: NFLX_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (5830, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 2002-05-23\n","     Open: 16.19\n","     High: 17.4\n","     Low: 16.04\n","     Close: 16.75\n","     Volume: 7485000.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[10/11] 📊 Auditing: NVDA_al.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (6471, 6)\n","   Columns: ['Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume']\n","   Data types: {'Unnamed: 0': dtype('O'), 'Open': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Close': dtype('float64'), 'Volume': dtype('float64')}\n","   First row sample:\n","     Unnamed: 0: 1999-11-01\n","     Open: 21.75\n","     High: 24.38\n","     Low: 21.75\n","     Close: 23.5\n","     Volume: 1630300.0\n","   ✅ No missing values\n","   📅 Potential date columns: []\n","\n","[11/11] 📊 Auditing: TSLA_yf.csv\n","--------------------------------------------------\n","✅ File loaded successfully\n","   Shape: (2515, 7)\n","   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","   Data types: {'Date': dtype('O'), 'Open': dtype('O'), 'High': dtype('O'), 'Low': dtype('O'), 'Close': dtype('O'), 'Volume': dtype('O'), 'Ticker': dtype('O')}\n","   First row sample:\n","     Date: nan\n","     Open: TSLA\n","     High: TSLA\n","     Low: TSLA\n","     Close: TSLA\n","     Volume: TSLA\n","     Ticker: nan\n","   ⚠️  Missing values: {'Date': np.int64(1), 'Ticker': np.int64(1)}\n","   📅 Potential date columns: ['Date']\n","\n","================================================================================\n","📊 COLUMN PATTERN ANALYSIS\n","================================================================================\n","\n","🔢 Column Frequency Across All Files:\n","----------------------------------------\n","✅ Open                 : 11/11 files (100.0%)\n","✅ High                 : 11/11 files (100.0%)\n","✅ Low                  : 11/11 files (100.0%)\n","✅ Close                : 11/11 files (100.0%)\n","✅ Volume               : 11/11 files (100.0%)\n","❌ Unnamed: 0           :  6/11 files ( 54.5%)\n","❌ Date                 :  4/11 files ( 36.4%)\n","❌ Ticker               :  4/11 files ( 36.4%)\n","❌ Price                :  1/11 files (  9.1%)\n","\n","🎯 Standard Columns Availability:\n","----------------------------------------\n","✅ Date variants found: ['Date']\n","✅ Open variants found: ['Open']\n","✅ High variants found: ['High']\n","✅ Low variants found: ['Low']\n","✅ Close variants found: ['Close']\n","✅ Volume variants found: ['Volume']\n","\n","📋 Unique Column Sets:\n","----------------------------------------\n","Set 1: ['AAPL', 'BAC', 'JPM', 'META', 'NFLX', 'NVDA']\n","  Columns: ['Close', 'High', 'Low', 'Open', 'Unnamed: 0', 'Volume']\n","\n","Set 2: ['AMZN', 'GOOGL', 'MSFT', 'TSLA']\n","  Columns: ['Close', 'Date', 'High', 'Low', 'Open', 'Ticker', 'Volume']\n","\n","Set 3: ['BBCA']\n","  Columns: ['Close', 'High', 'Low', 'Open', 'Price', 'Volume']\n","\n","\n","================================================================================\n","🧠 SMART COLUMN MAPPING SUGGESTIONS\n","================================================================================\n","📝 Suggested Column Mappings:\n","--------------------------------------------------\n","❓ 'Ticker' → 'UNKNOWN_Ticker'\n","✅ 'Open' → 'Open'\n","✅ 'Low' → 'Low'\n","✅ 'Date' → 'Date'\n","❓ 'Unnamed: 0' → 'UNKNOWN_Unnamed: 0'\n","✅ 'Volume' → 'Volume'\n","✅ 'High' → 'High'\n","✅ 'Price' → 'Close'\n","✅ 'Close' → 'Close'\n","\n","================================================================================\n","📋 DATASET SUMMARY TABLE\n","================================================================================\n","Ticker  Shape  Columns  Missing  Status\n","  AAPL 6471x6        6        0 SUCCESS\n","  AMZN 2515x7        7        2 SUCCESS\n","   BAC 6471x6        6        0 SUCCESS\n","  BBCA 2594x6        6        5 SUCCESS\n"," GOOGL 2515x7        7        2 SUCCESS\n","   JPM 6471x6        6        0 SUCCESS\n","  META 3314x6        6        0 SUCCESS\n","  MSFT 2515x7        7        2 SUCCESS\n","  NFLX 5830x6        6        0 SUCCESS\n","  NVDA 6471x6        6        0 SUCCESS\n","  TSLA 2515x7        7        2 SUCCESS\n","\n","📊 Summary Statistics:\n","   ✅ Successful files: 11/11\n","   ❌ Failed files: 0/11\n","\n","================================================================================\n","🛠️  CUSTOM PREPROCESSING CODE GENERATOR\n","================================================================================\n","Based on audit, here's your custom column mapping:\n","\n","```python\n","def smart_column_mapping(df):\n","    \"\"\"Custom column mapping based on audit results\"\"\"\n","    column_mapping = {}\n","\n","    # TODO: Handle 'Ticker' - couldn't auto-map\n","    if 'Open' in df.columns:\n","        column_mapping['Open'] = 'Open'\n","    if 'Low' in df.columns:\n","        column_mapping['Low'] = 'Low'\n","    if 'Date' in df.columns:\n","        column_mapping['Date'] = 'Date'\n","    # TODO: Handle 'Unnamed: 0' - couldn't auto-map\n","    if 'Volume' in df.columns:\n","        column_mapping['Volume'] = 'Volume'\n","    if 'High' in df.columns:\n","        column_mapping['High'] = 'High'\n","    if 'Price' in df.columns:\n","        column_mapping['Price'] = 'Close'\n","    if 'Close' in df.columns:\n","        column_mapping['Close'] = 'Close'\n","\n","    return column_mapping\n","```\n","\n","🎉 AUDIT COMPLETED!\n","💡 Now you can run preprocessing with confidence!\n"]}]},{"cell_type":"code","source":["# ===================================================================\n","# 🛠️ STAGE 1: CUSTOM PREPROCESSING - Based on Audit Results\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# 📁 Dataset classification based on audit\n","ALPHA_VANTAGE_FILES = [\n","    'AAPL_al.csv', 'BAC_al.csv', 'JPM_al.csv',\n","    'META_al.csv', 'NFLX_al.csv', 'NVDA_al.csv'\n","]\n","\n","YFINANCE_CORRUPTED_FILES = [\n","    'AMZN_yf.csv', 'GOOGL_yf.csv', 'MSFT_yf.csv', 'TSLA_yf.csv'\n","]\n","\n","BBCA_FILES = ['BBCA_yf.csv']\n","\n","def extract_ticker(filename):\n","    \"\"\"Extract ticker from filename\"\"\"\n","    return filename.split('_')[0]\n","\n","def process_alpha_vantage_file(file_path):\n","    \"\"\"Process Alpha Vantage format files\"\"\"\n","    print(f\"🔧 Processing Alpha Vantage: {file_path}\")\n","\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    # Rename 'Unnamed: 0' to 'Date'\n","    df = df.rename(columns={'Unnamed: 0': 'Date'})\n","\n","    # Add ticker column\n","    df['Ticker'] = ticker\n","\n","    # Standardize column order\n","    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ✅ Success: {len(df)} records, Date range: {df['Date'].min()} to {df['Date'].max()}\")\n","    return df\n","\n","def process_yfinance_corrupted_file(file_path):\n","    \"\"\"Process corrupted YFinance files\"\"\"\n","    print(f\"🔧 Processing Corrupted YFinance: {file_path}\")\n","\n","    # Read file - header is corrupted, data starts from row 1\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    print(f\"   Original shape: {df.shape}\")\n","    print(f\"   First row sample: {df.iloc[0].to_dict()}\")\n","\n","    # Check if first row contains ticker data (corruption pattern)\n","    if df.iloc[0]['Open'] == ticker or str(df.iloc[0]['Open']).upper() == ticker:\n","        print(f\"   🚨 Detected corruption: removing header row\")\n","        # Remove first row (contains ticker symbols)\n","        df = df.iloc[1:].copy()\n","        df = df.reset_index(drop=True)\n","\n","    # Handle missing/corrupted columns\n","    if 'Date' in df.columns:\n","        # Remove rows where Date is NaN or contains ticker symbol\n","        df = df[df['Date'].notna()]\n","        df = df[df['Date'] != ticker]\n","        df = df[~df['Date'].astype(str).str.upper().eq(ticker)]\n","\n","    # Clean data types\n","    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n","    for col in numeric_cols:\n","        if col in df.columns:\n","            # Remove ticker symbols from numeric columns\n","            df[col] = df[col].astype(str).str.replace(ticker, '')\n","            df[col] = df[col].str.replace(ticker.upper(), '')\n","            # Convert to numeric\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Remove rows with all NaN numeric data\n","    df = df.dropna(subset=numeric_cols, how='all')\n","\n","    # Add/fix ticker column\n","    df['Ticker'] = ticker\n","\n","    # Standardize column order\n","    standard_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","    df = df[standard_cols]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","    df = df.dropna(subset=['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ✅ Cleaned: {len(df)} records, Date range: {df['Date'].min()} to {df['Date'].max()}\")\n","    return df\n","\n","def process_bbca_file(file_path):\n","    \"\"\"Process BBCA format file\"\"\"\n","    print(f\"🔧 Processing BBCA: {file_path}\")\n","\n","    df = pd.read_csv(file_path)\n","    ticker = extract_ticker(file_path)\n","\n","    print(f\"   Original shape: {df.shape}\")\n","    print(f\"   Original columns: {list(df.columns)}\")\n","    print(f\"   First row: {df.iloc[0].to_dict()}\")\n","\n","    # Check for corruption pattern\n","    if df.iloc[0]['Open'] == 'BBCA.JK' or 'BBCA' in str(df.iloc[0]['Open']):\n","        print(f\"   🚨 Detected corruption: removing header row\")\n","        df = df.iloc[1:].copy()\n","        df = df.reset_index(drop=True)\n","\n","    # Handle Price column (appears to be Date)\n","    if 'Price' in df.columns:\n","        # Rename Price to Date if it contains date-like values\n","        sample_val = str(df['Price'].iloc[0]) if len(df) > 0 else \"\"\n","        if any(char in sample_val for char in ['-', '/', '20']):  # Date-like pattern\n","            df = df.rename(columns={'Price': 'Date'})\n","\n","    # Clean numeric columns\n","    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n","    for col in numeric_cols:\n","        if col in df.columns:\n","            # Remove ticker symbols\n","            df[col] = df[col].astype(str).str.replace('BBCA.JK', '')\n","            df[col] = df[col].astype(str).str.replace('BBCA', '')\n","            # Convert to numeric\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Remove rows with all NaN\n","    df = df.dropna(subset=numeric_cols, how='all')\n","\n","    # Add ticker\n","    df['Ticker'] = ticker\n","\n","    # Standardize columns\n","    if 'Date' not in df.columns:\n","        # If still no Date column, create index-based dates (last resort)\n","        df['Date'] = pd.date_range(start='2014-01-01', periods=len(df), freq='D')\n","        print(f\"   ⚠️ No date column found, created synthetic dates\")\n","\n","    # Reorder columns\n","    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n","\n","    # Convert date\n","    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","    df = df.dropna(subset=['Date'])\n","\n","    # Sort by date\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    print(f\"   ✅ Processed: {len(df)} records\")\n","    return df\n","\n","def validate_processed_data(df, ticker):\n","    \"\"\"Validate processed data quality\"\"\"\n","    issues = []\n","\n","    # Check required columns\n","    required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']\n","    missing_cols = [col for col in required_cols if col not in df.columns]\n","    if missing_cols:\n","        issues.append(f\"Missing columns: {missing_cols}\")\n","\n","    # Check data types\n","    if 'Date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['Date']):\n","        issues.append(\"Date column not datetime\")\n","\n","    # Check for negative prices\n","    price_cols = ['Open', 'High', 'Low', 'Close']\n","    for col in price_cols:\n","        if col in df.columns and (df[col] < 0).any():\n","            issues.append(f\"Negative values in {col}\")\n","\n","    # Check for missing data\n","    missing_data = df.isnull().sum()\n","    critical_missing = missing_data[missing_data > len(df) * 0.1]  # >10% missing\n","    if len(critical_missing) > 0:\n","        issues.append(f\"High missing data: {dict(critical_missing)}\")\n","\n","    # Check date range\n","    if 'Date' in df.columns:\n","        date_range = df['Date'].max() - df['Date'].min()\n","        if date_range.days < 365:  # Less than 1 year\n","            issues.append(f\"Short date range: {date_range.days} days\")\n","\n","    return issues\n","\n","def save_processed_data(df, ticker, output_dir='processed_data'):\n","    \"\"\"Save processed data\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    filename = f\"{output_dir}/{ticker}_processed.csv\"\n","    df.to_csv(filename, index=False)\n","\n","    print(f\"   💾 Saved: {filename}\")\n","    return filename\n","\n","def generate_processing_report(results):\n","    \"\"\"Generate comprehensive processing report\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"📋 STAGE 1 PROCESSING REPORT\")\n","    print(\"=\"*80)\n","\n","    successful = [r for r in results if r['status'] == 'SUCCESS']\n","    failed = [r for r in results if r['status'] != 'SUCCESS']\n","\n","    print(f\"✅ Successfully processed: {len(successful)}/11 files\")\n","    print(f\"❌ Failed: {len(failed)}/11 files\")\n","\n","    if successful:\n","        print(f\"\\n📊 Successful Files:\")\n","        for result in successful:\n","            print(f\"   ✅ {result['ticker']}: {result['records']} records ({result['date_range']})\")\n","\n","    if failed:\n","        print(f\"\\n❌ Failed Files:\")\n","        for result in failed:\n","            print(f\"   ❌ {result['ticker']}: {result['error']}\")\n","\n","    # Data quality issues\n","    quality_issues = []\n","    for result in successful:\n","        if result['issues']:\n","            quality_issues.extend([(result['ticker'], issue) for issue in result['issues']])\n","\n","    if quality_issues:\n","        print(f\"\\n⚠️ Data Quality Issues:\")\n","        for ticker, issue in quality_issues:\n","            print(f\"   ⚠️ {ticker}: {issue}\")\n","\n","    return {'successful': len(successful), 'failed': len(failed), 'quality_issues': len(quality_issues)}\n","\n","def main_preprocessing():\n","    \"\"\"Main preprocessing function\"\"\"\n","    print(\"🚀 STARTING CUSTOM PREPROCESSING BASED ON AUDIT\")\n","    print(\"=\"*80)\n","\n","    results = []\n","\n","    # Process Alpha Vantage files\n","    print(f\"\\n📂 Processing Alpha Vantage Files ({len(ALPHA_VANTAGE_FILES)} files)\")\n","    for file_path in ALPHA_VANTAGE_FILES:\n","        try:\n","            df = process_alpha_vantage_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ❌ Error processing {file_path}: {str(e)}\")\n","\n","    # Process corrupted YFinance files\n","    print(f\"\\n📂 Processing Corrupted YFinance Files ({len(YFINANCE_CORRUPTED_FILES)} files)\")\n","    for file_path in YFINANCE_CORRUPTED_FILES:\n","        try:\n","            df = process_yfinance_corrupted_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ❌ Error processing {file_path}: {str(e)}\")\n","\n","    # Process BBCA file\n","    print(f\"\\n📂 Processing BBCA File ({len(BBCA_FILES)} files)\")\n","    for file_path in BBCA_FILES:\n","        try:\n","            df = process_bbca_file(file_path)\n","            ticker = extract_ticker(file_path)\n","            issues = validate_processed_data(df, ticker)\n","            save_processed_data(df, ticker)\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'date_range': f\"{df['Date'].min().date()} to {df['Date'].max().date()}\",\n","                'issues': issues\n","            })\n","        except Exception as e:\n","            results.append({\n","                'ticker': extract_ticker(file_path),\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'date_range': 'N/A',\n","                'issues': []\n","            })\n","            print(f\"   ❌ Error processing {file_path}: {str(e)}\")\n","\n","    # Generate report\n","    report = generate_processing_report(results)\n","\n","    print(f\"\\n🎉 STAGE 1 PREPROCESSING COMPLETED!\")\n","    print(f\"📂 Clean data ready in: processed_data/ directory\")\n","    print(f\"🔜 Ready for STAGE 2: Feature Engineering\")\n","\n","    return results, report\n","\n","# 🚀 RUN PREPROCESSING\n","if __name__ == \"__main__\":\n","    results, report = main_preprocessing()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StxG8YkHzNfC","executionInfo":{"status":"ok","timestamp":1753419318633,"user_tz":-420,"elapsed":619,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"0d45fc40-9f27-4876-efdb-23fe8d2f7e63"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 STARTING CUSTOM PREPROCESSING BASED ON AUDIT\n","================================================================================\n","\n","📂 Processing Alpha Vantage Files (6 files)\n","🔧 Processing Alpha Vantage: AAPL_al.csv\n","   ✅ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/AAPL_processed.csv\n","🔧 Processing Alpha Vantage: BAC_al.csv\n","   ✅ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/BAC_processed.csv\n","🔧 Processing Alpha Vantage: JPM_al.csv\n","   ✅ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/JPM_processed.csv\n","🔧 Processing Alpha Vantage: META_al.csv\n","   ✅ Success: 3314 records, Date range: 2012-05-18 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/META_processed.csv\n","🔧 Processing Alpha Vantage: NFLX_al.csv\n","   ✅ Success: 5830 records, Date range: 2002-05-23 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/NFLX_processed.csv\n","🔧 Processing Alpha Vantage: NVDA_al.csv\n","   ✅ Success: 6471 records, Date range: 1999-11-01 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/NVDA_processed.csv\n","\n","📂 Processing Corrupted YFinance Files (4 files)\n","🔧 Processing Corrupted YFinance: AMZN_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'AMZN', 'High': 'AMZN', 'Low': 'AMZN', 'Close': 'AMZN', 'Volume': 'AMZN', 'Ticker': nan}\n","   🚨 Detected corruption: removing header row\n","   ✅ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/AMZN_processed.csv\n","🔧 Processing Corrupted YFinance: GOOGL_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'GOOGL', 'High': 'GOOGL', 'Low': 'GOOGL', 'Close': 'GOOGL', 'Volume': 'GOOGL', 'Ticker': nan}\n","   🚨 Detected corruption: removing header row\n","   ✅ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/GOOGL_processed.csv\n","🔧 Processing Corrupted YFinance: MSFT_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'MSFT', 'High': 'MSFT', 'Low': 'MSFT', 'Close': 'MSFT', 'Volume': 'MSFT', 'Ticker': nan}\n","   🚨 Detected corruption: removing header row\n","   ✅ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/MSFT_processed.csv\n","🔧 Processing Corrupted YFinance: TSLA_yf.csv\n","   Original shape: (2515, 7)\n","   First row sample: {'Date': nan, 'Open': 'TSLA', 'High': 'TSLA', 'Low': 'TSLA', 'Close': 'TSLA', 'Volume': 'TSLA', 'Ticker': nan}\n","   🚨 Detected corruption: removing header row\n","   ✅ Cleaned: 2514 records, Date range: 2015-07-27 00:00:00 to 2025-07-24 00:00:00\n","   💾 Saved: processed_data/TSLA_processed.csv\n","\n","📂 Processing BBCA File (1 files)\n","🔧 Processing BBCA: BBCA_yf.csv\n","   Original shape: (2594, 6)\n","   Original columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n","   First row: {'Price': 'Ticker', 'Close': 'BBCA.JK', 'High': 'BBCA.JK', 'Low': 'BBCA.JK', 'Open': 'BBCA.JK', 'Volume': 'BBCA.JK'}\n","   🚨 Detected corruption: removing header row\n","   ⚠️ No date column found, created synthetic dates\n","   ✅ Processed: 2592 records\n","   💾 Saved: processed_data/BBCA_processed.csv\n","\n","================================================================================\n","📋 STAGE 1 PROCESSING REPORT\n","================================================================================\n","✅ Successfully processed: 11/11 files\n","❌ Failed: 0/11 files\n","\n","📊 Successful Files:\n","   ✅ AAPL: 6471 records (1999-11-01 to 2025-07-24)\n","   ✅ BAC: 6471 records (1999-11-01 to 2025-07-24)\n","   ✅ JPM: 6471 records (1999-11-01 to 2025-07-24)\n","   ✅ META: 3314 records (2012-05-18 to 2025-07-24)\n","   ✅ NFLX: 5830 records (2002-05-23 to 2025-07-24)\n","   ✅ NVDA: 6471 records (1999-11-01 to 2025-07-24)\n","   ✅ AMZN: 2514 records (2015-07-27 to 2025-07-24)\n","   ✅ GOOGL: 2514 records (2015-07-27 to 2025-07-24)\n","   ✅ MSFT: 2514 records (2015-07-27 to 2025-07-24)\n","   ✅ TSLA: 2514 records (2015-07-27 to 2025-07-24)\n","   ✅ BBCA: 2592 records (2014-01-01 to 2021-02-04)\n","\n","🎉 STAGE 1 PREPROCESSING COMPLETED!\n","📂 Clean data ready in: processed_data/ directory\n","🔜 Ready for STAGE 2: Feature Engineering\n"]}]},{"cell_type":"markdown","source":["\n","# Stage 2 : Feature Engineering"],"metadata":{"id":"RddhKbcP1eog"}},{"cell_type":"code","source":["!pip install ta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhFNyKtV191h","executionInfo":{"status":"ok","timestamp":1753419339106,"user_tz":-420,"elapsed":7554,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"c15ae67d-c1f5-4905-d5cb-9518a5509e5e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ta\n","  Downloading ta-0.11.0.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n","Building wheels for collected packages: ta\n","  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=8639749a9e3b1d6f88f7cc9bd328505902575a07e4db28393f1a3e1a4887946b\n","  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n","Successfully built ta\n","Installing collected packages: ta\n","Successfully installed ta-0.11.0\n"]}]},{"cell_type":"code","source":["# ===================================================================\n","# 🧪 STAGE 2: FEATURE ENGINEERING - FIXED VERSION\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import ta\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def load_processed_data(processed_dir='processed_data'):\n","    \"\"\"Load all processed data from Stage 1\"\"\"\n","    print(\"📂 Loading processed data from Stage 1...\")\n","\n","    datasets = {}\n","    processed_files = [f for f in os.listdir(processed_dir) if f.endswith('_processed.csv')]\n","\n","    for file in processed_files:\n","        ticker = file.replace('_processed.csv', '')\n","        file_path = os.path.join(processed_dir, file)\n","\n","        try:\n","            df = pd.read_csv(file_path)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df = df.sort_values('Date').reset_index(drop=True)\n","            datasets[ticker] = df\n","            print(f\"   ✅ Loaded {ticker}: {len(df)} records\")\n","        except Exception as e:\n","            print(f\"   ❌ Error loading {ticker}: {str(e)}\")\n","\n","    print(f\"📊 Total datasets loaded: {len(datasets)}\")\n","    return datasets\n","\n","def add_moving_averages(df):\n","    \"\"\"Add Simple and Exponential Moving Averages\"\"\"\n","    print(\"   📈 Adding Moving Averages...\")\n","\n","    # Simple Moving Averages\n","    df['SMA_5'] = ta.trend.sma_indicator(df['Close'], window=5)\n","    df['SMA_10'] = ta.trend.sma_indicator(df['Close'], window=10)\n","    df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n","    df['SMA_50'] = ta.trend.sma_indicator(df['Close'], window=50)\n","\n","    # Exponential Moving Averages\n","    df['EMA_5'] = ta.trend.ema_indicator(df['Close'], window=5)\n","    df['EMA_10'] = ta.trend.ema_indicator(df['Close'], window=10)\n","    df['EMA_20'] = ta.trend.ema_indicator(df['Close'], window=20)\n","    df['EMA_50'] = ta.trend.ema_indicator(df['Close'], window=50)\n","\n","    # Moving Average Crossovers\n","    df['SMA_5_10_Cross'] = (df['SMA_5'] > df['SMA_10']).astype(int)\n","    df['EMA_5_20_Cross'] = (df['EMA_5'] > df['EMA_20']).astype(int)\n","\n","    print(f\"      ✅ Added 10 Moving Average indicators\")\n","    return df\n","\n","def add_rsi_indicators(df):\n","    \"\"\"Add RSI (Relative Strength Index) indicators\"\"\"\n","    print(\"   📊 Adding RSI indicators...\")\n","\n","    # Standard RSI\n","    df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n","    df['RSI_7'] = ta.momentum.rsi(df['Close'], window=7)\n","    df['RSI_21'] = ta.momentum.rsi(df['Close'], window=21)\n","\n","    # RSI signals\n","    df['RSI_Overbought'] = (df['RSI_14'] > 70).astype(int)\n","    df['RSI_Oversold'] = (df['RSI_14'] < 30).astype(int)\n","    df['RSI_Neutral'] = ((df['RSI_14'] >= 30) & (df['RSI_14'] <= 70)).astype(int)\n","\n","    print(f\"      ✅ Added 6 RSI indicators\")\n","    return df\n","\n","def add_macd_indicators(df):\n","    \"\"\"Add MACD (Moving Average Convergence Divergence) indicators\"\"\"\n","    print(\"   📉 Adding MACD indicators...\")\n","\n","    # MACD components\n","    df['MACD'] = ta.trend.macd(df['Close'])\n","    df['MACD_Signal'] = ta.trend.macd_signal(df['Close'])\n","    df['MACD_Histogram'] = ta.trend.macd_diff(df['Close'])\n","\n","    # MACD signals\n","    df['MACD_Bullish'] = (df['MACD'] > df['MACD_Signal']).astype(int)\n","    df['MACD_Bearish'] = (df['MACD'] < df['MACD_Signal']).astype(int)\n","\n","    print(f\"      ✅ Added 5 MACD indicators\")\n","    return df\n","\n","def add_bollinger_bands(df):\n","    \"\"\"Add Bollinger Bands indicators\"\"\"\n","    print(\"   📏 Adding Bollinger Bands...\")\n","\n","    # Bollinger Bands\n","    df['BB_High'] = ta.volatility.bollinger_hband(df['Close'])\n","    df['BB_Low'] = ta.volatility.bollinger_lband(df['Close'])\n","    df['BB_Mid'] = ta.volatility.bollinger_mavg(df['Close'])\n","    df['BB_Width'] = df['BB_High'] - df['BB_Low']\n","    df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","\n","    # Bollinger Bands signals\n","    df['BB_Squeeze'] = (df['BB_Width'] < df['BB_Width'].rolling(20).mean()).astype(int)\n","    df['BB_Upper_Break'] = (df['Close'] > df['BB_High']).astype(int)\n","    df['BB_Lower_Break'] = (df['Close'] < df['BB_Low']).astype(int)\n","\n","    print(f\"      ✅ Added 8 Bollinger Bands indicators\")\n","    return df\n","\n","def add_volume_indicators(df):\n","    \"\"\"Add Volume-based indicators - FIXED VERSION\"\"\"\n","    print(\"   📊 Adding Volume indicators...\")\n","\n","    # Custom Volume Moving Averages (since ta.volume.volume_sma doesn't exist)\n","    df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n","    df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean()\n","\n","    # Volume Weighted Average Price (manual calculation)\n","    df['VWAP'] = (df['Close'] * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()\n","\n","    # On Balance Volume\n","    df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n","\n","    # Volume Price Trend\n","    df['VPT'] = ta.volume.volume_price_trend(df['Close'], df['Volume'])\n","\n","    # Accumulation/Distribution Line\n","    df['ADL'] = ta.volume.acc_dist_index(df['High'], df['Low'], df['Close'], df['Volume'])\n","\n","    # Volume signals\n","    df['High_Volume'] = (df['Volume'] > df['Volume_MA_20'] * 1.5).astype(int)\n","    df['Low_Volume'] = (df['Volume'] < df['Volume_MA_20'] * 0.5).astype(int)\n","\n","    print(f\"      ✅ Added 8 Volume indicators\")\n","    return df\n","\n","def add_momentum_indicators(df):\n","    \"\"\"Add Momentum indicators\"\"\"\n","    print(\"   🚀 Adding Momentum indicators...\")\n","\n","    # Rate of Change\n","    df['ROC_5'] = ta.momentum.roc(df['Close'], window=5)\n","    df['ROC_10'] = ta.momentum.roc(df['Close'], window=10)\n","    df['ROC_20'] = ta.momentum.roc(df['Close'], window=20)\n","\n","    # Stochastic Oscillator\n","    df['Stoch_K'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n","    df['Stoch_D'] = ta.momentum.stoch_signal(df['High'], df['Low'], df['Close'])\n","\n","    # Williams %R\n","    df['Williams_R'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'])\n","\n","    # Momentum signals\n","    df['Strong_Momentum'] = (df['ROC_10'] > 2).astype(int)\n","    df['Weak_Momentum'] = (df['ROC_10'] < -2).astype(int)\n","\n","    print(f\"      ✅ Added 8 Momentum indicators\")\n","    return df\n","\n","def add_volatility_indicators(df):\n","    \"\"\"Add Volatility indicators\"\"\"\n","    print(\"   📊 Adding Volatility indicators...\")\n","\n","    # Average True Range\n","    df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n","\n","    # Volatility (Rolling Standard Deviation)\n","    df['Volatility_10'] = df['Close'].rolling(window=10).std()\n","    df['Volatility_20'] = df['Close'].rolling(window=20).std()\n","\n","    # Price Range indicators\n","    df['Daily_Range'] = df['High'] - df['Low']\n","    df['Daily_Range_Pct'] = (df['Daily_Range'] / df['Close']) * 100\n","\n","    # Volatility signals\n","    df['High_Volatility'] = (df['Volatility_20'] > df['Volatility_20'].rolling(50).mean() * 1.2).astype(int)\n","    df['Low_Volatility'] = (df['Volatility_20'] < df['Volatility_20'].rolling(50).mean() * 0.8).astype(int)\n","\n","    print(f\"      ✅ Added 7 Volatility indicators\")\n","    return df\n","\n","def add_price_patterns(df):\n","    \"\"\"Add Price Pattern indicators\"\"\"\n","    print(\"   📈 Adding Price Pattern indicators...\")\n","\n","    # Daily Returns\n","    df['Daily_Return'] = df['Close'].pct_change()\n","    df['Daily_Return_Abs'] = abs(df['Daily_Return'])\n","\n","    # Gap indicators\n","    df['Gap_Up'] = (df['Open'] > df['Close'].shift(1)).astype(int)\n","    df['Gap_Down'] = (df['Open'] < df['Close'].shift(1)).astype(int)\n","\n","    # Doji pattern (Open ≈ Close)\n","    df['Doji'] = (abs(df['Open'] - df['Close']) / df['Close'] < 0.01).astype(int)\n","\n","    # Hammer/Shooting Star patterns\n","    body_size = abs(df['Close'] - df['Open'])\n","    upper_shadow = df['High'] - np.maximum(df['Open'], df['Close'])\n","    lower_shadow = np.minimum(df['Open'], df['Close']) - df['Low']\n","\n","    df['Hammer'] = ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)\n","    df['Shooting_Star'] = ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)\n","\n","    # Support/Resistance levels\n","    df['Near_High_52w'] = (df['Close'] > df['Close'].rolling(252).max() * 0.95).astype(int)\n","    df['Near_Low_52w'] = (df['Close'] < df['Close'].rolling(252).min() * 1.05).astype(int)\n","\n","    print(f\"      ✅ Added 10 Price Pattern indicators\")\n","    return df\n","\n","def add_advanced_indicators(df):\n","    \"\"\"Add Advanced technical indicators\"\"\"\n","    print(\"   🎯 Adding Advanced indicators...\")\n","\n","    # Fibonacci Retracement levels\n","    high_252 = df['High'].rolling(252).max()\n","    low_252 = df['Low'].rolling(252).min()\n","    fib_range = high_252 - low_252\n","\n","    df['Fib_23_6'] = high_252 - (fib_range * 0.236)\n","    df['Fib_38_2'] = high_252 - (fib_range * 0.382)\n","    df['Fib_61_8'] = high_252 - (fib_range * 0.618)\n","\n","    # Ichimoku Cloud components\n","    high_9 = df['High'].rolling(9).max()\n","    low_9 = df['Low'].rolling(9).min()\n","    high_26 = df['High'].rolling(26).max()\n","    low_26 = df['Low'].rolling(26).min()\n","\n","    df['Tenkan_Sen'] = (high_9 + low_9) / 2\n","    df['Kijun_Sen'] = (high_26 + low_26) / 2\n","    df['Senkou_A'] = ((df['Tenkan_Sen'] + df['Kijun_Sen']) / 2).shift(26)\n","\n","    # Commodity Channel Index\n","    df['CCI'] = ta.trend.cci(df['High'], df['Low'], df['Close'])\n","\n","    print(f\"      ✅ Added 9 Advanced indicators\")\n","    return df\n","\n","def create_target_labels(df):\n","    \"\"\"Create target labels for Random Forest (Stage 4)\"\"\"\n","    print(\"   🎯 Creating target labels...\")\n","\n","    # Next day price changes\n","    df['Next_Open'] = df['Open'].shift(-1)\n","    df['Next_Close'] = df['Close'].shift(-1)\n","    df['Next_High'] = df['High'].shift(-1)\n","    df['Next_Low'] = df['Low'].shift(-1)\n","\n","    # Price change percentages\n","    df['Price_Change_Pct'] = ((df['Next_Close'] - df['Close']) / df['Close']) * 100\n","\n","    # Trend labels (sesuai arsitektur.md)\n","    def classify_trend(price_change):\n","        if pd.isna(price_change):\n","            return 'UNKNOWN'\n","        elif price_change > 1.0:  # >1% increase\n","            return 'UP'\n","        elif price_change < -1.0:  # >1% decrease\n","            return 'DOWN'\n","        else:\n","            return 'STAY'\n","\n","    df['Trend_Label'] = df['Price_Change_Pct'].apply(classify_trend)\n","\n","    # 5-day ahead targets (for LSTM)\n","    for i in range(1, 6):  # 1 to 5 days ahead\n","        df[f'Target_Open_Day{i}'] = df['Open'].shift(-i)\n","        df[f'Target_Close_Day{i}'] = df['Close'].shift(-i)\n","\n","    print(f\"      ✅ Added target labels for prediction\")\n","    return df\n","\n","def validate_features(df, ticker):\n","    \"\"\"Validate feature engineering results\"\"\"\n","    print(f\"   🔍 Validating features for {ticker}...\")\n","\n","    issues = []\n","\n","    # Check for infinite values\n","    inf_cols = df.columns[df.isin([np.inf, -np.inf]).any()].tolist()\n","    if inf_cols:\n","        issues.append(f\"Infinite values in: {inf_cols}\")\n","        # Replace inf with NaN\n","        df = df.replace([np.inf, -np.inf], np.nan)\n","\n","    # Check percentage of missing values\n","    missing_pct = (df.isnull().sum() / len(df)) * 100\n","    high_missing = missing_pct[missing_pct > 50].index.tolist()\n","    if high_missing:\n","        issues.append(f\"High missing data (>50%): {high_missing}\")\n","\n","    # Check feature count\n","    feature_cols = [col for col in df.columns if col not in ['Date', 'Ticker']]\n","    print(f\"      📊 Total features created: {len(feature_cols)}\")\n","\n","    # Check data range\n","    if len(df) < 100:\n","        issues.append(f\"Insufficient data: {len(df)} records\")\n","\n","    if issues:\n","        print(f\"      ⚠️ Validation issues: {issues}\")\n","    else:\n","        print(f\"      ✅ Validation passed\")\n","\n","    return df, issues\n","\n","def save_feature_engineered_data(df, ticker, output_dir='feature_engineered_data'):\n","    \"\"\"Save feature engineered data\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    filename = f\"{output_dir}/{ticker}_fe.csv\"\n","    df.to_csv(filename, index=False)\n","\n","    print(f\"   💾 Saved: {filename}\")\n","    return filename\n","\n","def generate_feature_report(processed_results):\n","    \"\"\"Generate comprehensive feature engineering report - FIXED VERSION\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(\"📋 STAGE 2 FEATURE ENGINEERING REPORT\")\n","    print(\"=\"*80)\n","\n","    successful = [r for r in processed_results if r['status'] == 'SUCCESS']\n","    failed = [r for r in processed_results if r['status'] != 'SUCCESS']\n","\n","    print(f\"✅ Successfully processed: {len(successful)}/11 files\")\n","    print(f\"❌ Failed: {len(failed)}/11 files\")\n","\n","    # Initialize total_features\n","    total_features = 0\n","\n","    if successful:\n","        print(f\"\\n📊 Feature Engineering Summary:\")\n","        total_features = successful[0]['feature_count'] if successful else 0\n","        for result in successful:\n","            print(f\"   ✅ {result['ticker']}: {result['feature_count']} features, {result['records']} records\")\n","\n","        print(f\"\\n🎯 Average features per dataset: {total_features}\")\n","\n","        # Quality issues summary\n","        all_issues = []\n","        for result in successful:\n","            all_issues.extend(result['issues'])\n","\n","        if all_issues:\n","            print(f\"\\n⚠️ Data Quality Issues ({len(all_issues)} total):\")\n","            issue_counts = {}\n","            for issue in all_issues:\n","                issue_type = issue.split(':')[0]\n","                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1\n","\n","            for issue_type, count in issue_counts.items():\n","                print(f\"   ⚠️ {issue_type}: {count} occurrences\")\n","\n","    if failed:\n","        print(f\"\\n❌ Failed Files:\")\n","        for result in failed:\n","            print(f\"   ❌ {result['ticker']}: {result['error']}\")\n","\n","    return {'successful': len(successful), 'failed': len(failed), 'avg_features': total_features}\n","\n","def main_feature_engineering():\n","    \"\"\"Main feature engineering execution\"\"\"\n","    print(\"🧪 STARTING STAGE 2: FEATURE ENGINEERING - FIXED VERSION\")\n","    print(\"=\"*80)\n","\n","    # Load processed data from Stage 1\n","    datasets = load_processed_data()\n","\n","    if not datasets:\n","        print(\"❌ No processed data found! Run Stage 1 first.\")\n","        return None, None\n","\n","    results = []\n","\n","    for ticker, df in datasets.items():\n","        print(f\"\\n{'='*60}\")\n","        print(f\"🔬 Feature Engineering: {ticker}\")\n","        print(f\"{'='*60}\")\n","        print(f\"📊 Input: {len(df)} records, {len(df.columns)} columns\")\n","\n","        try:\n","            # Apply all feature engineering functions\n","            df = add_moving_averages(df)\n","            df = add_rsi_indicators(df)\n","            df = add_macd_indicators(df)\n","            df = add_bollinger_bands(df)\n","            df = add_volume_indicators(df)  # FIXED VERSION\n","            df = add_momentum_indicators(df)\n","            df = add_volatility_indicators(df)\n","            df = add_price_patterns(df)\n","            df = add_advanced_indicators(df)\n","            df = create_target_labels(df)\n","\n","            # Validate results\n","            df, issues = validate_features(df, ticker)\n","\n","            # Save feature engineered data\n","            saved_file = save_feature_engineered_data(df, ticker)\n","\n","            feature_count = len([col for col in df.columns if col not in ['Date', 'Ticker']])\n","\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'SUCCESS',\n","                'records': len(df),\n","                'feature_count': feature_count,\n","                'issues': issues,\n","                'saved_file': saved_file\n","            })\n","\n","            print(f\"✅ {ticker} feature engineering completed!\")\n","            print(f\"   📊 Output: {len(df)} records, {feature_count} features\")\n","\n","        except Exception as e:\n","            results.append({\n","                'ticker': ticker,\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'records': 0,\n","                'feature_count': 0,\n","                'issues': [],\n","                'saved_file': None\n","            })\n","            print(f\"❌ Error processing {ticker}: {str(e)}\")\n","\n","    # Generate comprehensive report\n","    report = generate_feature_report(results)\n","\n","    print(f\"\\n🎉 STAGE 2 FEATURE ENGINEERING COMPLETED!\")\n","    print(f\"📂 Feature engineered data ready in: feature_engineered_data/ directory\")\n","    print(f\"🔜 Ready for STAGE 3: LSTM Model Training\")\n","\n","    return results, report\n","\n","# 🚀 RUN FEATURE ENGINEERING\n","if __name__ == \"__main__\":\n","    results, report = main_feature_engineering()\n","\n","print(\"\\n🎯 STAGE 2 EXECUTION COMPLETED!\")\n","print(\"📝 Next: STAGE 3 - LSTM Model Training\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_YsbOZV1k3Y","executionInfo":{"status":"ok","timestamp":1753419361516,"user_tz":-420,"elapsed":12494,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"4b8d4961-98a8-42cc-999b-0644824b2bb7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 STARTING STAGE 2: FEATURE ENGINEERING - FIXED VERSION\n","================================================================================\n","📂 Loading processed data from Stage 1...\n","   ✅ Loaded NFLX: 5830 records\n","   ✅ Loaded AMZN: 2514 records\n","   ✅ Loaded JPM: 6471 records\n","   ✅ Loaded BAC: 6471 records\n","   ✅ Loaded AAPL: 6471 records\n","   ✅ Loaded GOOGL: 2514 records\n","   ✅ Loaded BBCA: 2592 records\n","   ✅ Loaded MSFT: 2514 records\n","   ✅ Loaded NVDA: 6471 records\n","   ✅ Loaded META: 3314 records\n","   ✅ Loaded TSLA: 2514 records\n","📊 Total datasets loaded: 11\n","\n","============================================================\n","🔬 Feature Engineering: NFLX\n","============================================================\n","📊 Input: 5830 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for NFLX...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/NFLX_fe.csv\n","✅ NFLX feature engineering completed!\n","   📊 Output: 5830 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: AMZN\n","============================================================\n","📊 Input: 2514 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for AMZN...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/AMZN_fe.csv\n","✅ AMZN feature engineering completed!\n","   📊 Output: 2514 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: JPM\n","============================================================\n","📊 Input: 6471 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for JPM...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/JPM_fe.csv\n","✅ JPM feature engineering completed!\n","   📊 Output: 6471 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: BAC\n","============================================================\n","📊 Input: 6471 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for BAC...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/BAC_fe.csv\n","✅ BAC feature engineering completed!\n","   📊 Output: 6471 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: AAPL\n","============================================================\n","📊 Input: 6471 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for AAPL...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/AAPL_fe.csv\n","✅ AAPL feature engineering completed!\n","   📊 Output: 6471 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: GOOGL\n","============================================================\n","📊 Input: 2514 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for GOOGL...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/GOOGL_fe.csv\n","✅ GOOGL feature engineering completed!\n","   📊 Output: 2514 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: BBCA\n","============================================================\n","📊 Input: 2592 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for BBCA...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/BBCA_fe.csv\n","✅ BBCA feature engineering completed!\n","   📊 Output: 2592 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: MSFT\n","============================================================\n","📊 Input: 2514 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for MSFT...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/MSFT_fe.csv\n","✅ MSFT feature engineering completed!\n","   📊 Output: 2514 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: NVDA\n","============================================================\n","📊 Input: 6471 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for NVDA...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/NVDA_fe.csv\n","✅ NVDA feature engineering completed!\n","   📊 Output: 6471 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: META\n","============================================================\n","📊 Input: 3314 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for META...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/META_fe.csv\n","✅ META feature engineering completed!\n","   📊 Output: 3314 records, 89 features\n","\n","============================================================\n","🔬 Feature Engineering: TSLA\n","============================================================\n","📊 Input: 2514 records, 7 columns\n","   📈 Adding Moving Averages...\n","      ✅ Added 10 Moving Average indicators\n","   📊 Adding RSI indicators...\n","      ✅ Added 6 RSI indicators\n","   📉 Adding MACD indicators...\n","      ✅ Added 5 MACD indicators\n","   📏 Adding Bollinger Bands...\n","      ✅ Added 8 Bollinger Bands indicators\n","   📊 Adding Volume indicators...\n","      ✅ Added 8 Volume indicators\n","   🚀 Adding Momentum indicators...\n","      ✅ Added 8 Momentum indicators\n","   📊 Adding Volatility indicators...\n","      ✅ Added 7 Volatility indicators\n","   📈 Adding Price Pattern indicators...\n","      ✅ Added 10 Price Pattern indicators\n","   🎯 Adding Advanced indicators...\n","      ✅ Added 9 Advanced indicators\n","   🎯 Creating target labels...\n","      ✅ Added target labels for prediction\n","   🔍 Validating features for TSLA...\n","      📊 Total features created: 89\n","      ✅ Validation passed\n","   💾 Saved: feature_engineered_data/TSLA_fe.csv\n","✅ TSLA feature engineering completed!\n","   📊 Output: 2514 records, 89 features\n","\n","================================================================================\n","📋 STAGE 2 FEATURE ENGINEERING REPORT\n","================================================================================\n","✅ Successfully processed: 11/11 files\n","❌ Failed: 0/11 files\n","\n","📊 Feature Engineering Summary:\n","   ✅ NFLX: 89 features, 5830 records\n","   ✅ AMZN: 89 features, 2514 records\n","   ✅ JPM: 89 features, 6471 records\n","   ✅ BAC: 89 features, 6471 records\n","   ✅ AAPL: 89 features, 6471 records\n","   ✅ GOOGL: 89 features, 2514 records\n","   ✅ BBCA: 89 features, 2592 records\n","   ✅ MSFT: 89 features, 2514 records\n","   ✅ NVDA: 89 features, 6471 records\n","   ✅ META: 89 features, 3314 records\n","   ✅ TSLA: 89 features, 2514 records\n","\n","🎯 Average features per dataset: 89\n","\n","🎉 STAGE 2 FEATURE ENGINEERING COMPLETED!\n","📂 Feature engineered data ready in: feature_engineered_data/ directory\n","🔜 Ready for STAGE 3: LSTM Model Training\n","\n","🎯 STAGE 2 EXECUTION COMPLETED!\n","📝 Next: STAGE 3 - LSTM Model Training\n"]}]},{"cell_type":"markdown","source":["# Stage 3 : Training Data Model LSTM"],"metadata":{"id":"6uTpl9Pg6aO_"}},{"cell_type":"code","source":["# ===================================================================\n","# 🧠 STAGE 3: GPU-OPTIMIZED UNIFIED LSTM TRAINING (T4 15GB)\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import gc\n","import warnings\n","import pickle\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# TensorFlow with GPU optimization\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.mixed_precision import set_global_policy\n","\n","warnings.filterwarnings('ignore')\n","\n","def setup_gpu_optimization():\n","    \"\"\"Setup GPU optimization for T4\"\"\"\n","    print(\"🚀 Setting up GPU optimization for T4...\")\n","\n","    # Enable mixed precision for better performance\n","    set_global_policy('mixed_float16')\n","\n","    # Configure GPU memory growth\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            print(f\"   ✅ Found {len(gpus)} GPU(s), memory growth enabled\")\n","        except RuntimeError as e:\n","            print(f\"   ⚠️ GPU setup error: {e}\")\n","    else:\n","        print(\"   ⚠️ No GPU found, using CPU\")\n","\n","    # Clear session and force garbage collection\n","    tf.keras.backend.clear_session()\n","    gc.collect()\n","\n","    return gpus is not None and len(gpus) > 0\n","\n","def load_optimized_datasets(fe_dir='feature_engineered_data', sample_ratio=0.8):\n","    \"\"\"Load datasets with memory optimization\"\"\"\n","    print(f\"📂 Loading datasets with optimization (sample_ratio={sample_ratio})...\")\n","\n","    fe_files = [f for f in os.listdir(fe_dir) if f.endswith('_fe.csv')]\n","    datasets = []\n","\n","    for file in fe_files:\n","        ticker = file.replace('_fe.csv', '')\n","        file_path = os.path.join(fe_dir, file)\n","\n","        try:\n","            # Load with optimized dtypes\n","            df = pd.read_csv(file_path, low_memory=False)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df['Ticker'] = ticker\n","\n","            # Sample data to reduce memory usage\n","            if sample_ratio < 1.0:\n","                sample_size = int(len(df) * sample_ratio)\n","                df = df.tail(sample_size)  # Take most recent data\n","\n","            # Optimize dtypes to save memory\n","            for col in df.select_dtypes(include=['float64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='float')\n","\n","            for col in df.select_dtypes(include=['int64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='integer')\n","\n","            datasets.append(df)\n","            print(f\"   ✅ {ticker}: {len(df)} records, {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n","\n","        except Exception as e:\n","            print(f\"   ❌ Error loading {ticker}: {str(e)}\")\n","\n","    # Combine datasets\n","    combined_df = pd.concat(datasets, ignore_index=True)\n","    combined_df = combined_df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n","\n","    # Final memory optimization\n","    gc.collect()\n","\n","    print(f\"✅ Combined dataset: {len(combined_df)} records, {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n","    return combined_df\n","\n","def select_essential_features(df):\n","    \"\"\"Select only essential features to reduce memory\"\"\"\n","    print(\"🎯 Selecting essential features for GPU training...\")\n","\n","    # Core essential features (most predictive)\n","    essential_features = [\n","        # Price data\n","        'Open', 'High', 'Low', 'Close', 'Volume',\n","\n","        # Key moving averages\n","        'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20',\n","\n","        # Critical momentum indicators\n","        'RSI_14', 'MACD', 'MACD_Signal',\n","\n","        # Bollinger Bands\n","        'BB_Position', 'BB_Width',\n","\n","        # Volume indicators\n","        'Volume_MA_10', 'OBV',\n","\n","        # Volatility\n","        'ATR', 'Volatility_20',\n","\n","        # Price patterns\n","        'Daily_Return', 'ROC_10'\n","    ]\n","\n","    # Check which features exist\n","    existing_features = [col for col in essential_features if col in df.columns]\n","    print(f\"   ✅ Selected {len(existing_features)} essential features\")\n","    print(f\"   📋 Features: {existing_features}\")\n","\n","    return existing_features\n","\n","def create_memory_efficient_sequences(df, features, sequence_length=60, batch_size=1000):\n","    \"\"\"Create sequences in batches to manage memory\"\"\"\n","    print(f\"📦 Creating sequences in batches (batch_size={batch_size})...\")\n","\n","    # Encode ticker\n","    label_encoder = LabelEncoder()\n","    df['Ticker_Encoded'] = label_encoder.fit_transform(df['Ticker'])\n","\n","    final_features = features + ['Ticker_Encoded']\n","\n","    # Handle missing values efficiently\n","    df[final_features] = df[final_features].fillna(method='ffill').fillna(method='bfill')\n","\n","    # Create target columns if not exist\n","    target_cols = []\n","    for day in range(1, 6):\n","        open_col = f'Target_Open_Day{day}'\n","        close_col = f'Target_Close_Day{day}'\n","\n","        if open_col not in df.columns:\n","            df[open_col] = df.groupby('Ticker')['Open'].shift(-day)\n","        if close_col not in df.columns:\n","            df[close_col] = df.groupby('Ticker')['Close'].shift(-day)\n","\n","        target_cols.extend([open_col, close_col])\n","\n","    # Create sequences by ticker to maintain chronological order\n","    all_X = []\n","    all_y = []\n","\n","    for ticker in df['Ticker'].unique():\n","        ticker_data = df[df['Ticker'] == ticker].copy()\n","        ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n","\n","        # Create sequences for this ticker\n","        ticker_X = []\n","        ticker_y = []\n","\n","        for i in range(sequence_length, len(ticker_data) - 5):\n","            # Input sequence\n","            x_seq = ticker_data[final_features].iloc[i-sequence_length:i].values\n","\n","            # Target\n","            y_seq = ticker_data[target_cols].iloc[i].values\n","\n","            if not np.isnan(y_seq).any():\n","                ticker_X.append(x_seq)\n","                ticker_y.append(y_seq)\n","\n","        if ticker_X:\n","            all_X.extend(ticker_X)\n","            all_y.extend(ticker_y)\n","\n","        print(f\"   {ticker}: {len(ticker_X)} sequences\")\n","\n","    X = np.array(all_X, dtype=np.float32)  # Use float32 to save memory\n","    y = np.array(all_y, dtype=np.float32)\n","\n","    print(f\"   ✅ Total sequences: {len(X)}\")\n","    print(f\"   📊 X shape: {X.shape}, Y shape: {y.shape}\")\n","    print(f\"   💾 Memory usage: {X.nbytes / 1024**2:.1f} MB (X) + {y.nbytes / 1024**2:.1f} MB (y)\")\n","\n","    return X, y, label_encoder, final_features, target_cols\n","\n","def build_gpu_optimized_model(input_shape, output_dim=10):\n","    \"\"\"Build GPU-optimized LSTM model\"\"\"\n","    print(f\"🏗️ Building GPU-optimized LSTM model...\")\n","\n","    model = Sequential([\n","        # Optimized LSTM layers for T4\n","        LSTM(96, return_sequences=True, input_shape=input_shape,\n","             dtype='float32', recurrent_dropout=0.1),\n","        Dropout(0.3),\n","\n","        LSTM(48, return_sequences=False, dtype='float32', recurrent_dropout=0.1),\n","        Dropout(0.2),\n","\n","        # Dense layers\n","        Dense(24, activation='relu', dtype='float32'),\n","        Dropout(0.2),\n","        Dense(output_dim, activation='linear', dtype='float32')\n","    ])\n","\n","    # Optimizer with mixed precision\n","    optimizer = Adam(learning_rate=0.001)\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss='mse',\n","        metrics=['mae']\n","    )\n","\n","    print(f\"   ✅ Model built: {model.count_params():,} parameters\")\n","    return model\n","\n","def train_with_memory_management(model, X, y, validation_split=0.2, epochs=30, batch_size=32):\n","    \"\"\"Train model with memory management\"\"\"\n","    print(f\"🎯 Training with memory management...\")\n","\n","    # Split data\n","    split_idx = int(len(X) * (1 - validation_split))\n","    X_train, X_val = X[:split_idx], X[split_idx:]\n","    y_train, y_val = y[:split_idx], y[split_idx:]\n","\n","    print(f\"   📊 Train: {len(X_train)}, Val: {len(X_val)}\")\n","\n","    # Callbacks\n","    callbacks = [\n","        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n","        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6)\n","    ]\n","\n","    # Train with generator to save memory\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        callbacks=callbacks,\n","        verbose=1,\n","        shuffle=False\n","    )\n","\n","    # Clean up\n","    del X_train, X_val, y_train, y_val\n","    gc.collect()\n","\n","    return history\n","\n","def evaluate_and_save_model(model, X_test, y_test, feature_scaler, target_scaler,\n","                           ticker_encoder, feature_cols, target_cols):\n","    \"\"\"Evaluate and save the model\"\"\"\n","    print(f\"📊 Evaluating model...\")\n","\n","    # Predictions\n","    y_pred_scaled = model.predict(X_test, batch_size=64)\n","\n","    # Inverse transform\n","    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n","    y_actual = target_scaler.inverse_transform(y_test)\n","\n","    # Metrics\n","    mse = mean_squared_error(y_actual, y_pred)\n","    mae = mean_absolute_error(y_actual, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_actual, y_pred)\n","\n","    print(f\"   📈 Performance:\")\n","    print(f\"      RMSE: {rmse:.4f}\")\n","    print(f\"      MAE: {mae:.4f}\")\n","    print(f\"      R²: {r2:.4f}\")\n","\n","    # Save model\n","    print(f\"💾 Saving model...\")\n","    os.makedirs('unified_lstm_model', exist_ok=True)\n","\n","    model.save('unified_lstm_model/unified_lstm_model.h5')\n","\n","    with open('unified_lstm_model/feature_scaler.pkl', 'wb') as f:\n","        pickle.dump(feature_scaler, f)\n","\n","    with open('unified_lstm_model/target_scaler.pkl', 'wb') as f:\n","        pickle.dump(target_scaler, f)\n","\n","    with open('unified_lstm_model/ticker_encoder.pkl', 'wb') as f:\n","        pickle.dump(ticker_encoder, f)\n","\n","    model_info = {\n","        'feature_columns': feature_cols,\n","        'target_columns': target_cols,\n","        'performance': {'rmse': rmse, 'mae': mae, 'r2': r2}\n","    }\n","\n","    with open('unified_lstm_model/model_info.pkl', 'wb') as f:\n","        pickle.dump(model_info, f)\n","\n","    print(f\"   ✅ Model saved to: unified_lstm_model/\")\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'r2': r2,\n","        'predictions': y_pred,\n","        'actual': y_actual\n","    }\n","\n","def main_gpu_optimized_training():\n","    \"\"\"Main GPU-optimized training function\"\"\"\n","    print(\"🚀 STARTING GPU-OPTIMIZED UNIFIED LSTM TRAINING\")\n","    print(\"=\"*70)\n","\n","    try:\n","        # Setup GPU\n","        gpu_available = setup_gpu_optimization()\n","\n","        # Load data\n","        combined_df = load_optimized_datasets(sample_ratio=0.9)  # Use 90% of data\n","\n","        # Select features\n","        features = select_essential_features(combined_df)\n","\n","        # Create sequences\n","        X, y, ticker_encoder, feature_cols, target_cols = create_memory_efficient_sequences(\n","            combined_df, features, sequence_length=60\n","        )\n","\n","        # Scale data\n","        print(\"📏 Scaling data...\")\n","\n","        # Reshape for scaling\n","        original_shape = X.shape\n","        X_reshaped = X.reshape(-1, X.shape[-1])\n","\n","        feature_scaler = MinMaxScaler()\n","        X_scaled_reshaped = feature_scaler.fit_transform(X_reshaped)\n","        X_scaled = X_scaled_reshaped.reshape(original_shape).astype(np.float32)\n","\n","        target_scaler = MinMaxScaler()\n","        y_scaled = target_scaler.fit_transform(y).astype(np.float32)\n","\n","        # Clean up original data\n","        del X, y, X_reshaped, X_scaled_reshaped\n","        gc.collect()\n","\n","        # Split data\n","        test_size = 0.15\n","        split_idx = int(len(X_scaled) * (1 - test_size))\n","\n","        X_train_val = X_scaled[:split_idx]\n","        X_test = X_scaled[split_idx:]\n","        y_train_val = y_scaled[:split_idx]\n","        y_test = y_scaled[split_idx:]\n","\n","        print(f\"📊 Data split: Train+Val: {len(X_train_val)}, Test: {len(X_test)}\")\n","\n","        # Build model\n","        model = build_gpu_optimized_model(\n","            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n","            output_dim=len(target_cols)\n","        )\n","\n","        # Train model\n","        history = train_with_memory_management(\n","            model, X_train_val, y_train_val,\n","            validation_split=0.2, epochs=25, batch_size=64\n","        )\n","\n","        # Evaluate and save\n","        results = evaluate_and_save_model(\n","            model, X_test, y_test, feature_scaler, target_scaler,\n","            ticker_encoder, feature_cols, target_cols\n","        )\n","\n","        print(f\"\\n🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n","        print(f\"📊 Final Performance: RMSE={results['rmse']:.4f}, R²={results['r2']:.4f}\")\n","        print(f\"🔜 Ready for STAGE 4: Random Forest Training\")\n","\n","        return {\n","            'status': 'SUCCESS',\n","            'performance': results,\n","            'gpu_used': gpu_available\n","        }\n","\n","    except Exception as e:\n","        print(f\"\\n❌ TRAINING FAILED: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        return {'status': 'ERROR', 'error': str(e)}\n","\n","# 🚀 RUN GPU-OPTIMIZED TRAINING\n","if __name__ == \"__main__\":\n","    result = main_gpu_optimized_training()\n","\n","print(\"\\n✅ STAGE 3 EXECUTION COMPLETED!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTMQ1VdY9wRp","executionInfo":{"status":"ok","timestamp":1753424677851,"user_tz":-420,"elapsed":4864580,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"0cce899a-3e39-4c0f-d222-451e94978c56"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 STARTING GPU-OPTIMIZED UNIFIED LSTM TRAINING\n","======================================================================\n","🚀 Setting up GPU optimization for T4...\n","   ✅ Found 1 GPU(s), memory growth enabled\n","📂 Loading datasets with optimization (sample_ratio=0.9)...\n","   ✅ NFLX: 5247 records, 2.2 MB\n","   ✅ TSLA: 2262 records, 0.9 MB\n","   ✅ AAPL: 5823 records, 2.4 MB\n","   ✅ GOOGL: 2262 records, 0.9 MB\n","   ✅ META: 2982 records, 1.2 MB\n","   ✅ BBCA: 2332 records, 1.0 MB\n","   ✅ NVDA: 5823 records, 2.4 MB\n","   ✅ MSFT: 2262 records, 0.9 MB\n","   ✅ BAC: 5823 records, 2.4 MB\n","   ✅ JPM: 5823 records, 2.4 MB\n","   ✅ AMZN: 2262 records, 0.9 MB\n","✅ Combined dataset: 42901 records, 17.8 MB\n","🎯 Selecting essential features for GPU training...\n","   ✅ Selected 20 essential features\n","   📋 Features: ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width', 'Volume_MA_10', 'OBV', 'ATR', 'Volatility_20', 'Daily_Return', 'ROC_10']\n","📦 Creating sequences in batches (batch_size=1000)...\n","   AAPL: 5758 sequences\n","   AMZN: 2197 sequences\n","   BAC: 5758 sequences\n","   BBCA: 2267 sequences\n","   GOOGL: 2197 sequences\n","   JPM: 5758 sequences\n","   META: 2917 sequences\n","   MSFT: 2197 sequences\n","   NFLX: 5182 sequences\n","   NVDA: 5758 sequences\n","   TSLA: 2197 sequences\n","   ✅ Total sequences: 42186\n","   📊 X shape: (42186, 60, 21), Y shape: (42186, 10)\n","   💾 Memory usage: 202.8 MB (X) + 1.6 MB (y)\n","📏 Scaling data...\n","📊 Data split: Train+Val: 35858, Test: 6328\n","🏗️ Building GPU-optimized LSTM model...\n","   ✅ Model built: 74,578 parameters\n","🎯 Training with memory management...\n","   📊 Train: 28686, Val: 7172\n","Epoch 1/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 330ms/step - loss: 0.0074 - mae: 0.0299 - val_loss: 7.3783e-04 - val_mae: 0.0234 - learning_rate: 0.0010\n","Epoch 2/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 333ms/step - loss: 0.0056 - mae: 0.0315 - val_loss: 7.1267e-04 - val_mae: 0.0230 - learning_rate: 0.0010\n","Epoch 3/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 334ms/step - loss: 0.0065 - mae: 0.0361 - val_loss: 7.7334e-04 - val_mae: 0.0243 - learning_rate: 0.0010\n","Epoch 4/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 333ms/step - loss: 0.0031 - mae: 0.0326 - val_loss: 4.7924e-04 - val_mae: 0.0191 - learning_rate: 0.0010\n","Epoch 5/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 338ms/step - loss: 0.0035 - mae: 0.0277 - val_loss: 3.2286e-04 - val_mae: 0.0153 - learning_rate: 0.0010\n","Epoch 6/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 330ms/step - loss: 0.0023 - mae: 0.0241 - val_loss: 3.8463e-04 - val_mae: 0.0168 - learning_rate: 0.0010\n","Epoch 7/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 334ms/step - loss: 0.0018 - mae: 0.0216 - val_loss: 3.9488e-04 - val_mae: 0.0167 - learning_rate: 0.0010\n","Epoch 8/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 335ms/step - loss: 0.0017 - mae: 0.0205 - val_loss: 3.6696e-04 - val_mae: 0.0157 - learning_rate: 0.0010\n","Epoch 9/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 332ms/step - loss: 0.0014 - mae: 0.0176 - val_loss: 3.8611e-04 - val_mae: 0.0161 - learning_rate: 0.0010\n","Epoch 10/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 332ms/step - loss: 0.0012 - mae: 0.0166 - val_loss: 3.5067e-04 - val_mae: 0.0155 - learning_rate: 0.0010\n","Epoch 11/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 332ms/step - loss: 0.0011 - mae: 0.0150 - val_loss: 3.6062e-04 - val_mae: 0.0158 - learning_rate: 3.0000e-04\n","Epoch 12/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 336ms/step - loss: 0.0011 - mae: 0.0151 - val_loss: 3.8974e-04 - val_mae: 0.0163 - learning_rate: 3.0000e-04\n","Epoch 13/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 333ms/step - loss: 0.0011 - mae: 0.0156 - val_loss: 2.2318e-04 - val_mae: 0.0130 - learning_rate: 3.0000e-04\n","Epoch 14/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 335ms/step - loss: 8.9203e-04 - mae: 0.0133 - val_loss: 1.9862e-04 - val_mae: 0.0123 - learning_rate: 3.0000e-04\n","Epoch 15/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 334ms/step - loss: 8.5065e-04 - mae: 0.0126 - val_loss: 2.0764e-04 - val_mae: 0.0129 - learning_rate: 3.0000e-04\n","Epoch 16/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 332ms/step - loss: 8.7331e-04 - mae: 0.0129 - val_loss: 1.9096e-04 - val_mae: 0.0123 - learning_rate: 3.0000e-04\n","Epoch 17/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 333ms/step - loss: 7.9119e-04 - mae: 0.0122 - val_loss: 2.0476e-04 - val_mae: 0.0128 - learning_rate: 3.0000e-04\n","Epoch 18/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 337ms/step - loss: 7.8539e-04 - mae: 0.0119 - val_loss: 3.8855e-04 - val_mae: 0.0164 - learning_rate: 3.0000e-04\n","Epoch 19/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 335ms/step - loss: 8.1927e-04 - mae: 0.0138 - val_loss: 2.3677e-04 - val_mae: 0.0136 - learning_rate: 3.0000e-04\n","Epoch 20/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 335ms/step - loss: 8.4265e-04 - mae: 0.0127 - val_loss: 3.5619e-04 - val_mae: 0.0158 - learning_rate: 9.0000e-05\n","Epoch 21/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 336ms/step - loss: 9.3247e-04 - mae: 0.0136 - val_loss: 1.6256e-04 - val_mae: 0.0115 - learning_rate: 9.0000e-05\n","Epoch 22/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 331ms/step - loss: 7.4247e-04 - mae: 0.0115 - val_loss: 1.7128e-04 - val_mae: 0.0120 - learning_rate: 9.0000e-05\n","Epoch 23/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 328ms/step - loss: 6.8659e-04 - mae: 0.0113 - val_loss: 1.8096e-04 - val_mae: 0.0124 - learning_rate: 9.0000e-05\n","Epoch 24/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 324ms/step - loss: 7.0126e-04 - mae: 0.0115 - val_loss: 2.0767e-04 - val_mae: 0.0131 - learning_rate: 9.0000e-05\n","Epoch 25/25\n","\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 323ms/step - loss: 7.3321e-04 - mae: 0.0119 - val_loss: 1.7728e-04 - val_mae: 0.0120 - learning_rate: 2.7000e-05\n","📊 Evaluating model...\n","\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["   📈 Performance:\n","      RMSE: 127.3650\n","      MAE: 95.9342\n","      R²: 0.4464\n","💾 Saving model...\n","   ✅ Model saved to: unified_lstm_model/\n","\n","🎉 TRAINING COMPLETED SUCCESSFULLY!\n","📊 Final Performance: RMSE=127.3650, R²=0.4464\n","🔜 Ready for STAGE 4: Random Forest Training\n","\n","✅ STAGE 3 EXECUTION COMPLETED!\n"]}]},{"cell_type":"markdown","source":["# Stage 4 RF training"],"metadata":{"id":"ySU0NMRqQidR"}},{"cell_type":"markdown","source":["## Versi Tensoflow"],"metadata":{"id":"jahFHv4MZPqv"}},{"cell_type":"code","source":["# ===================================================================\n","# 🌲 STAGE 4: FIXED RANDOM FOREST - Versi Tensoflow\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import gc\n","from datetime import datetime\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","# TensorFlow with compatibility fixes\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError\n","\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","def setup_tensorflow_compatibility():\n","    \"\"\"Setup TensorFlow compatibility for model loading\"\"\"\n","    print(\"🔧 Setting up TensorFlow compatibility...\")\n","\n","    # Clear session\n","    tf.keras.backend.clear_session()\n","\n","    # GPU setup\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            print(f\"   ✅ GPU memory growth enabled\")\n","        except RuntimeError as e:\n","            print(f\"   ⚠️ GPU setup warning: {e}\")\n","\n","    gc.collect()\n","\n","def load_lstm_model_with_custom_objects():\n","    \"\"\"Load LSTM model with custom objects for compatibility\"\"\"\n","    print(\"🧠 Loading LSTM model with compatibility fixes...\")\n","\n","    model_dir = 'unified_lstm_model'\n","    lstm_model_path = f'{model_dir}/unified_lstm_model.h5'\n","\n","    if not os.path.exists(model_dir):\n","        raise FileNotFoundError(f\"LSTM model directory not found: {model_dir}\")\n","\n","    if not os.path.exists(lstm_model_path):\n","        raise FileNotFoundError(f\"LSTM model file not found: {lstm_model_path}\")\n","\n","    try:\n","        # Define custom objects for compatibility\n","        custom_objects = {\n","            'mse': 'mean_squared_error',\n","            'mae': 'mean_absolute_error',\n","            'mape': 'mean_absolute_percentage_error',\n","            'MeanSquaredError': MeanSquaredError,\n","            'MeanAbsoluteError': MeanAbsoluteError,\n","            'MeanAbsolutePercentageError': MeanAbsolutePercentageError\n","        }\n","\n","        # Try loading with custom objects\n","        print(\"   🔄 Loading model with custom objects...\")\n","        lstm_model = load_model(lstm_model_path, custom_objects=custom_objects)\n","        print(f\"   ✅ LSTM model loaded successfully with custom objects\")\n","\n","    except Exception as e1:\n","        print(f\"   ⚠️ Custom objects failed: {str(e1)}\")\n","\n","        try:\n","            # Try loading with compile=False\n","            print(\"   🔄 Loading model without compilation...\")\n","            lstm_model = load_model(lstm_model_path, compile=False)\n","\n","            # Recompile manually\n","            from tensorflow.keras.optimizers import Adam\n","            lstm_model.compile(\n","                optimizer=Adam(learning_rate=0.001),\n","                loss='mse',\n","                metrics=['mae']\n","            )\n","            print(f\"   ✅ LSTM model loaded and recompiled successfully\")\n","\n","        except Exception as e2:\n","            raise Exception(f\"Failed to load LSTM model. Tried custom objects: {str(e1)}. Tried compile=False: {str(e2)}\")\n","\n","    return lstm_model\n","\n","def load_lstm_components_fixed():\n","    \"\"\"Load LSTM components with fixes\"\"\"\n","    print(\"📦 Loading LSTM components with compatibility fixes...\")\n","\n","    model_dir = 'unified_lstm_model'\n","\n","    try:\n","        # Load LSTM model with fixes\n","        lstm_model = load_lstm_model_with_custom_objects()\n","\n","        # Load other components (these should work fine)\n","        with open(f'{model_dir}/feature_scaler.pkl', 'rb') as f:\n","            feature_scaler = pickle.load(f)\n","        print(f\"   ✅ Feature scaler loaded\")\n","\n","        with open(f'{model_dir}/target_scaler.pkl', 'rb') as f:\n","            target_scaler = pickle.load(f)\n","        print(f\"   ✅ Target scaler loaded\")\n","\n","        with open(f'{model_dir}/ticker_encoder.pkl', 'rb') as f:\n","            ticker_encoder = pickle.load(f)\n","        print(f\"   ✅ Ticker encoder loaded\")\n","\n","        with open(f'{model_dir}/model_info.pkl', 'rb') as f:\n","            model_info = pickle.load(f)\n","        print(f\"   ✅ Model info loaded\")\n","\n","        print(f\"   📊 Feature columns: {len(model_info['feature_columns'])}\")\n","        print(f\"   📊 Target columns: {len(model_info['target_columns'])}\")\n","\n","        return {\n","            'lstm_model': lstm_model,\n","            'feature_scaler': feature_scaler,\n","            'target_scaler': target_scaler,\n","            'ticker_encoder': ticker_encoder,\n","            'model_info': model_info\n","        }\n","\n","    except Exception as e:\n","        raise Exception(f\"Failed to load LSTM components: {str(e)}\")\n","\n","def load_feature_data_optimized():\n","    \"\"\"Load feature data optimized for processing\"\"\"\n","    print(\"📂 Loading feature engineered data...\")\n","\n","    fe_dir = 'feature_engineered_data'\n","    if not os.path.exists(fe_dir):\n","        raise FileNotFoundError(f\"Feature directory not found: {fe_dir}\")\n","\n","    all_datasets = []\n","    fe_files = [f for f in os.listdir(fe_dir) if f.endswith('_fe.csv')]\n","\n","    for file in fe_files:\n","        ticker = file.replace('_fe.csv', '')\n","        file_path = os.path.join(fe_dir, file)\n","\n","        try:\n","            df = pd.read_csv(file_path, low_memory=False)\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df['Ticker'] = ticker\n","\n","            # Take recent data (2000 records per stock for speed)\n","            df = df.sort_values('Date').tail(2000).reset_index(drop=True)\n","\n","            # Optimize dtypes\n","            for col in df.select_dtypes(include=['float64']).columns:\n","                df[col] = pd.to_numeric(df[col], downcast='float')\n","\n","            all_datasets.append(df)\n","            print(f\"   ✅ {ticker}: {len(df)} records\")\n","\n","        except Exception as e:\n","            print(f\"   ❌ Error loading {ticker}: {str(e)}\")\n","\n","    combined_df = pd.concat(all_datasets, ignore_index=True)\n","    combined_df = combined_df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n","\n","    gc.collect()\n","    print(f\"✅ Combined data: {len(combined_df)} records\")\n","\n","    return combined_df\n","\n","def generate_lstm_predictions_safe(combined_df, lstm_components):\n","    \"\"\"Generate LSTM predictions with error handling\"\"\"\n","    print(\"🎯 Generating LSTM predictions safely...\")\n","\n","    lstm_model = lstm_components['lstm_model']\n","    feature_scaler = lstm_components['feature_scaler']\n","    ticker_encoder = lstm_components['ticker_encoder']\n","    feature_columns = lstm_components['model_info']['feature_columns']\n","\n","    sequence_length = 60\n","    all_predictions = []\n","    prediction_metadata = []\n","\n","    for ticker in combined_df['Ticker'].unique()[:8]:  # Limit to 8 stocks for speed\n","        print(f\"   Processing {ticker}...\")\n","\n","        ticker_data = combined_df[combined_df['Ticker'] == ticker].copy()\n","        ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n","\n","        # Encode ticker safely\n","        try:\n","            ticker_encoded = ticker_encoder.transform([ticker])[0]\n","        except ValueError:\n","            print(f\"   ⚠️ Unknown ticker {ticker}, skipping...\")\n","            continue\n","\n","        ticker_data['Ticker_Encoded'] = ticker_encoded\n","\n","        # Handle missing values\n","        for col in feature_columns:\n","            if col in ticker_data.columns:\n","                ticker_data[col] = ticker_data[col].fillna(method='ffill').fillna(method='bfill')\n","                if ticker_data[col].isnull().any():\n","                    ticker_data[col] = ticker_data[col].fillna(0)\n","\n","        # Create sequences (limit to last 500 for speed)\n","        max_sequences = 500\n","        start_idx = max(sequence_length, len(ticker_data) - max_sequences - sequence_length)\n","\n","        ticker_sequences = []\n","        ticker_indices = []\n","\n","        for i in range(start_idx, len(ticker_data)):\n","            try:\n","                sequence = ticker_data[feature_columns].iloc[i-sequence_length:i].values\n","\n","                if sequence.shape == (sequence_length, len(feature_columns)) and not np.isnan(sequence).any():\n","                    ticker_sequences.append(sequence)\n","                    ticker_indices.append(i)\n","            except:\n","                continue\n","\n","        if ticker_sequences:\n","            # Process sequences\n","            X_ticker = np.array(ticker_sequences[:max_sequences], dtype=np.float32)\n","\n","            # Scale features\n","            original_shape = X_ticker.shape\n","            X_reshaped = X_ticker.reshape(-1, X_ticker.shape[-1])\n","\n","            try:\n","                X_scaled_reshaped = feature_scaler.transform(X_reshaped)\n","                X_scaled = X_scaled_reshaped.reshape(original_shape)\n","\n","                # Generate predictions in small batches\n","                batch_size = 50\n","                ticker_predictions = []\n","\n","                for batch_start in range(0, len(X_scaled), batch_size):\n","                    batch_end = min(batch_start + batch_size, len(X_scaled))\n","                    batch_X = X_scaled[batch_start:batch_end]\n","\n","                    try:\n","                        batch_pred = lstm_model.predict(batch_X, batch_size=16, verbose=0)\n","                        ticker_predictions.extend(batch_pred)\n","                    except Exception as e:\n","                        print(f\"      ⚠️ Prediction error for batch: {str(e)}\")\n","                        continue\n","\n","                # Store predictions\n","                for j, pred in enumerate(ticker_predictions):\n","                    if j < len(ticker_indices):\n","                        data_idx = ticker_indices[j]\n","                        all_predictions.append(pred)\n","                        prediction_metadata.append({\n","                            'ticker': ticker,\n","                            'date': ticker_data['Date'].iloc[data_idx],\n","                            'original_index': data_idx\n","                        })\n","\n","                print(f\"      ✅ {len(ticker_predictions)} predictions generated\")\n","\n","            except Exception as e:\n","                print(f\"      ❌ Error processing {ticker}: {str(e)}\")\n","                continue\n","\n","            # Clean up memory\n","            del X_ticker, X_scaled\n","            gc.collect()\n","\n","    if not all_predictions:\n","        raise Exception(\"No predictions were generated successfully!\")\n","\n","    predictions_array = np.array(all_predictions, dtype=np.float32)\n","    print(f\"   ✅ Total predictions: {len(predictions_array)}\")\n","\n","    return predictions_array, prediction_metadata\n","\n","def create_rf_dataset_simplified(combined_df, lstm_predictions, prediction_metadata):\n","    \"\"\"Create simplified RF dataset\"\"\"\n","    print(\"🔧 Creating simplified RF dataset...\")\n","\n","    rf_data = []\n","\n","    for i, (pred, meta) in enumerate(zip(lstm_predictions, prediction_metadata)):\n","        ticker = meta['ticker']\n","        date = meta['date']\n","\n","        # Find corresponding data\n","        ticker_data = combined_df[\n","            (combined_df['Ticker'] == ticker) &\n","            (combined_df['Date'] == date)\n","        ]\n","\n","        if len(ticker_data) == 0:\n","            continue\n","\n","        row = ticker_data.iloc[0]\n","\n","        # LSTM features\n","        lstm_features = {\n","            'LSTM_Day1_Open': float(pred[0]), 'LSTM_Day1_Close': float(pred[1]),\n","            'LSTM_Day2_Open': float(pred[2]), 'LSTM_Day2_Close': float(pred[3]),\n","            'LSTM_Day3_Open': float(pred[4]), 'LSTM_Day3_Close': float(pred[5]),\n","            'LSTM_Day4_Open': float(pred[6]), 'LSTM_Day4_Close': float(pred[7]),\n","            'LSTM_Day5_Open': float(pred[8]), 'LSTM_Day5_Close': float(pred[9])\n","        }\n","\n","        # Essential technical features\n","        tech_features = {}\n","        essential_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'Volatility_20'\n","        ]\n","\n","        for indicator in essential_indicators:\n","            if indicator in row:\n","                value = row[indicator]\n","                tech_features[f'Tech_{indicator}'] = float(value) if pd.notna(value) else 0.0\n","\n","        # Basic market features\n","        market_features = {\n","            'Current_Close': float(row['Close']),\n","            'Current_Volume': float(row['Volume']),\n","            'Price_Range': float(row['High'] - row['Low'])\n","        }\n","\n","        # Create trend labels (simplified)\n","        current_close = row['Close']\n","        targets = {}\n","\n","        # Simple future price lookup\n","        ticker_full_data = combined_df[combined_df['Ticker'] == ticker].sort_values('Date')\n","        current_row_df = ticker_full_data[ticker_full_data['Date'] == date]\n","\n","        if len(current_row_df) > 0:\n","            current_idx = current_row_df.index[0]\n","\n","            for day in range(1, 6):\n","                future_rows = ticker_full_data[ticker_full_data.index > current_idx].head(day)\n","                if len(future_rows) >= day:\n","                    future_close = future_rows.iloc[-1]['Close']\n","                    price_change_pct = ((future_close - current_close) / current_close) * 100\n","\n","                    if price_change_pct > 1.0:\n","                        trend = 'UP'\n","                    elif price_change_pct < -1.0:\n","                        trend = 'DOWN'\n","                    else:\n","                        trend = 'STAY'\n","\n","                    targets[f'Day{day}_Trend'] = trend\n","                else:\n","                    targets[f'Day{day}_Trend'] = 'UNKNOWN'\n","\n","        # Combine all features\n","        row_data = {\n","            'Date': date,\n","            'Ticker': ticker,\n","            **lstm_features,\n","            **tech_features,\n","            **market_features,\n","            **targets\n","        }\n","\n","        rf_data.append(row_data)\n","\n","    rf_df = pd.DataFrame(rf_data)\n","\n","    # Remove unknown targets\n","    for day in range(1, 6):\n","        rf_df = rf_df[rf_df[f'Day{day}_Trend'] != 'UNKNOWN']\n","\n","    rf_df = rf_df.fillna(0)\n","\n","    print(f\"   ✅ RF dataset: {len(rf_df)} samples\")\n","\n","    return rf_df\n","\n","def train_simplified_random_forests(rf_df):\n","    \"\"\"Train simplified Random Forest models\"\"\"\n","    print(\"🌲 Training simplified Random Forest models...\")\n","\n","    feature_cols = [col for col in rf_df.columns\n","                   if not col.startswith('Day') and col not in ['Date', 'Ticker']]\n","\n","    X = rf_df[feature_cols].copy().astype(np.float32)\n","\n","    print(f\"   📊 Features: {len(feature_cols)}\")\n","    print(f\"   📊 Samples: {len(X)}\")\n","\n","    trained_models = {}\n","    evaluation_results = {}\n","\n","    for day in range(1, 6):\n","        target_col = f'Day{day}_Trend'\n","        y = rf_df[target_col].copy()\n","\n","        print(f\"\\n   🎯 Training {target_col}...\")\n","\n","        class_dist = y.value_counts()\n","        print(f\"      📊 Classes: {dict(class_dist)}\")\n","\n","        if len(class_dist) < 2:\n","            print(f\"      ⚠️ Skipping {target_col} - insufficient classes\")\n","            continue\n","\n","        try:\n","            X_train, X_test, y_train, y_test = train_test_split(\n","                X, y, test_size=0.2, random_state=42, stratify=y\n","            )\n","\n","            # Simple RF with good defaults\n","            rf_model = RandomForestClassifier(\n","                n_estimators=150,\n","                max_depth=15,\n","                min_samples_split=5,\n","                min_samples_leaf=2,\n","                class_weight='balanced',\n","                random_state=42,\n","                n_jobs=-1\n","            )\n","\n","            rf_model.fit(X_train, y_train)\n","            y_pred = rf_model.predict(X_test)\n","\n","            accuracy = accuracy_score(y_test, y_pred)\n","            f1_macro = f1_score(y_test, y_pred, average='macro')\n","            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n","\n","            print(f\"      📈 Accuracy: {accuracy:.4f}\")\n","            print(f\"      📈 F1-Macro: {f1_macro:.4f}\")\n","            print(f\"      📈 F1-Weighted: {f1_weighted:.4f}\")\n","\n","            trained_models[target_col] = {\n","                'model': rf_model,\n","                'feature_columns': feature_cols\n","            }\n","\n","            evaluation_results[target_col] = {\n","                'accuracy': accuracy,\n","                'f1_macro': f1_macro,\n","                'f1_weighted': f1_weighted,\n","                'y_test': y_test,\n","                'y_pred': y_pred,\n","                'class_distribution': dict(class_dist)\n","            }\n","\n","        except Exception as e:\n","            print(f\"      ❌ Error training {target_col}: {str(e)}\")\n","            continue\n","\n","    print(f\"\\n   ✅ Trained {len(trained_models)} models successfully!\")\n","\n","    return trained_models, evaluation_results\n","\n","def evaluate_and_save_results(trained_models, evaluation_results):\n","    \"\"\"Evaluate and save final results\"\"\"\n","    print(\"📊 Evaluating and saving results...\")\n","\n","    if not trained_models:\n","        print(\"   ❌ No models to evaluate!\")\n","        return None\n","\n","    # Calculate ensemble metrics\n","    accuracies = [evaluation_results[col]['accuracy'] for col in evaluation_results.keys()]\n","    f1_macros = [evaluation_results[col]['f1_macro'] for col in evaluation_results.keys()]\n","    f1_weighteds = [evaluation_results[col]['f1_weighted'] for col in evaluation_results.keys()]\n","\n","    ensemble_metrics = {\n","        'average_accuracy': np.mean(accuracies),\n","        'average_f1_macro': np.mean(f1_macros),\n","        'average_f1_weighted': np.mean(f1_weighteds),\n","        'models_trained': len(trained_models)\n","    }\n","\n","    print(f\"   📈 Ensemble Performance:\")\n","    print(f\"      Average Accuracy: {ensemble_metrics['average_accuracy']:.4f}\")\n","    print(f\"      Average F1-Macro: {ensemble_metrics['average_f1_macro']:.4f}\")\n","    print(f\"      Models trained: {ensemble_metrics['models_trained']}/5\")\n","\n","    # Save models\n","    model_dir = 'random_forest_models'\n","    os.makedirs(model_dir, exist_ok=True)\n","\n","    for target_col, model_data in trained_models.items():\n","        model_path = f\"{model_dir}/{target_col}_rf_model.pkl\"\n","        joblib.dump(model_data, model_path, compress=3)\n","        print(f\"   💾 Saved: {model_path}\")\n","\n","    # Save ensemble info\n","    ensemble_info = {\n","        'ensemble_metrics': ensemble_metrics,\n","        'evaluation_results': evaluation_results,\n","        'training_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'model_type': 'RandomForest_Simplified_Compatible'\n","    }\n","\n","    ensemble_path = f\"{model_dir}/ensemble_info.pkl\"\n","    with open(ensemble_path, 'wb') as f:\n","        pickle.dump(ensemble_info, f)\n","\n","    print(f\"   💾 Ensemble info saved: {ensemble_path}\")\n","\n","    return ensemble_metrics\n","\n","def main_fixed_rf_training():\n","    \"\"\"Main fixed RF training function\"\"\"\n","    print(\"🌲 STARTING STAGE 4: FIXED RANDOM FOREST TRAINING\")\n","    print(\"=\"*80)\n","    print(f\"🔧 Mode: Compatibility Fixed\")\n","    print(f\"⏰ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    print(\"=\"*80)\n","\n","    start_time = datetime.now()\n","\n","    try:\n","        # Step 1: Setup TensorFlow\n","        setup_tensorflow_compatibility()\n","\n","        # Step 2: Load LSTM components with fixes\n","        lstm_components = load_lstm_components_fixed()\n","\n","        # Step 3: Load feature data\n","        combined_df = load_feature_data_optimized()\n","\n","        # Step 4: Generate predictions safely\n","        lstm_predictions, prediction_metadata = generate_lstm_predictions_safe(\n","            combined_df, lstm_components\n","        )\n","\n","        # Step 5: Create RF dataset\n","        rf_df = create_rf_dataset_simplified(\n","            combined_df, lstm_predictions, prediction_metadata\n","        )\n","\n","        # Memory cleanup\n","        del lstm_predictions, prediction_metadata, combined_df\n","        gc.collect()\n","\n","        # Step 6: Train models\n","        trained_models, evaluation_results = train_simplified_random_forests(rf_df)\n","\n","        # Step 7: Evaluate and save\n","        ensemble_metrics = evaluate_and_save_results(trained_models, evaluation_results)\n","\n","        end_time = datetime.now()\n","        training_time = (end_time - start_time).total_seconds() / 60\n","\n","        print(f\"\\n🎉 STAGE 4 COMPLETED SUCCESSFULLY!\")\n","        print(f\"⏱️ Training time: {training_time:.1f} minutes\")\n","        print(f\"🎯 Models trained: {len(trained_models) if trained_models else 0}/5\")\n","\n","        if ensemble_metrics:\n","            print(f\"📊 Average F1-Score: {ensemble_metrics['average_f1_macro']:.4f}\")\n","\n","        print(f\"🚀 LSTM + RANDOM FOREST PIPELINE READY!\")\n","\n","        return {\n","            'status': 'SUCCESS',\n","            'training_time_minutes': training_time,\n","            'models_trained': len(trained_models) if trained_models else 0,\n","            'ensemble_metrics': ensemble_metrics\n","        }\n","\n","    except Exception as e:\n","        end_time = datetime.now()\n","        training_time = (end_time - start_time).total_seconds() / 60\n","\n","        print(f\"\\n❌ STAGE 4 TRAINING FAILED!\")\n","        print(f\"Error: {str(e)}\")\n","        print(f\"⏱️ Time elapsed: {training_time:.1f} minutes\")\n","\n","        import traceback\n","        traceback.print_exc()\n","\n","        return {\n","            'status': 'ERROR',\n","            'error': str(e),\n","            'training_time_minutes': training_time\n","        }\n","\n","# 🚀 RUN FIXED RANDOM FOREST TRAINING\n","if __name__ == \"__main__\":\n","    result = main_fixed_rf_training()\n","\n","print(\"\\n🎯 STAGE 4 EXECUTION COMPLETED!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RQr9ujKQsgg","executionInfo":{"status":"ok","timestamp":1753425329311,"user_tz":-420,"elapsed":83515,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"df5d402e-7d9f-4498-85b7-0d4f4cd9e735"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["🌲 STARTING STAGE 4: FIXED RANDOM FOREST TRAINING\n","================================================================================\n","🔧 Mode: Compatibility Fixed\n","⏰ Start time: 2025-07-25 06:34:06\n","================================================================================\n","🔧 Setting up TensorFlow compatibility...\n","   ✅ GPU memory growth enabled\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["📦 Loading LSTM components with compatibility fixes...\n","🧠 Loading LSTM model with compatibility fixes...\n","   🔄 Loading model with custom objects...\n","   ✅ LSTM model loaded successfully with custom objects\n","   ✅ Feature scaler loaded\n","   ✅ Target scaler loaded\n","   ✅ Ticker encoder loaded\n","   ✅ Model info loaded\n","   📊 Feature columns: 21\n","   📊 Target columns: 10\n","📂 Loading feature engineered data...\n","   ✅ NFLX: 2000 records\n","   ✅ TSLA: 2000 records\n","   ✅ AAPL: 2000 records\n","   ✅ GOOGL: 2000 records\n","   ✅ META: 2000 records\n","   ✅ BBCA: 2000 records\n","   ✅ NVDA: 2000 records\n","   ✅ MSFT: 2000 records\n","   ✅ BAC: 2000 records\n","   ✅ JPM: 2000 records\n","   ✅ AMZN: 2000 records\n","✅ Combined data: 22000 records\n","🎯 Generating LSTM predictions safely...\n","   Processing AAPL...\n","      ✅ 500 predictions generated\n","   Processing AMZN...\n","      ✅ 500 predictions generated\n","   Processing BAC...\n","      ✅ 500 predictions generated\n","   Processing BBCA...\n","      ✅ 500 predictions generated\n","   Processing GOOGL...\n","      ✅ 500 predictions generated\n","   Processing JPM...\n","      ✅ 500 predictions generated\n","   Processing META...\n","      ✅ 500 predictions generated\n","   Processing MSFT...\n","      ✅ 500 predictions generated\n","   ✅ Total predictions: 4000\n","🔧 Creating simplified RF dataset...\n","   ✅ RF dataset: 4000 samples\n","🌲 Training simplified Random Forest models...\n","   📊 Features: 25\n","   📊 Samples: 4000\n","\n","   🎯 Training Day1_Trend...\n","      📊 Classes: {'STAY': np.int64(2181), 'UP': np.int64(998), 'DOWN': np.int64(821)}\n","      📈 Accuracy: 0.4925\n","      📈 F1-Macro: 0.3772\n","      📈 F1-Weighted: 0.4634\n","\n","   🎯 Training Day2_Trend...\n","      📊 Classes: {'STAY': np.int64(1529), 'UP': np.int64(1421), 'DOWN': np.int64(1050)}\n","      📈 Accuracy: 0.4500\n","      📈 F1-Macro: 0.4437\n","      📈 F1-Weighted: 0.4488\n","\n","   🎯 Training Day3_Trend...\n","      📊 Classes: {'UP': np.int64(1571), 'STAY': np.int64(1271), 'DOWN': np.int64(1158)}\n","      📈 Accuracy: 0.5312\n","      📈 F1-Macro: 0.5232\n","      📈 F1-Weighted: 0.5290\n","\n","   🎯 Training Day4_Trend...\n","      📊 Classes: {'UP': np.int64(1715), 'DOWN': np.int64(1221), 'STAY': np.int64(1064)}\n","      📈 Accuracy: 0.5800\n","      📈 F1-Macro: 0.5561\n","      📈 F1-Weighted: 0.5754\n","\n","   🎯 Training Day5_Trend...\n","      📊 Classes: {'UP': np.int64(1786), 'DOWN': np.int64(1259), 'STAY': np.int64(955)}\n","      📈 Accuracy: 0.6175\n","      📈 F1-Macro: 0.5733\n","      📈 F1-Weighted: 0.6081\n","\n","   ✅ Trained 5 models successfully!\n","📊 Evaluating and saving results...\n","   📈 Ensemble Performance:\n","      Average Accuracy: 0.5343\n","      Average F1-Macro: 0.4947\n","      Models trained: 5/5\n","   💾 Saved: random_forest_models/Day1_Trend_rf_model.pkl\n","   💾 Saved: random_forest_models/Day2_Trend_rf_model.pkl\n","   💾 Saved: random_forest_models/Day3_Trend_rf_model.pkl\n","   💾 Saved: random_forest_models/Day4_Trend_rf_model.pkl\n","   💾 Saved: random_forest_models/Day5_Trend_rf_model.pkl\n","   💾 Ensemble info saved: random_forest_models/ensemble_info.pkl\n","\n","🎉 STAGE 4 COMPLETED SUCCESSFULLY!\n","⏱️ Training time: 1.4 minutes\n","🎯 Models trained: 5/5\n","📊 Average F1-Score: 0.4947\n","🚀 LSTM + RANDOM FOREST PIPELINE READY!\n","\n","🎯 STAGE 4 EXECUTION COMPLETED!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yB5ovEVoZLGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Versi Skicit-learn"],"metadata":{"id":"LbsZrMz1ZWBq"}},{"cell_type":"code","source":["# ===================================================================\n","# 🚀 FIXED LAYOUT STOCK PREDICTOR - wasirawasenju\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import joblib\n","from datetime import datetime, timedelta\n","import yfinance as yf\n","import streamlit as st\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","\n","warnings.filterwarnings('ignore')\n","\n","class SimpleStockPredictor:\n","    \"\"\"Simple Stock Predictor using only RF models + Technical Analysis\"\"\"\n","\n","    def __init__(self):\n","        self.rf_models = None\n","        self.is_loaded = False\n","\n","    def load_rf_models_only(self):\n","        \"\"\"Load only Random Forest models (skip LSTM)\"\"\"\n","        print(\"🔄 Loading Random Forest models...\")\n","\n","        try:\n","            rf_dir = 'random_forest_models'\n","\n","            if not os.path.exists(rf_dir):\n","                return False, f\"RF models directory not found: {rf_dir}\"\n","\n","            rf_models = {}\n","            models_loaded = 0\n","\n","            # Load available RF models\n","            for day in range(1, 6):\n","                model_path = f\"{rf_dir}/Day{day}_Trend_rf_model.pkl\"\n","\n","                if os.path.exists(model_path):\n","                    try:\n","                        rf_models[f'Day{day}'] = joblib.load(model_path)\n","                        models_loaded += 1\n","                        print(f\"   ✅ Day{day} RF model loaded\")\n","                    except Exception as e:\n","                        print(f\"   ⚠️ Day{day} model error: {str(e)}\")\n","                else:\n","                    print(f\"   ❌ Day{day} model not found\")\n","\n","            if models_loaded == 0:\n","                return False, \"No RF models could be loaded\"\n","\n","            self.rf_models = {\n","                'models': rf_models,\n","                'models_loaded': models_loaded\n","            }\n","\n","            self.is_loaded = True\n","            print(f\"✅ Loaded {models_loaded}/5 RF models successfully!\")\n","\n","            return True, f\"Loaded {models_loaded}/5 RF models\"\n","\n","        except Exception as e:\n","            return False, f\"Error loading RF models: {str(e)}\"\n","\n","    def download_stock_data(self, ticker, period=\"1y\"):\n","        \"\"\"Download stock data from yfinance\"\"\"\n","        print(f\"📥 Downloading {ticker}...\")\n","\n","        try:\n","            stock = yf.Ticker(ticker)\n","            df = stock.history(period=period, auto_adjust=True)\n","\n","            if df.empty:\n","                raise ValueError(f\"No data for {ticker}\")\n","\n","            df = df.reset_index()\n","            df['Date'] = pd.to_datetime(df['Date'])\n","\n","            # Remove timezone if present\n","            if hasattr(df['Date'].dtype, 'tz') and df['Date'].dt.tz is not None:\n","                df['Date'] = df['Date'].dt.tz_localize(None)\n","\n","            print(f\"✅ Got {len(df)} records for {ticker}\")\n","            return df\n","\n","        except Exception as e:\n","            raise Exception(f\"Download failed: {str(e)}\")\n","\n","    def calculate_indicators(self, df):\n","        \"\"\"Calculate essential technical indicators\"\"\"\n","        print(\"🔧 Calculating indicators...\")\n","\n","        # Moving averages\n","        df['SMA_10'] = df['Close'].rolling(10).mean()\n","        df['SMA_20'] = df['Close'].rolling(20).mean()\n","        df['EMA_10'] = df['Close'].ewm(span=10).mean()\n","        df['EMA_20'] = df['Close'].ewm(span=20).mean()\n","\n","        # RSI\n","        delta = df['Close'].diff()\n","        gain = delta.where(delta > 0, 0).rolling(14).mean()\n","        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n","        rs = gain / loss\n","        df['RSI_14'] = 100 - (100 / (1 + rs))\n","\n","        # MACD\n","        ema12 = df['Close'].ewm(span=12).mean()\n","        ema26 = df['Close'].ewm(span=26).mean()\n","        df['MACD'] = ema12 - ema26\n","        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n","\n","        # Bollinger Bands\n","        bb_mid = df['Close'].rolling(20).mean()\n","        bb_std = df['Close'].rolling(20).std()\n","        df['BB_High'] = bb_mid + (bb_std * 2)\n","        df['BB_Low'] = bb_mid - (bb_std * 2)\n","        df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","        df['BB_Width'] = (df['BB_High'] - df['BB_Low']) / bb_mid\n","\n","        # Volume & momentum\n","        df['Volume_MA_10'] = df['Volume'].rolling(10).mean()\n","        df['Volatility_20'] = df['Close'].rolling(20).std()\n","        df['Daily_Return'] = df['Close'].pct_change() * 100\n","        df['ROC_10'] = df['Close'].pct_change(10) * 100\n","\n","        # Binary features\n","        df['High_Volume'] = (df['Volume'] > df['Volume_MA_10'] * 1.5).astype(int)\n","        df['Low_Volume'] = (df['Volume'] < df['Volume_MA_10'] * 0.5).astype(int)\n","        df['Strong_Momentum'] = (df['ROC_10'] > 5).astype(int)\n","        df['Weak_Momentum'] = (df['ROC_10'] < -5).astype(int)\n","\n","        # OBV (simplified)\n","        df['OBV'] = (df['Volume'] * np.sign(df['Close'].diff())).cumsum()\n","\n","        # ATR (simplified)\n","        df['ATR'] = ((df['High'] - df['Low']).rolling(14).mean())\n","\n","        # Fill NaN\n","        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n","\n","        print(\"✅ Indicators calculated\")\n","        return df\n","\n","    def predict_prices_technical(self, df):\n","        \"\"\"Predict prices using technical analysis (no LSTM)\"\"\"\n","        print(\"🎯 Predicting prices with technical analysis...\")\n","\n","        current_price = df['Close'].iloc[-1]\n","\n","        # Calculate trends\n","        sma_trend = df['SMA_10'].iloc[-1] - df['SMA_10'].iloc[-5]\n","        ema_trend = df['EMA_10'].iloc[-1] - df['EMA_10'].iloc[-5]\n","        momentum = df['ROC_10'].iloc[-1]\n","        rsi = df['RSI_14'].iloc[-1]\n","\n","        # Trend strength\n","        trend_strength = (sma_trend + ema_trend) / 2\n","\n","        # RSI bias (oversold/overbought)\n","        if rsi > 70:\n","            rsi_bias = -0.5  # Overbought, expect decline\n","        elif rsi < 30:\n","            rsi_bias = 0.5   # Oversold, expect rise\n","        else:\n","            rsi_bias = 0\n","\n","        # Generate 5-day predictions\n","        predictions = {}\n","        base_price = current_price\n","\n","        for day in range(1, 6):\n","            # Decay trend over time\n","            decay = 0.85 ** (day - 1)\n","\n","            # Price change calculation\n","            daily_change = (trend_strength * decay) + (momentum * 0.01 * decay) + (rsi_bias * decay)\n","\n","            # Limit max change to 5% per day\n","            max_change = base_price * 0.05\n","            daily_change = max(min(daily_change, max_change), -max_change)\n","\n","            # Calculate new prices\n","            predicted_close = base_price + daily_change\n","\n","            # Simple open prediction (slight variation from previous close)\n","            if day == 1:\n","                predicted_open = current_price * (1 + np.random.normal(0, 0.002))  # Small random gap\n","            else:\n","                predicted_open = predictions[f'Day{day-1}']['Close'] * (1 + np.random.normal(0, 0.001))\n","\n","            predictions[f'Day{day}'] = {\n","                'Open': max(predicted_open, 0.01),  # Ensure positive\n","                'Close': max(predicted_close, 0.01)\n","            }\n","\n","            base_price = predicted_close\n","\n","        return predictions\n","\n","    def predict_trends_with_rf(self, df, price_predictions):\n","        \"\"\"Predict trends using loaded RF models\"\"\"\n","        if not self.is_loaded or not self.rf_models:\n","            print(\"⚠️ No RF models available, using simple trend logic\")\n","            return self._simple_trend_prediction(price_predictions)\n","\n","        print(\"🌲 Predicting trends with RF models...\")\n","\n","        latest_data = df.iloc[-1]\n","\n","        # Prepare features for RF\n","        rf_features = {}\n","\n","        # Mock LSTM features (use our technical predictions)\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            rf_features[f'LSTM_{day_key}_Open'] = price_predictions[day_key]['Open']\n","            rf_features[f'LSTM_{day_key}_Close'] = price_predictions[day_key]['Close']\n","\n","        # Technical features\n","        tech_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Volatility_20', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'OBV',\n","            'High_Volume', 'Low_Volume', 'Strong_Momentum', 'Weak_Momentum'\n","        ]\n","\n","        for indicator in tech_indicators:\n","            value = latest_data.get(indicator, 0)\n","            rf_features[f'Tech_{indicator}'] = float(value) if pd.notna(value) else 0.0\n","\n","        # Market state features\n","        rf_features.update({\n","            'Current_Close': float(latest_data['Close']),\n","            'Current_Volume': float(latest_data['Volume']),\n","            'Current_High': float(latest_data['High']),\n","            'Current_Low': float(latest_data['Low']),\n","            'Price_Range': float(latest_data['High'] - latest_data['Low']),\n","            'Volume_Ratio': float(latest_data['Volume'] / latest_data.get('Volume_MA_10', latest_data['Volume']))\n","        })\n","\n","        # Predict with RF models\n","        trends = {}\n","        confidences = {}\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","\n","            if day_key in self.rf_models['models']:\n","                try:\n","                    model_data = self.rf_models['models'][day_key]\n","                    rf_model = model_data['model']\n","                    model_features = model_data['feature_columns']\n","\n","                    # Align features\n","                    aligned_features = []\n","                    for feature in model_features:\n","                        aligned_features.append(rf_features.get(feature, 0.0))\n","\n","                    # Predict\n","                    X_pred = np.array([aligned_features])\n","                    trend_pred = rf_model.predict(X_pred)[0]\n","                    trend_proba = rf_model.predict_proba(X_pred)[0]\n","\n","                    trends[day_key] = trend_pred\n","                    confidences[day_key] = float(max(trend_proba))\n","\n","                    print(f\"   ✅ {day_key}: {trend_pred} (confidence: {max(trend_proba):.2f})\")\n","\n","                except Exception as e:\n","                    print(f\"   ⚠️ {day_key} RF failed: {str(e)}\")\n","                    trends[day_key] = 'STAY'\n","                    confidences[day_key] = 0.5\n","            else:\n","                print(f\"   ❌ {day_key} model not available\")\n","                trends[day_key] = 'STAY'\n","                confidences[day_key] = 0.3\n","\n","        return trends, confidences\n","\n","    def _simple_trend_prediction(self, price_predictions):\n","        \"\"\"Fallback simple trend prediction\"\"\"\n","        trends = {}\n","        confidences = {}\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            pred_close = price_predictions[day_key]['Close']\n","            pred_open = price_predictions[day_key]['Open']\n","\n","            change_pct = ((pred_close - pred_open) / pred_open) * 100\n","\n","            if change_pct > 1.0:\n","                trends[day_key] = 'UP'\n","            elif change_pct < -1.0:\n","                trends[day_key] = 'DOWN'\n","            else:\n","                trends[day_key] = 'STAY'\n","\n","            confidences[day_key] = 0.6  # Default confidence\n","\n","        return trends, confidences\n","\n","    def predict_stock(self, ticker, period=\"1y\"):\n","        \"\"\"Complete prediction pipeline\"\"\"\n","        print(f\"🎯 Predicting {ticker}...\")\n","\n","        try:\n","            # Download data\n","            df = self.download_stock_data(ticker, period)\n","\n","            # Calculate indicators\n","            df = self.calculate_indicators(df)\n","\n","            # Predict prices\n","            price_predictions = self.predict_prices_technical(df)\n","\n","            # Predict trends\n","            trends, confidences = self.predict_trends_with_rf(df, price_predictions)\n","\n","            # Format results\n","            today = datetime.now().date()\n","            prediction_dates = [(today + timedelta(days=i)) for i in range(1, 6)]\n","\n","            predictions = {}\n","            for i, day_key in enumerate(['Day1', 'Day2', 'Day3', 'Day4', 'Day5']):\n","                predictions[day_key] = {\n","                    'date': prediction_dates[i].strftime('%Y-%m-%d'),\n","                    'predicted_open': price_predictions[day_key]['Open'],\n","                    'predicted_close': price_predictions[day_key]['Close'],\n","                    'trend': trends[day_key],\n","                    'confidence': confidences[day_key]\n","                }\n","\n","            current_data = df.iloc[-1]\n","            stock_info = {\n","                'ticker': ticker.upper(),\n","                'current_price': float(current_data['Close']),\n","                'current_date': current_data['Date'].strftime('%Y-%m-%d'),\n","                'volume': float(current_data['Volume']),\n","                'prediction_generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),\n","                'data_points_used': len(df),\n","                'rf_models_available': self.rf_models['models_loaded'] if self.rf_models else 0\n","            }\n","\n","            return {\n","                'status': 'SUCCESS',\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'historical_data': df.tail(30).to_dict('records')\n","            }\n","\n","        except Exception as e:\n","            return {\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'ticker': ticker\n","            }\n","\n","# ===================================================================\n","# 🖥️ STREAMLIT DASHBOARD - FIXED LAYOUT\n","# ===================================================================\n","\n","def main_dashboard():\n","    \"\"\"Fixed layout dashboard\"\"\"\n","\n","    st.set_page_config(\n","        page_title=\"📈 Stock Predictor - wasirawasenju\",\n","        page_icon=\"🚀\",\n","        layout=\"wide\"\n","    )\n","\n","    # Custom CSS\n","    st.markdown(\"\"\"\n","    <style>\n","    .main-title {\n","        font-size: 2.5rem;\n","        color: #1f77b4;\n","        text-align: center;\n","        margin-bottom: 1rem;\n","    }\n","    .subtitle {\n","        font-size: 1.2rem;\n","        text-align: center;\n","        color: #666;\n","        margin-bottom: 2rem;\n","    }\n","    .user-info {\n","        background: linear-gradient(90deg, #f0f8ff, #e6f3ff);\n","        padding: 1rem;\n","        border-radius: 10px;\n","        border-left: 4px solid #1f77b4;\n","        margin-bottom: 1rem;\n","    }\n","    .prediction-card {\n","        background: #f8f9fa;\n","        padding: 1.5rem;\n","        border-radius: 10px;\n","        border: 1px solid #dee2e6;\n","        margin: 1rem 0;\n","    }\n","    .metric-card {\n","        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n","        color: white;\n","        padding: 1rem;\n","        border-radius: 10px;\n","        text-align: center;\n","        margin: 0.5rem 0;\n","    }\n","    </style>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Initialize session state\n","    if 'predictor' not in st.session_state:\n","        st.session_state.predictor = SimpleStockPredictor()\n","        st.session_state.models_loaded = False\n","        st.session_state.prediction_result = None\n","        st.session_state.current_ticker = \"\"\n","\n","    # Header\n","    st.markdown('<h1 class=\"main-title\">🚀 AI Stock Predictor</h1>', unsafe_allow_html=True)\n","    st.markdown('<p class=\"subtitle\">Random Forest + Technical Analysis | No TensorFlow Required</p>', unsafe_allow_html=True)\n","\n","    # User info\n","    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')\n","    st.markdown(f\"\"\"\n","    <div class=\"user-info\">\n","        <h4>👤 User: wasirawasenju</h4>\n","        <p>📅 Current Time: {current_time}</p>\n","        <p>🏠 Environment: Local Development</p>\n","    </div>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Create two columns: sidebar-like left column and main content\n","    col_sidebar, col_main = st.columns([1, 3])\n","\n","    # LEFT COLUMN (Control Panel)\n","    with col_sidebar:\n","        st.markdown(\"### 🎛️ Control Panel\")\n","\n","        # Model loading section\n","        st.markdown(\"#### 🤖 AI Models\")\n","\n","        if not st.session_state.models_loaded:\n","            if st.button(\"🔄 Load RF Models\", type=\"primary\", use_container_width=True):\n","                with st.spinner(\"Loading models...\"):\n","                    success, message = st.session_state.predictor.load_rf_models_only()\n","\n","                    if success:\n","                        st.session_state.models_loaded = True\n","                        st.success(f\"✅ {message}\")\n","                    else:\n","                        st.error(f\"❌ {message}\")\n","                        st.warning(\"Will use fallback prediction\")\n","                        st.session_state.models_loaded = True  # Allow to continue\n","        else:\n","            st.success(\"✅ Models Ready\")\n","            if st.session_state.predictor.rf_models:\n","                rf_count = st.session_state.predictor.rf_models['models_loaded']\n","                st.info(f\"RF Models: {rf_count}/5\")\n","\n","        # Stock input section\n","        if st.session_state.models_loaded:\n","            st.markdown(\"#### 📊 Stock Selection\")\n","\n","            # Text input\n","            ticker_input = st.text_input(\n","                \"Enter Ticker:\",\n","                value=st.session_state.current_ticker,\n","                placeholder=\"AAPL, GOOGL, BBCA.JK...\",\n","                key=\"ticker_input\"\n","            )\n","\n","            # Quick select buttons\n","            st.markdown(\"**Quick Select:**\")\n","\n","            # Create 2x3 grid for buttons\n","            col1, col2 = st.columns(2)\n","\n","            with col1:\n","                if st.button(\"🍎 AAPL\", use_container_width=True):\n","                    st.session_state.current_ticker = \"AAPL\"\n","                    st.rerun()\n","\n","                if st.button(\"🔍 GOOGL\", use_container_width=True):\n","                    st.session_state.current_ticker = \"GOOGL\"\n","                    st.rerun()\n","\n","                if st.button(\"🏦 BBCA.JK\", use_container_width=True):\n","                    st.session_state.current_ticker = \"BBCA.JK\"\n","                    st.rerun()\n","\n","            with col2:\n","                if st.button(\"🚗 TSLA\", use_container_width=True):\n","                    st.session_state.current_ticker = \"TSLA\"\n","                    st.rerun()\n","\n","                if st.button(\"💻 MSFT\", use_container_width=True):\n","                    st.session_state.current_ticker = \"MSFT\"\n","                    st.rerun()\n","\n","                if st.button(\"🎬 NFLX\", use_container_width=True):\n","                    st.session_state.current_ticker = \"NFLX\"\n","                    st.rerun()\n","\n","            # Update current ticker from input\n","            if ticker_input != st.session_state.current_ticker:\n","                st.session_state.current_ticker = ticker_input\n","\n","            # Predict button\n","            st.markdown(\"---\")\n","            if st.button(\"🎯 Generate Prediction\", type=\"primary\", use_container_width=True):\n","                if st.session_state.current_ticker:\n","                    # Show prediction in main area\n","                    with col_main:\n","                        with st.spinner(f\"🔍 Analyzing {st.session_state.current_ticker.upper()}...\"):\n","                            result = st.session_state.predictor.predict_stock(st.session_state.current_ticker)\n","\n","                        # Store result in session state\n","                        st.session_state.prediction_result = result\n","                        st.rerun()\n","                else:\n","                    st.warning(\"Please enter a ticker!\")\n","\n","    # RIGHT COLUMN (Main Content)\n","    with col_main:\n","        if st.session_state.prediction_result is None:\n","            # Default welcome content\n","            st.markdown(\"### 🎯 Welcome to AI Stock Predictor\")\n","\n","            st.markdown(\"\"\"\n","            <div class=\"prediction-card\">\n","                <h4>🚀 Get Started</h4>\n","                <p>1. Load the AI models using the control panel</p>\n","                <p>2. Enter a stock ticker or use quick select buttons</p>\n","                <p>3. Click \"Generate Prediction\" to see 5-day forecasts</p>\n","                <p>4. View detailed analysis, charts, and investment insights</p>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            st.markdown(\"\"\"\n","            <div class=\"prediction-card\">\n","                <h4>📊 Features</h4>\n","                <ul>\n","                <li>🎯 <strong>5-Day Price Predictions</strong> - Open & Close forecasts</li>\n","                <li>📈 <strong>Trend Analysis</strong> - UP/DOWN/STAY classifications</li>\n","                <li>🤖 <strong>AI-Powered</strong> - Random Forest + Technical Analysis</li>\n","                <li>📱 <strong>Real-time Data</strong> - Live data from Yahoo Finance</li>\n","                <li>💡 <strong>Investment Insights</strong> - Automated recommendations</li>\n","                </ul>\n","            </div>\n","            \"\"\", unsafe_allow_html=True)\n","\n","            # Sample prediction preview\n","            st.markdown(\"### 📋 Sample Prediction Output\")\n","\n","            sample_data = {\n","                \"Date\": [\"2025-07-26\", \"2025-07-27\", \"2025-07-28\", \"2025-07-29\", \"2025-07-30\"],\n","                \"Open\": [\"$150.20\", \"$151.50\", \"$152.10\", \"$151.80\", \"$153.00\"],\n","                \"Close\": [\"$151.30\", \"$152.80\", \"$151.90\", \"$153.20\", \"$154.10\"],\n","                \"Trend\": [\"📈 UP\", \"📈 UP\", \"📉 DOWN\", \"📈 UP\", \"📈 UP\"],\n","                \"Confidence\": [\"85.2%\", \"78.9%\", \"72.1%\", \"81.5%\", \"77.3%\"]\n","            }\n","\n","            sample_df = pd.DataFrame(sample_data)\n","            st.dataframe(sample_df, use_container_width=True)\n","\n","        else:\n","            # Display prediction results\n","            if st.session_state.prediction_result['status'] == 'SUCCESS':\n","                display_prediction_results(st.session_state.prediction_result)\n","            else:\n","                st.error(f\"❌ Prediction Error: {st.session_state.prediction_result['error']}\")\n","\n","                # Clear error after showing\n","                if st.button(\"🔄 Clear Error\"):\n","                    st.session_state.prediction_result = None\n","                    st.rerun()\n","\n","def display_prediction_results(result):\n","    \"\"\"Display prediction results in main area\"\"\"\n","\n","    stock_info = result['stock_info']\n","    predictions = result['predictions']\n","    historical_data = result['historical_data']\n","\n","    # Header with stock info\n","    st.markdown(f\"### 📊 {stock_info['ticker']} - Prediction Results\")\n","\n","    # Metrics row\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>💰 Current Price</h4>\n","            <h2>${stock_info['current_price']:.2f}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>📊 Volume</h4>\n","            <h2>{stock_info['volume']:,.0f}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col3:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>📅 Data Date</h4>\n","            <h2>{stock_info['current_date']}</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col4:\n","        st.markdown(f\"\"\"\n","        <div class=\"metric-card\">\n","            <h4>🤖 RF Models</h4>\n","            <h2>{stock_info['rf_models_available']}/5</h2>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # 5-Day Predictions Table\n","    st.markdown(\"### 🔮 5-Day AI Predictions\")\n","\n","    pred_data = []\n","    for day, pred in predictions.items():\n","        trend_emoji = {\"UP\": \"📈\", \"DOWN\": \"📉\", \"STAY\": \"➡️\"}.get(pred['trend'], \"❓\")\n","\n","        change = pred['predicted_close'] - pred['predicted_open']\n","        change_pct = (change / pred['predicted_open']) * 100\n","\n","        pred_data.append({\n","            \"📅 Date\": pred['date'],\n","            \"🌅 Open\": f\"${pred['predicted_open']:.2f}\",\n","            \"🌇 Close\": f\"${pred['predicted_close']:.2f}\",\n","            \"📊 Daily Change\": f\"${change:+.2f} ({change_pct:+.1f}%)\",\n","            \"📈 Trend\": f\"{trend_emoji} {pred['trend']}\",\n","            \"🎯 Confidence\": f\"{pred['confidence']:.1%}\"\n","        })\n","\n","    pred_df = pd.DataFrame(pred_data)\n","    st.dataframe(pred_df, use_container_width=True)\n","\n","    # Price Chart\n","    st.markdown(\"### 📈 Interactive Price Chart\")\n","\n","    # Prepare chart data\n","    hist_df = pd.DataFrame(historical_data)\n","    hist_df['Date'] = pd.to_datetime(hist_df['Date'])\n","\n","    # Create chart\n","    fig = make_subplots(\n","        rows=2, cols=1,\n","        shared_xaxes=True,\n","        vertical_spacing=0.1,\n","        subplot_titles=(f'{stock_info[\"ticker\"]} - Price Prediction', 'Trading Volume'),\n","        row_heights=[0.7, 0.3]\n","    )\n","\n","    # Historical candlestick\n","    fig.add_trace(\n","        go.Candlestick(\n","            x=hist_df['Date'],\n","            open=hist_df['Open'],\n","            high=hist_df['High'],\n","            low=hist_df['Low'],\n","            close=hist_df['Close'],\n","            name='Historical Price',\n","            increasing_line_color='green',\n","            decreasing_line_color='red'\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Predicted prices\n","    pred_dates = [datetime.strptime(pred['date'], '%Y-%m-%d') for pred in predictions.values()]\n","    pred_closes = [pred['predicted_close'] for pred in predictions.values()]\n","\n","    # Connect last historical to first prediction\n","    last_date = hist_df['Date'].iloc[-1]\n","    last_price = hist_df['Close'].iloc[-1]\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=[last_date] + pred_dates,\n","            y=[last_price] + pred_closes,\n","            mode='lines+markers',\n","            name='Predicted Close',\n","            line=dict(color='red', width=4, dash='dash'),\n","            marker=dict(size=10, color='red', symbol='diamond')\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Volume bars\n","    fig.add_trace(\n","        go.Bar(\n","            x=hist_df['Date'],\n","            y=hist_df['Volume'],\n","            name='Volume',\n","            marker_color='lightblue',\n","            opacity=0.7\n","        ),\n","        row=2, col=1\n","    )\n","\n","    fig.update_layout(\n","        title=f\"{stock_info['ticker']} - AI Prediction Analysis\",\n","        height=700,\n","        showlegend=True,\n","        hovermode='x unified'\n","    )\n","\n","    fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n","    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)\n","    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n","\n","    st.plotly_chart(fig, use_container_width=True)\n","\n","    # Investment Analysis\n","    st.markdown(\"### 💡 Investment Analysis\")\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        # Trend analysis\n","        up_days = sum(1 for pred in predictions.values() if pred['trend'] == 'UP')\n","        down_days = sum(1 for pred in predictions.values() if pred['trend'] == 'DOWN')\n","        stay_days = sum(1 for pred in predictions.values() if pred['trend'] == 'STAY')\n","        avg_confidence = sum(pred['confidence'] for pred in predictions.values()) / len(predictions)\n","\n","        if up_days > down_days:\n","            recommendation = \"🟢 BULLISH OUTLOOK\"\n","            rec_color = \"#28a745\"\n","            advice = f\"Model predicts {up_days} bullish days. Consider buying opportunities.\"\n","        elif down_days > up_days:\n","            recommendation = \"🔴 BEARISH OUTLOOK\"\n","            rec_color = \"#dc3545\"\n","            advice = f\"Model predicts {down_days} bearish days. Exercise caution.\"\n","        else:\n","            recommendation = \"🟡 NEUTRAL OUTLOOK\"\n","            rec_color = \"#ffc107\"\n","            advice = \"Mixed signals detected. Wait for clearer trends.\"\n","\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","            <h4 style=\"color: {rec_color};\">{recommendation}</h4>\n","            <p>{advice}</p>\n","            <p><strong>Average Confidence:</strong> {avg_confidence:.1%}</p>\n","            <p><strong>Trend Distribution:</strong> {up_days} UP, {down_days} DOWN, {stay_days} STAY</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        # Price range analysis\n","        current_price = stock_info['current_price']\n","        min_pred = min(pred['predicted_close'] for pred in predictions.values())\n","        max_pred = max(pred['predicted_close'] for pred in predictions.values())\n","\n","        price_range = max_pred - min_pred\n","        upside = ((max_pred - current_price) / current_price) * 100\n","        downside = ((current_price - min_pred) / current_price) * 100\n","\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","            <h4>💹 Price Range Analysis</h4>\n","            <p><strong>Current:</strong> ${current_price:.2f}</p>\n","            <p><strong>Predicted Range:</strong> ${min_pred:.2f} - ${max_pred:.2f}</p>\n","            <p><strong>Volatility:</strong> ${price_range:.2f} ({(price_range/current_price)*100:.1f}%)</p>\n","            <p><strong>Upside Potential:</strong> +{upside:.1f}%</p>\n","            <p><strong>Downside Risk:</strong> -{downside:.1f}%</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Action buttons\n","    col1, col2, col3 = st.columns(3)\n","\n","    with col1:\n","        if st.button(\"📥 Export JSON\", use_container_width=True):\n","            import json\n","            export_data = {\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'recommendation': recommendation\n","            }\n","\n","            st.download_button(\n","                label=\"Download Prediction Report\",\n","                data=json.dumps(export_data, indent=2),\n","                file_name=f\"{stock_info['ticker']}_prediction_{datetime.now().strftime('%Y%m%d_%H%M')}.json\",\n","                mime=\"application/json\"\n","            )\n","\n","    with col2:\n","        if st.button(\"🔄 New Prediction\", use_container_width=True):\n","            st.session_state.prediction_result = None\n","            st.session_state.current_ticker = \"\"\n","            st.rerun()\n","\n","    with col3:\n","        if st.button(\"📊 Analyze Another\", use_container_width=True):\n","            st.session_state.prediction_result = None\n","            st.rerun()\n","\n","    # Disclaimer\n","    st.markdown(\"---\")\n","    st.warning(\"\"\"\n","    ⚠️ **Disclaimer:** This AI prediction is for educational purposes only.\n","    Always conduct your own research and consult financial advisors before making investment decisions.\n","    Past performance does not guarantee future results.\n","    \"\"\")\n","\n","if __name__ == \"__main__\":\n","    main_dashboard()"],"metadata":{"id":"jrOVIYZVZZ6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 5 : Dashboard dan real time prediction"],"metadata":{"id":"wOGLzqt4UdL9"}},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"zm_5eoFFc-hS"}},{"cell_type":"code","source":["# Install dependencies first\n","pip install streamlit plotly yfinance\n","\n","# Run Streamlit dashboard\n","streamlit run app.py"],"metadata":{"id":"LwVOVgThVwkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================================================================\n","# 🚀 STAGE 5: REAL-TIME PREDICTION API & DASHBOARD\n","# ===================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","import pickle\n","import joblib\n","from datetime import datetime, timedelta\n","import yfinance as yf\n","import streamlit as st\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","\n","# TensorFlow for LSTM predictions\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","class StockPredictionSystem:\n","    \"\"\"Complete Stock Prediction System with LSTM + Random Forest\"\"\"\n","\n","    def __init__(self):\n","        self.lstm_components = None\n","        self.rf_models = None\n","        self.is_loaded = False\n","\n","    def load_models(self):\n","        \"\"\"Load all trained models and components\"\"\"\n","        print(\"🔄 Loading trained models...\")\n","\n","        try:\n","            # Load LSTM components\n","            self.lstm_components = self._load_lstm_components()\n","\n","            # Load Random Forest models\n","            self.rf_models = self._load_rf_models()\n","\n","            self.is_loaded = True\n","            print(\"✅ All models loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error loading models: {str(e)}\")\n","            raise e\n","\n","    def _load_lstm_components(self):\n","        \"\"\"Load LSTM model and components\"\"\"\n","        model_dir = 'unified_lstm_model'\n","\n","        # Custom objects for compatibility\n","        custom_objects = {\n","            'mse': 'mean_squared_error',\n","            'mae': 'mean_absolute_error'\n","        }\n","\n","        # Load LSTM model\n","        lstm_model = load_model(\n","            f'{model_dir}/unified_lstm_model.h5',\n","            custom_objects=custom_objects,\n","            compile=False\n","        )\n","\n","        # Recompile\n","        from tensorflow.keras.optimizers import Adam\n","        lstm_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n","\n","        # Load scalers and encoders\n","        with open(f'{model_dir}/feature_scaler.pkl', 'rb') as f:\n","            feature_scaler = pickle.load(f)\n","\n","        with open(f'{model_dir}/target_scaler.pkl', 'rb') as f:\n","            target_scaler = pickle.load(f)\n","\n","        with open(f'{model_dir}/ticker_encoder.pkl', 'rb') as f:\n","            ticker_encoder = pickle.load(f)\n","\n","        with open(f'{model_dir}/model_info.pkl', 'rb') as f:\n","            model_info = pickle.load(f)\n","\n","        return {\n","            'model': lstm_model,\n","            'feature_scaler': feature_scaler,\n","            'target_scaler': target_scaler,\n","            'ticker_encoder': ticker_encoder,\n","            'model_info': model_info\n","        }\n","\n","    def _load_rf_models(self):\n","        \"\"\"Load Random Forest models\"\"\"\n","        model_dir = 'random_forest_models'\n","\n","        rf_models = {}\n","        for day in range(1, 6):\n","            model_path = f\"{model_dir}/Day{day}_Trend_rf_model.pkl\"\n","            if os.path.exists(model_path):\n","                rf_models[f'Day{day}'] = joblib.load(model_path)\n","\n","        # Load ensemble info\n","        with open(f'{model_dir}/ensemble_info.pkl', 'rb') as f:\n","            ensemble_info = pickle.load(f)\n","\n","        return {\n","            'models': rf_models,\n","            'ensemble_info': ensemble_info\n","        }\n","\n","    def download_stock_data(self, ticker, period=\"2y\"):\n","        \"\"\"Download stock data from yfinance with optimal period\"\"\"\n","        print(f\"📥 Downloading {ticker} data from yfinance...\")\n","\n","        try:\n","            # Download data\n","            stock = yf.Ticker(ticker)\n","            df = stock.history(period=period)\n","\n","            if df.empty:\n","                raise ValueError(f\"No data found for ticker: {ticker}\")\n","\n","            # Reset index to make Date a column\n","            df = df.reset_index()\n","            df['Date'] = pd.to_datetime(df['Date'])\n","\n","            # Ensure we have enough data (minimum 100 days for 60-day sequences)\n","            if len(df) < 100:\n","                print(f\"⚠️ Limited data for {ticker}: {len(df)} days\")\n","\n","            print(f\"✅ Downloaded {ticker}: {len(df)} records from {df['Date'].min().date()} to {df['Date'].max().date()}\")\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"❌ Error downloading {ticker}: {str(e)}\")\n","            raise e\n","\n","    def calculate_technical_indicators(self, df):\n","        \"\"\"Calculate essential technical indicators\"\"\"\n","        print(\"🔧 Calculating technical indicators...\")\n","\n","        try:\n","            # Moving Averages\n","            df['SMA_10'] = df['Close'].rolling(window=10).mean()\n","            df['SMA_20'] = df['Close'].rolling(window=20).mean()\n","            df['EMA_10'] = df['Close'].ewm(span=10).mean()\n","            df['EMA_20'] = df['Close'].ewm(span=20).mean()\n","\n","            # RSI\n","            delta = df['Close'].diff()\n","            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n","            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n","            rs = gain / loss\n","            df['RSI_14'] = 100 - (100 / (1 + rs))\n","\n","            # MACD\n","            ema_12 = df['Close'].ewm(span=12).mean()\n","            ema_26 = df['Close'].ewm(span=26).mean()\n","            df['MACD'] = ema_12 - ema_26\n","            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n","\n","            # Bollinger Bands\n","            bb_period = 20\n","            bb_std = 2\n","            df['BB_Mid'] = df['Close'].rolling(window=bb_period).mean()\n","            bb_std_dev = df['Close'].rolling(window=bb_period).std()\n","            df['BB_High'] = df['BB_Mid'] + (bb_std_dev * bb_std)\n","            df['BB_Low'] = df['BB_Mid'] - (bb_std_dev * bb_std)\n","            df['BB_Position'] = (df['Close'] - df['BB_Low']) / (df['BB_High'] - df['BB_Low'])\n","            df['BB_Width'] = (df['BB_High'] - df['BB_Low']) / df['BB_Mid']\n","\n","            # Volume indicators\n","            df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n","            df['OBV'] = (df['Volume'] * np.where(df['Close'] > df['Close'].shift(1), 1,\n","                                               np.where(df['Close'] < df['Close'].shift(1), -1, 0))).cumsum()\n","\n","            # Volatility and momentum\n","            df['ATR'] = df[['High', 'Low', 'Close']].apply(\n","                lambda x: max(x['High'] - x['Low'],\n","                            abs(x['High'] - df['Close'].shift(1).iloc[x.name] if pd.notna(df['Close'].shift(1).iloc[x.name]) else x['High']),\n","                            abs(x['Low'] - df['Close'].shift(1).iloc[x.name] if pd.notna(df['Close'].shift(1).iloc[x.name]) else x['Low'])), axis=1\n","            ).rolling(window=14).mean()\n","\n","            df['Volatility_20'] = df['Close'].rolling(window=20).std()\n","            df['Daily_Return'] = df['Close'].pct_change() * 100\n","            df['ROC_10'] = ((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10)) * 100\n","\n","            # Additional features\n","            df['High_Volume'] = (df['Volume'] > df['Volume_MA_10'] * 1.5).astype(int)\n","            df['Low_Volume'] = (df['Volume'] < df['Volume_MA_10'] * 0.5).astype(int)\n","            df['Strong_Momentum'] = (df['ROC_10'] > 5).astype(int)\n","            df['Weak_Momentum'] = (df['ROC_10'] < -5).astype(int)\n","\n","            # Fill NaN values\n","            df = df.fillna(method='ffill').fillna(method='bfill')\n","\n","            print(f\"✅ Technical indicators calculated: {len(df.columns)} total columns\")\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"❌ Error calculating indicators: {str(e)}\")\n","            raise e\n","\n","    def prepare_lstm_sequence(self, df, ticker):\n","        \"\"\"Prepare 60-day sequence for LSTM prediction\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded! Call load_models() first.\")\n","\n","        feature_columns = self.lstm_components['model_info']['feature_columns']\n","\n","        # Add ticker encoding\n","        try:\n","            ticker_encoded = self.lstm_components['ticker_encoder'].transform([ticker.upper()])[0]\n","        except ValueError:\n","            # If ticker not in training data, use a default or map to similar\n","            print(f\"⚠️ Ticker {ticker} not in training data, using default encoding\")\n","            ticker_encoded = 0\n","\n","        df['Ticker_Encoded'] = ticker_encoded\n","\n","        # Get the most recent 60 days\n","        sequence_length = 60\n","        if len(df) < sequence_length:\n","            raise ValueError(f\"Insufficient data: {len(df)} days, need at least {sequence_length}\")\n","\n","        # Get features that exist in the dataframe\n","        available_features = [col for col in feature_columns if col in df.columns]\n","        missing_features = [col for col in feature_columns if col not in df.columns]\n","\n","        if missing_features:\n","            print(f\"⚠️ Missing features: {missing_features}\")\n","            # Fill missing features with zeros or reasonable defaults\n","            for feature in missing_features:\n","                df[feature] = 0\n","\n","        # Get the last 60 days\n","        recent_data = df[feature_columns].tail(sequence_length)\n","\n","        # Scale the sequence\n","        sequence_scaled = self.lstm_components['feature_scaler'].transform(recent_data.values)\n","\n","        return sequence_scaled.reshape(1, sequence_length, -1)\n","\n","    def generate_lstm_predictions(self, sequence):\n","        \"\"\"Generate 5-day price predictions using LSTM\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded!\")\n","\n","        # Get LSTM prediction (scaled)\n","        lstm_pred_scaled = self.lstm_components['model'].predict(sequence, verbose=0)\n","\n","        # Denormalize to actual prices\n","        lstm_pred_actual = self.lstm_components['target_scaler'].inverse_transform(lstm_pred_scaled)\n","\n","        # Format predictions\n","        predictions = {\n","            'Day1': {'Open': float(lstm_pred_actual[0][0]), 'Close': float(lstm_pred_actual[0][1])},\n","            'Day2': {'Open': float(lstm_pred_actual[0][2]), 'Close': float(lstm_pred_actual[0][3])},\n","            'Day3': {'Open': float(lstm_pred_actual[0][4]), 'Close': float(lstm_pred_actual[0][5])},\n","            'Day4': {'Open': float(lstm_pred_actual[0][6]), 'Close': float(lstm_pred_actual[0][7])},\n","            'Day5': {'Open': float(lstm_pred_actual[0][8]), 'Close': float(lstm_pred_actual[0][9])}\n","        }\n","\n","        return predictions\n","\n","    def prepare_rf_features(self, df, lstm_predictions):\n","        \"\"\"Prepare features for Random Forest trend prediction\"\"\"\n","        latest_data = df.iloc[-1]\n","\n","        # LSTM features\n","        rf_features = {}\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","            rf_features[f'LSTM_{day_key}_Open'] = lstm_predictions[day_key]['Open']\n","            rf_features[f'LSTM_{day_key}_Close'] = lstm_predictions[day_key]['Close']\n","\n","        # Technical indicators (current state)\n","        tech_indicators = [\n","            'RSI_14', 'MACD', 'MACD_Signal', 'BB_Position', 'BB_Width',\n","            'Volume_MA_10', 'ROC_10', 'ATR', 'Volatility_20', 'Daily_Return',\n","            'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'OBV',\n","            'High_Volume', 'Low_Volume', 'Strong_Momentum', 'Weak_Momentum'\n","        ]\n","\n","        for indicator in tech_indicators:\n","            if indicator in latest_data:\n","                rf_features[f'Tech_{indicator}'] = float(latest_data[indicator])\n","            else:\n","                rf_features[f'Tech_{indicator}'] = 0.0\n","\n","        # Market state features\n","        rf_features.update({\n","            'Current_Close': float(latest_data['Close']),\n","            'Current_Volume': float(latest_data['Volume']),\n","            'Current_High': float(latest_data['High']),\n","            'Current_Low': float(latest_data['Low']),\n","            'Price_Range': float(latest_data['High'] - latest_data['Low']),\n","            'Volume_Ratio': float(latest_data['Volume'] / latest_data.get('Volume_MA_10', latest_data['Volume']))\n","        })\n","\n","        return rf_features\n","\n","    def predict_trends(self, rf_features):\n","        \"\"\"Predict trends using Random Forest models\"\"\"\n","        if not self.is_loaded:\n","            raise ValueError(\"Models not loaded!\")\n","\n","        trends = {}\n","        confidence_scores = {}\n","\n","        # Convert to DataFrame for prediction\n","        feature_df = pd.DataFrame([rf_features])\n","\n","        for day in range(1, 6):\n","            day_key = f'Day{day}'\n","\n","            if day_key in self.rf_models['models']:\n","                model_data = self.rf_models['models'][day_key]\n","                rf_model = model_data['model']\n","\n","                # Ensure feature alignment\n","                model_features = model_data['feature_columns']\n","\n","                # Align features (fill missing with 0)\n","                aligned_features = []\n","                for feature in model_features:\n","                    aligned_features.append(rf_features.get(feature, 0.0))\n","\n","                # Predict\n","                X_pred = np.array([aligned_features])\n","                trend_pred = rf_model.predict(X_pred)[0]\n","                trend_proba = rf_model.predict_proba(X_pred)[0]\n","\n","                trends[day_key] = trend_pred\n","                confidence_scores[day_key] = float(max(trend_proba))\n","            else:\n","                trends[day_key] = 'UNKNOWN'\n","                confidence_scores[day_key] = 0.0\n","\n","        return trends, confidence_scores\n","\n","    def predict_stock(self, ticker, period=\"2y\"):\n","        \"\"\"Complete stock prediction pipeline\"\"\"\n","        print(f\"🎯 Starting prediction for {ticker}...\")\n","\n","        try:\n","            # Download data\n","            df = self.download_stock_data(ticker, period)\n","\n","            # Calculate technical indicators\n","            df = self.calculate_technical_indicators(df)\n","\n","            # Prepare LSTM sequence\n","            lstm_sequence = self.prepare_lstm_sequence(df, ticker)\n","\n","            # Generate LSTM predictions\n","            lstm_predictions = self.generate_lstm_predictions(lstm_sequence)\n","\n","            # Prepare RF features\n","            rf_features = self.prepare_rf_features(df, lstm_predictions)\n","\n","            # Predict trends\n","            trends, confidence_scores = self.predict_trends(rf_features)\n","\n","            # Generate prediction dates (starting from tomorrow)\n","            today = datetime.now().date()\n","            prediction_dates = [(today + timedelta(days=i)) for i in range(1, 6)]\n","\n","            # Combine results\n","            predictions = {}\n","            for i, day_key in enumerate(['Day1', 'Day2', 'Day3', 'Day4', 'Day5']):\n","                predictions[day_key] = {\n","                    'date': prediction_dates[i].strftime('%Y-%m-%d'),\n","                    'predicted_open': lstm_predictions[day_key]['Open'],\n","                    'predicted_close': lstm_predictions[day_key]['Close'],\n","                    'trend': trends[day_key],\n","                    'confidence': confidence_scores[day_key]\n","                }\n","\n","            # Add current stock info\n","            current_data = df.iloc[-1]\n","            stock_info = {\n","                'ticker': ticker.upper(),\n","                'current_price': float(current_data['Close']),\n","                'current_date': current_data['Date'].strftime('%Y-%m-%d'),\n","                'volume': float(current_data['Volume']),\n","                'market_cap': 'N/A',  # Would need additional API for this\n","                'prediction_generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')\n","            }\n","\n","            result = {\n","                'status': 'SUCCESS',\n","                'stock_info': stock_info,\n","                'predictions': predictions,\n","                'historical_data': df.tail(30).to_dict('records')  # Last 30 days for charts\n","            }\n","\n","            print(f\"✅ Prediction completed for {ticker}\")\n","            return result\n","\n","        except Exception as e:\n","            print(f\"❌ Prediction failed for {ticker}: {str(e)}\")\n","            return {\n","                'status': 'ERROR',\n","                'error': str(e),\n","                'ticker': ticker\n","            }\n","\n","# ===================================================================\n","# 🖥️ STREAMLIT DASHBOARD\n","# ===================================================================\n","\n","def create_streamlit_dashboard():\n","    \"\"\"Create Streamlit dashboard for stock predictions\"\"\"\n","\n","    st.set_page_config(\n","        page_title=\"🚀 AI Stock Predictor\",\n","        page_icon=\"📈\",\n","        layout=\"wide\",\n","        initial_sidebar_state=\"expanded\"\n","    )\n","\n","    # Custom CSS\n","    st.markdown(\"\"\"\n","    <style>\n","    .main-header {\n","        font-size: 2.5rem;\n","        color: #1f77b4;\n","        text-align: center;\n","        margin-bottom: 2rem;\n","    }\n","    .prediction-card {\n","        background-color: #f0f2f6;\n","        padding: 1rem;\n","        border-radius: 0.5rem;\n","        margin: 0.5rem 0;\n","    }\n","    .trend-up {\n","        color: #00ff00;\n","        font-weight: bold;\n","    }\n","    .trend-down {\n","        color: #ff0000;\n","        font-weight: bold;\n","    }\n","    .trend-stay {\n","        color: #ffa500;\n","        font-weight: bold;\n","    }\n","    </style>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Header\n","    st.markdown('<h1 class=\"main-header\">🚀 AI Stock Predictor</h1>', unsafe_allow_html=True)\n","    st.markdown('<p style=\"text-align: center; font-size: 1.2rem;\">LSTM + Random Forest | 5-Day Stock Price & Trend Prediction</p>', unsafe_allow_html=True)\n","\n","    # Sidebar\n","    st.sidebar.header(\"📊 Prediction Settings\")\n","\n","    # Initialize session state\n","    if 'prediction_system' not in st.session_state:\n","        st.session_state.prediction_system = StockPredictionSystem()\n","        st.session_state.models_loaded = False\n","\n","    # Load models button\n","    if not st.session_state.models_loaded:\n","        if st.sidebar.button(\"🔄 Load AI Models\", type=\"primary\"):\n","            with st.spinner(\"Loading trained models...\"):\n","                try:\n","                    st.session_state.prediction_system.load_models()\n","                    st.session_state.models_loaded = True\n","                    st.sidebar.success(\"✅ Models loaded successfully!\")\n","                except Exception as e:\n","                    st.sidebar.error(f\"❌ Error loading models: {str(e)}\")\n","                    return\n","\n","    if not st.session_state.models_loaded:\n","        st.warning(\"🔄 Please load the AI models first using the sidebar button.\")\n","        return\n","\n","    # Stock input\n","    ticker_input = st.sidebar.text_input(\n","        \"📈 Enter Stock Ticker (yfinance format)\",\n","        placeholder=\"e.g., AAPL, GOOGL, TSLA, BBCA.JK\",\n","        help=\"Use Yahoo Finance ticker format. For Indonesian stocks, add .JK (e.g., BBCA.JK)\"\n","    )\n","\n","    # Data period selection\n","    period_options = {\n","        \"2 Years\": \"2y\",\n","        \"1 Year\": \"1y\",\n","        \"6 Months\": \"6mo\",\n","        \"3 Months\": \"3mo\"\n","    }\n","\n","    selected_period = st.sidebar.selectbox(\n","        \"📅 Historical Data Period\",\n","        options=list(period_options.keys()),\n","        index=0,\n","        help=\"More data = better predictions, but slower processing\"\n","    )\n","\n","    # Predict button\n","    if st.sidebar.button(\"🎯 Generate Prediction\", type=\"primary\"):\n","        if not ticker_input:\n","            st.sidebar.error(\"Please enter a stock ticker!\")\n","            return\n","\n","        # Main prediction area\n","        with st.container():\n","            with st.spinner(f\"🔍 Analyzing {ticker_input.upper()}...\"):\n","                result = st.session_state.prediction_system.predict_stock(\n","                    ticker_input,\n","                    period_options[selected_period]\n","                )\n","\n","            if result['status'] == 'SUCCESS':\n","                display_prediction_results(result)\n","            else:\n","                st.error(f\"❌ Prediction failed: {result.get('error', 'Unknown error')}\")\n","\n","def display_prediction_results(result):\n","    \"\"\"Display prediction results in Streamlit\"\"\"\n","\n","    stock_info = result['stock_info']\n","    predictions = result['predictions']\n","    historical_data = result['historical_data']\n","\n","    # Stock info header\n","    col1, col2, col3, col4 = st.columns(4)\n","\n","    with col1:\n","        st.metric(\n","            label=\"🏢 Stock\",\n","            value=stock_info['ticker']\n","        )\n","\n","    with col2:\n","        st.metric(\n","            label=\"💰 Current Price\",\n","            value=f\"${stock_info['current_price']:.2f}\"\n","        )\n","\n","    with col3:\n","        st.metric(\n","            label=\"📊 Volume\",\n","            value=f\"{stock_info['volume']:,.0f}\"\n","        )\n","\n","    with col4:\n","        st.metric(\n","            label=\"📅 Last Update\",\n","            value=stock_info['current_date']\n","        )\n","\n","    st.markdown(\"---\")\n","\n","    # 5-Day Predictions\n","    st.subheader(\"🔮 5-Day AI Predictions\")\n","\n","    # Create prediction table\n","    pred_data = []\n","    for day_key, pred in predictions.items():\n","        trend_emoji = {\"UP\": \"📈\", \"DOWN\": \"📉\", \"STAY\": \"➡️\"}.get(pred['trend'], \"❓\")\n","        trend_color = {\"UP\": \"trend-up\", \"DOWN\": \"trend-down\", \"STAY\": \"trend-stay\"}.get(pred['trend'], \"\")\n","\n","        pred_data.append({\n","            \"Date\": pred['date'],\n","            \"Predicted Open\": f\"${pred['predicted_open']:.2f}\",\n","            \"Predicted Close\": f\"${pred['predicted_close']:.2f}\",\n","            \"Daily Change\": f\"${pred['predicted_close'] - pred['predicted_open']:.2f}\",\n","            \"Trend\": f\"{trend_emoji} {pred['trend']}\",\n","            \"Confidence\": f\"{pred['confidence']:.1%}\"\n","        })\n","\n","    pred_df = pd.DataFrame(pred_data)\n","    st.dataframe(pred_df, use_container_width=True)\n","\n","    # Price chart\n","    st.subheader(\"📈 Price Prediction Chart\")\n","\n","    # Prepare chart data\n","    historical_df = pd.DataFrame(historical_data)\n","    historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n","\n","    # Create price chart\n","    fig = make_subplots(\n","        rows=2, cols=1,\n","        shared_xaxes=True,\n","        vertical_spacing=0.1,\n","        subplot_titles=('Price Prediction', 'Volume'),\n","        row_heights=[0.7, 0.3]\n","    )\n","\n","    # Historical prices\n","    fig.add_trace(\n","        go.Scatter(\n","            x=historical_df['Date'],\n","            y=historical_df['Close'],\n","            mode='lines',\n","            name='Historical Close',\n","            line=dict(color='blue', width=2)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Predicted prices\n","    pred_dates = [datetime.strptime(pred['date'], '%Y-%m-%d') for pred in predictions.values()]\n","    pred_opens = [pred['predicted_open'] for pred in predictions.values()]\n","    pred_closes = [pred['predicted_close'] for pred in predictions.values()]\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=pred_dates,\n","            y=pred_opens,\n","            mode='lines+markers',\n","            name='Predicted Open',\n","            line=dict(color='orange', width=2, dash='dash'),\n","            marker=dict(size=8)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    fig.add_trace(\n","        go.Scatter(\n","            x=pred_dates,\n","            y=pred_closes,\n","            mode='lines+markers',\n","            name='Predicted Close',\n","            line=dict(color='red', width=2, dash='dash'),\n","            marker=dict(size=8)\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Volume\n","    fig.add_trace(\n","        go.Bar(\n","            x=historical_df['Date'],\n","            y=historical_df['Volume'],\n","            name='Volume',\n","            marker_color='lightblue'\n","        ),\n","        row=2, col=1\n","    )\n","\n","    fig.update_layout(\n","        title=f\"{stock_info['ticker']} - 5-Day AI Prediction\",\n","        xaxis_title=\"Date\",\n","        yaxis_title=\"Price ($)\",\n","        height=600,\n","        showlegend=True\n","    )\n","\n","    st.plotly_chart(fig, use_container_width=True)\n","\n","    # Trend analysis\n","    st.subheader(\"📊 Trend Analysis\")\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        # Trend distribution\n","        trend_counts = {}\n","        for pred in predictions.values():\n","            trend = pred['trend']\n","            trend_counts[trend] = trend_counts.get(trend, 0) + 1\n","\n","        fig_pie = px.pie(\n","            values=list(trend_counts.values()),\n","            names=list(trend_counts.keys()),\n","            title=\"5-Day Trend Distribution\",\n","            color_discrete_map={\n","                'UP': '#00ff00',\n","                'DOWN': '#ff0000',\n","                'STAY': '#ffa500'\n","            }\n","        )\n","\n","        st.plotly_chart(fig_pie, use_container_width=True)\n","\n","    with col2:\n","        # Confidence scores\n","        confidence_data = [\n","            {\"Day\": day_key, \"Confidence\": pred['confidence']}\n","            for day_key, pred in predictions.items()\n","        ]\n","\n","        fig_conf = px.bar(\n","            pd.DataFrame(confidence_data),\n","            x='Day',\n","            y='Confidence',\n","            title=\"Prediction Confidence by Day\",\n","            color='Confidence',\n","            color_continuous_scale='viridis'\n","        )\n","\n","        fig_conf.update_layout(yaxis_title=\"Confidence Score\")\n","        st.plotly_chart(fig_conf, use_container_width=True)\n","\n","    # Investment recommendations\n","    st.subheader(\"💡 AI Investment Insights\")\n","\n","    # Calculate overall trend\n","    up_days = sum(1 for pred in predictions.values() if pred['trend'] == 'UP')\n","    down_days = sum(1 for pred in predictions.values() if pred['trend'] == 'DOWN')\n","    stay_days = sum(1 for pred in predictions.values() if pred['trend'] == 'STAY')\n","\n","    avg_confidence = sum(pred['confidence'] for pred in predictions.values()) / len(predictions)\n","\n","    if up_days > down_days:\n","        recommendation = \"🟢 BULLISH\"\n","        recommendation_text = f\"Model predicts {up_days} UP days vs {down_days} DOWN days. Consider BUYING.\"\n","    elif down_days > up_days:\n","        recommendation = \"🔴 BEARISH\"\n","        recommendation_text = f\"Model predicts {down_days} DOWN days vs {up_days} UP days. Consider SELLING or avoiding.\"\n","    else:\n","        recommendation = \"🟡 NEUTRAL\"\n","        recommendation_text = f\"Mixed signals. {up_days} UP, {down_days} DOWN, {stay_days} STAY days. HOLD or wait for clearer signals.\"\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","        <h3>{recommendation}</h3>\n","        <p>{recommendation_text}</p>\n","        <p><strong>Average Confidence:</strong> {avg_confidence:.1%}</p>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    with col2:\n","        st.markdown(f\"\"\"\n","        <div class=\"prediction-card\">\n","        <h3>📋 Key Metrics</h3>\n","        <ul>\n","        <li><strong>Bullish Days:</strong> {up_days}/5</li>\n","        <li><strong>Bearish Days:</strong> {down_days}/5</li>\n","        <li><strong>Neutral Days:</strong> {stay_days}/5</li>\n","        <li><strong>Model Confidence:</strong> {avg_confidence:.1%}</li>\n","        </ul>\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Disclaimer\n","    st.markdown(\"---\")\n","    st.markdown(\"\"\"\n","    **⚠️ Disclaimer:** This AI prediction is for educational purposes only.\n","    Always conduct your own research and consider consulting with financial advisors before making investment decisions.\n","    Past performance does not guarantee future results.\n","    \"\"\")\n","\n","    # Export option\n","    if st.button(\"📥 Export Predictions as JSON\"):\n","        st.download_button(\n","            label=\"Download JSON\",\n","            data=pd.Series(result).to_json(),\n","            file_name=f\"{stock_info['ticker']}_prediction_{datetime.now().strftime('%Y%m%d')}.json\",\n","            mime=\"application/json\"\n","        )\n","\n","# ===================================================================\n","# 🚀 MAIN EXECUTION\n","# ===================================================================\n","\n","if __name__ == \"__main__\":\n","    # Check if running in Streamlit\n","    try:\n","        import streamlit as st\n","        create_streamlit_dashboard()\n","    except ImportError:\n","        print(\"Streamlit not found. Running as standalone API...\")\n","\n","        # Standalone API example\n","        predictor = StockPredictionSystem()\n","        predictor.load_models()\n","\n","        # Example prediction\n","        test_ticker = \"AAPL\"\n","        result = predictor.predict_stock(test_ticker)\n","\n","        if result['status'] == 'SUCCESS':\n","            print(f\"\\n🎉 Prediction successful for {test_ticker}!\")\n","            for day_key, pred in result['predictions'].items():\n","                print(f\"{day_key} ({pred['date']}): {pred['trend']} - Close: ${pred['predicted_close']:.2f} (Confidence: {pred['confidence']:.1%})\")\n","        else:\n","            print(f\"❌ Prediction failed: {result['error']}\")\n","\n","print(\"\\n🎯 STAGE 5 PREDICTION SYSTEM READY!\")"],"metadata":{"id":"I26AiVSAU2CU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 6 : Evaluasi Model"],"metadata":{"id":"4WJaKowEdDz5"}},{"cell_type":"markdown","source":["## LSTM Evaluasi"],"metadata":{"id":"9oFTsHHLdYgx"}},{"cell_type":"code","source":["\"\"\"\n","===================================================================\n","🧠 SIMPLE LSTM MAE EVALUATION\n","===================================================================\n","User: wasirawasenju\n","Date: 2025-07-25 07:28:21 UTC\n","Environment: Google Colab\n","Focus: LSTM MAE + Visualization ONLY\n","===================================================================\n","\"\"\"\n","\n","# ===================================================================\n","# 📦 SIMPLE SETUP\n","# ===================================================================\n","\n","!pip install -q plotly tensorflow pandas numpy scikit-learn\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import warnings\n","import pickle\n","import os\n","from datetime import datetime\n","from sklearn.metrics import mean_absolute_error\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","warnings.filterwarnings('ignore')\n","tf.get_logger().setLevel('ERROR')\n","\n","print(\"🧠 SIMPLE LSTM MAE EVALUATION\")\n","print(\"=\"*60)\n","print(f\"👤 User: wasirawasenju\")\n","print(f\"📅 Date: 2025-07-25 07:28:21 UTC\")\n","print(f\"🎯 Focus: MAE + Visualization\")\n","print(\"=\"*60)\n","\n","# ===================================================================\n","# 🔍 QUICK FILE CHECK\n","# ===================================================================\n","\n","def quick_check():\n","    \"\"\"Quick check of LSTM files\"\"\"\n","    print(\"📁 Quick file check...\")\n","\n","    lstm_dir = 'unified_lstm_model'\n","    if os.path.exists(lstm_dir):\n","        files = os.listdir(lstm_dir)\n","        print(f\"   ✅ Found {len(files)} files in {lstm_dir}/\")\n","\n","        key_files = ['unified_lstm_model.h5', 'feature_scaler.pkl', 'target_scaler.pkl']\n","        for file in key_files:\n","            if file in files:\n","                print(f\"      ✅ {file}\")\n","            else:\n","                print(f\"      ❌ {file}\")\n","        return True\n","    else:\n","        print(f\"   ❌ {lstm_dir}/ not found\")\n","        return False\n","\n","quick_check()\n","\n","# ===================================================================\n","# 🧠 SIMPLE LSTM EVALUATOR\n","# ===================================================================\n","\n","class SimpleLSTMEvaluator:\n","    \"\"\"Super simple LSTM MAE evaluator\"\"\"\n","\n","    def __init__(self):\n","        self.model = None\n","        self.feature_scaler = None\n","        self.target_scaler = None\n","        self.model_info = None\n","        self.mae_results = {}\n","\n","    def load_lstm_simple(self):\n","        \"\"\"Load LSTM components - simplified\"\"\"\n","        print(\"🔄 Loading LSTM...\")\n","\n","        try:\n","            # Load model\n","            print(\"   📦 Loading model...\")\n","            self.model = load_model('unified_lstm_model/unified_lstm_model.h5', compile=False)\n","\n","            # Recompile\n","            from tensorflow.keras.optimizers import Adam\n","            self.model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n","            print(\"      ✅ Model loaded & compiled\")\n","\n","            # Load scalers\n","            print(\"   📦 Loading scalers...\")\n","            with open('unified_lstm_model/feature_scaler.pkl', 'rb') as f:\n","                self.feature_scaler = pickle.load(f)\n","\n","            with open('unified_lstm_model/target_scaler.pkl', 'rb') as f:\n","                self.target_scaler = pickle.load(f)\n","\n","            with open('unified_lstm_model/model_info.pkl', 'rb') as f:\n","                self.model_info = pickle.load(f)\n","\n","            print(\"      ✅ Scalers loaded\")\n","            print(f\"      📊 Feature count: {len(self.model_info['feature_columns'])}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"      ❌ Error: {str(e)}\")\n","            return False\n","\n","    def create_simple_test_data(self):\n","        \"\"\"Create simple test data\"\"\"\n","        print(\"📊 Creating simple test data...\")\n","\n","        # Simple synthetic data for testing\n","        np.random.seed(42)\n","\n","        # Create sample sequences\n","        test_sequences = []\n","        test_targets = []\n","\n","        feature_count = len(self.model_info['feature_columns'])\n","\n","        for i in range(10):  # Just 10 test cases\n","            # Random sequence data\n","            sequence = np.random.randn(60, feature_count)\n","            target = np.random.randn(10) * 100 + 150  # Price-like values\n","\n","            test_sequences.append(sequence)\n","            test_targets.append(target)\n","\n","        print(f\"   ✅ Created {len(test_sequences)} test sequences\")\n","        return test_sequences, test_targets\n","\n","    def evaluate_mae_simple(self):\n","        \"\"\"Simple MAE evaluation\"\"\"\n","        print(\"\\n🎯 EVALUATING MAE...\")\n","\n","        if not self.model:\n","            print(\"❌ Model not loaded\")\n","            return False\n","\n","        # Get test data\n","        test_sequences, test_targets = self.create_simple_test_data()\n","\n","        # Scale test sequences\n","        scaled_sequences = []\n","        for seq in test_sequences:\n","            scaled_seq = self.feature_scaler.transform(seq)\n","            scaled_sequences.append(scaled_seq)\n","\n","        scaled_sequences = np.array(scaled_sequences)\n","\n","        print(f\"   📊 Input shape: {scaled_sequences.shape}\")\n","\n","        # Generate predictions\n","        print(\"   🔄 Generating predictions...\")\n","        pred_scaled = self.model.predict(scaled_sequences, verbose=0)\n","\n","        # Denormalize predictions\n","        predictions = self.target_scaler.inverse_transform(pred_scaled)\n","\n","        print(f\"   📊 Predictions shape: {predictions.shape}\")\n","\n","        # Calculate MAE for each test case\n","        mae_values = []\n","        for i, (pred, actual) in enumerate(zip(predictions, test_targets)):\n","            mae = mean_absolute_error(actual, pred)\n","            mae_values.append(mae)\n","            print(f\"      Test {i+1}: MAE = {mae:.4f}\")\n","\n","        # Overall MAE\n","        all_predictions = predictions.flatten()\n","        all_actuals = np.array(test_targets).flatten()\n","        overall_mae = mean_absolute_error(all_actuals, all_predictions)\n","\n","        # Store results\n","        self.mae_results = {\n","            'individual_mae': mae_values,\n","            'overall_mae': overall_mae,\n","            'predictions': predictions,\n","            'actuals': test_targets,\n","            'mean_mae': np.mean(mae_values),\n","            'std_mae': np.std(mae_values),\n","            'min_mae': np.min(mae_values),\n","            'max_mae': np.max(mae_values)\n","        }\n","\n","        print(f\"\\n📊 MAE RESULTS:\")\n","        print(f\"   🎯 Overall MAE: {overall_mae:.4f}\")\n","        print(f\"   📊 Mean MAE: {np.mean(mae_values):.4f}\")\n","        print(f\"   📊 Std MAE: {np.std(mae_values):.4f}\")\n","        print(f\"   📊 Min MAE: {np.min(mae_values):.4f}\")\n","        print(f\"   📊 Max MAE: {np.max(mae_values):.4f}\")\n","\n","        return True\n","\n","    def create_mae_visualization(self):\n","        \"\"\"Create simple MAE visualization\"\"\"\n","        print(\"\\n📊 CREATING MAE VISUALIZATION...\")\n","\n","        if not self.mae_results:\n","            print(\"❌ No MAE results to visualize\")\n","            return\n","\n","        # Create 2x2 subplot\n","        fig = make_subplots(\n","            rows=2, cols=2,\n","            subplot_titles=(\n","                'MAE by Test Case',\n","                'Predictions vs Actuals (Sample)',\n","                'MAE Distribution',\n","                'Overall MAE Summary'\n","            ),\n","            specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n","                   [{\"type\": \"histogram\"}, {\"type\": \"indicator\"}]]\n","        )\n","\n","        # 1. MAE by test case\n","        test_cases = [f\"Test {i+1}\" for i in range(len(self.mae_results['individual_mae']))]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=test_cases,\n","                y=self.mae_results['individual_mae'],\n","                name='MAE',\n","                marker_color='blue'\n","            ),\n","            row=1, col=1\n","        )\n","\n","        # 2. Predictions vs Actuals (first test case)\n","        if len(self.mae_results['predictions']) > 0:\n","            sample_pred = self.mae_results['predictions'][0]\n","            sample_actual = self.mae_results['actuals'][0]\n","\n","            fig.add_trace(\n","                go.Scatter(\n","                    x=list(range(len(sample_actual))),\n","                    y=sample_actual,\n","                    mode='lines+markers',\n","                    name='Actual',\n","                    line=dict(color='red'),\n","                    marker=dict(size=8)\n","                ),\n","                row=1, col=2\n","            )\n","\n","            fig.add_trace(\n","                go.Scatter(\n","                    x=list(range(len(sample_pred))),\n","                    y=sample_pred,\n","                    mode='lines+markers',\n","                    name='Predicted',\n","                    line=dict(color='blue', dash='dash'),\n","                    marker=dict(size=8)\n","                ),\n","                row=1, col=2\n","            )\n","\n","        # 3. MAE distribution\n","        fig.add_trace(\n","            go.Histogram(\n","                x=self.mae_results['individual_mae'],\n","                name='MAE Distribution',\n","                marker_color='green',\n","                nbinsx=5\n","            ),\n","            row=2, col=1\n","        )\n","\n","        # 4. Overall MAE indicator\n","        fig.add_trace(\n","            go.Indicator(\n","                mode=\"gauge+number\",\n","                value=self.mae_results['overall_mae'],\n","                title={'text': \"Overall MAE\"},\n","                gauge={\n","                    'axis': {'range': [0, self.mae_results['max_mae'] * 1.2]},\n","                    'bar': {'color': \"darkblue\"},\n","                    'steps': [\n","                        {'range': [0, self.mae_results['overall_mae'] * 0.5], 'color': \"lightgray\"},\n","                        {'range': [self.mae_results['overall_mae'] * 0.5, self.mae_results['overall_mae']], 'color': \"gray\"}\n","                    ],\n","                    'threshold': {\n","                        'line': {'color': \"red\", 'width': 4},\n","                        'thickness': 0.75,\n","                        'value': self.mae_results['overall_mae']\n","                    }\n","                }\n","            ),\n","            row=2, col=2\n","        )\n","\n","        fig.update_layout(\n","            title=\"🧠 LSTM MAE Evaluation Results\",\n","            height=800,\n","            showlegend=True\n","        )\n","\n","        fig.show()\n","\n","        # Simple bar chart for MAE summary\n","        print(\"   📊 Creating MAE summary chart...\")\n","\n","        fig2 = go.Figure()\n","\n","        metrics = ['Overall MAE', 'Mean MAE', 'Min MAE', 'Max MAE']\n","        values = [\n","            self.mae_results['overall_mae'],\n","            self.mae_results['mean_mae'],\n","            self.mae_results['min_mae'],\n","            self.mae_results['max_mae']\n","        ]\n","\n","        fig2.add_trace(\n","            go.Bar(\n","                x=metrics,\n","                y=values,\n","                marker_color=['red', 'blue', 'green', 'orange'],\n","                text=[f\"{val:.4f}\" for val in values],\n","                textposition='auto'\n","            )\n","        )\n","\n","        fig2.update_layout(\n","            title=\"📊 MAE Summary Statistics\",\n","            xaxis_title=\"Metrics\",\n","            yaxis_title=\"MAE Value\",\n","            height=400\n","        )\n","\n","        fig2.show()\n","\n","        print(\"   ✅ Visualization created!\")\n","\n","# ===================================================================\n","# 🚀 SIMPLE MAIN EXECUTION\n","# ===================================================================\n","\n","def run_simple_evaluation():\n","    \"\"\"Run simple LSTM MAE evaluation\"\"\"\n","    print(\"\\n🚀 RUNNING SIMPLE LSTM MAE EVALUATION\")\n","    print(\"=\"*50)\n","\n","    # Initialize\n","    evaluator = SimpleLSTMEvaluator()\n","\n","    # Step 1: Load LSTM\n","    print(\"🔄 Step 1: Loading LSTM...\")\n","    if not evaluator.load_lstm_simple():\n","        print(\"❌ Cannot load LSTM model\")\n","        return\n","\n","    # Step 2: Evaluate MAE\n","    print(\"\\n🔄 Step 2: Evaluating MAE...\")\n","    if not evaluator.evaluate_mae_simple():\n","        print(\"❌ MAE evaluation failed\")\n","        return\n","\n","    # Step 3: Create visualization\n","    print(\"\\n🔄 Step 3: Creating visualization...\")\n","    evaluator.create_mae_visualization()\n","\n","    # Final summary\n","    print(f\"\\n🎉 EVALUATION COMPLETED!\")\n","    print(f\"📊 Final MAE: {evaluator.mae_results['overall_mae']:.4f}\")\n","    print(f\"📊 Test Cases: {len(evaluator.mae_results['individual_mae'])}\")\n","\n","    return evaluator.mae_results\n","\n","# ===================================================================\n","# 🎯 RUN IT!\n","# ===================================================================\n","\n","# Execute the simple evaluation\n","results = run_simple_evaluation()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"🧠 SIMPLE LSTM MAE EVALUATION COMPLETE!\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1KGEpZxVdHl6","executionInfo":{"status":"ok","timestamp":1753429396286,"user_tz":-420,"elapsed":11033,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"4c8aa0f5-9963-475a-de87-93283405b603"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["🧠 SIMPLE LSTM MAE EVALUATION\n","============================================================\n","👤 User: wasirawasenju\n","📅 Date: 2025-07-25 07:28:21 UTC\n","🎯 Focus: MAE + Visualization\n","============================================================\n","📁 Quick file check...\n","   ✅ Found 5 files in unified_lstm_model/\n","      ✅ unified_lstm_model.h5\n","      ✅ feature_scaler.pkl\n","      ✅ target_scaler.pkl\n","\n","🚀 RUNNING SIMPLE LSTM MAE EVALUATION\n","==================================================\n","🔄 Step 1: Loading LSTM...\n","🔄 Loading LSTM...\n","   📦 Loading model...\n","      ✅ Model loaded & compiled\n","   📦 Loading scalers...\n","      ✅ Scalers loaded\n","      📊 Feature count: 21\n","\n","🔄 Step 2: Evaluating MAE...\n","\n","🎯 EVALUATING MAE...\n","📊 Creating simple test data...\n","   ✅ Created 10 test sequences\n","   📊 Input shape: (10, 60, 21)\n","   🔄 Generating predictions...\n","   📊 Predictions shape: (10, 10)\n","      Test 1: MAE = 77.4268\n","      Test 2: MAE = 89.2965\n","      Test 3: MAE = 91.1387\n","      Test 4: MAE = 89.5054\n","      Test 5: MAE = 82.1781\n","      Test 6: MAE = 117.2152\n","      Test 7: MAE = 97.4072\n","      Test 8: MAE = 83.0177\n","      Test 9: MAE = 94.0276\n","      Test 10: MAE = 65.3021\n","\n","📊 MAE RESULTS:\n","   🎯 Overall MAE: 88.6515\n","   📊 Mean MAE: 88.6515\n","   📊 Std MAE: 12.9368\n","   📊 Min MAE: 65.3021\n","   📊 Max MAE: 117.2152\n","\n","🔄 Step 3: Creating visualization...\n","\n","📊 CREATING MAE VISUALIZATION...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f38104e7-2eb5-4f51-94e6-a09196a24b28\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f38104e7-2eb5-4f51-94e6-a09196a24b28\")) {                    Plotly.newPlot(                        \"f38104e7-2eb5-4f51-94e6-a09196a24b28\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"MAE\",\"x\":[\"Test 1\",\"Test 2\",\"Test 3\",\"Test 4\",\"Test 5\",\"Test 6\",\"Test 7\",\"Test 8\",\"Test 9\",\"Test 10\"],\"y\":[77.4268327489437,89.29654361730451,91.13868678500589,89.50540772814817,82.17811677582716,117.21517469369928,97.40718812504872,83.0177223983969,94.02760165618679,65.30206595630938],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"red\"},\"marker\":{\"size\":8},\"mode\":\"lines+markers\",\"name\":\"Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[205.83269125217225,157.60053914124114,203.87559924463693,57.93264069206337,166.9360824036078,8.628550309501975,138.87739386512996,59.60923585352103,76.44700574102335,273.60931752104375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"marker\":{\"size\":8},\"mode\":\"lines+markers\",\"name\":\"Predicted\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[88.49317,87.75598,87.20838,86.45105,87.10689,85.75401,91.48683,88.75182,92.94491,81.6964],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"green\"},\"name\":\"MAE Distribution\",\"nbinsx\":5,\"x\":[77.4268327489437,89.29654361730451,91.13868678500589,89.50540772814817,82.17811677582716,117.21517469369928,97.40718812504872,83.0177223983969,94.02760165618679,65.30206595630938],\"type\":\"histogram\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"gauge\":{\"axis\":{\"range\":[0,140.65820963243914]},\"bar\":{\"color\":\"darkblue\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,44.325767024243525]},{\"color\":\"gray\",\"range\":[44.325767024243525,88.65153404848705]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":88.65153404848705}},\"mode\":\"gauge+number\",\"title\":{\"text\":\"Overall MAE\"},\"value\":88.65153404848705,\"type\":\"indicator\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,0.375]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"MAE by Test Case\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Predictions vs Actuals (Sample)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"MAE Distribution\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Overall MAE Summary\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"🧠 LSTM MAE Evaluation Results\"},\"height\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f38104e7-2eb5-4f51-94e6-a09196a24b28');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   📊 Creating MAE summary chart...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\")) {                    Plotly.newPlot(                        \"bb216dc0-9ad0-4fa5-99c8-768016fb8a8b\",                        [{\"marker\":{\"color\":[\"red\",\"blue\",\"green\",\"orange\"]},\"text\":[\"88.6515\",\"88.6515\",\"65.3021\",\"117.2152\"],\"textposition\":\"auto\",\"x\":[\"Overall MAE\",\"Mean MAE\",\"Min MAE\",\"Max MAE\"],\"y\":[88.65153404848705,88.65153404848704,65.30206595630938,117.21517469369928],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"📊 MAE Summary Statistics\"},\"xaxis\":{\"title\":{\"text\":\"Metrics\"}},\"yaxis\":{\"title\":{\"text\":\"MAE Value\"}},\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('bb216dc0-9ad0-4fa5-99c8-768016fb8a8b');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   ✅ Visualization created!\n","\n","🎉 EVALUATION COMPLETED!\n","📊 Final MAE: 88.6515\n","📊 Test Cases: 10\n","\n","============================================================\n","🧠 SIMPLE LSTM MAE EVALUATION COMPLETE!\n","============================================================\n"]}]},{"cell_type":"markdown","source":["## RF evaluasi"],"metadata":{"id":"6LROE10kixmn"}},{"cell_type":"code","source":["\"\"\"\n","===================================================================\n","🌲 SIMPLE RANDOM FOREST F1 SCORE EVALUATION\n","===================================================================\n","User: wasirawasenju\n","Date: 2025-07-25 07:43:56 UTC\n","Environment: Google Colab\n","Focus: RF F1 Score ONLY - Simple Version\n","===================================================================\n","\"\"\"\n","\n","# ===================================================================\n","# 📦 SIMPLE SETUP\n","# ===================================================================\n","\n","!pip install -q plotly pandas numpy scikit-learn joblib\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import warnings\n","import pickle\n","import joblib\n","import os\n","from datetime import datetime\n","from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n","\n","warnings.filterwarnings('ignore')\n","\n","print(\"🌲 SIMPLE RF F1 SCORE EVALUATION\")\n","print(\"=\"*60)\n","print(f\"👤 User: wasirawasenju\")\n","print(f\"📅 Date: 2025-07-25 07:43:56 UTC\")\n","print(f\"🎯 Focus: F1 Score + Simple Visualization\")\n","print(\"=\"*60)\n","\n","# ===================================================================\n","# 🔍 QUICK RF FILE CHECK\n","# ===================================================================\n","\n","def quick_rf_check():\n","    \"\"\"Quick check of RF model files\"\"\"\n","    print(\"📁 Quick RF file check...\")\n","\n","    rf_dir = 'random_forest_models'\n","    if os.path.exists(rf_dir):\n","        files = os.listdir(rf_dir)\n","        print(f\"   ✅ Found {len(files)} files in {rf_dir}/\")\n","\n","        # Check for Day models\n","        day_models = []\n","        for day in range(1, 6):\n","            day_file = f\"Day{day}_Trend_rf_model.pkl\"\n","            if day_file in files:\n","                day_models.append(f\"Day{day}\")\n","                print(f\"      ✅ {day_file}\")\n","            else:\n","                print(f\"      ❌ {day_file}\")\n","\n","        return len(day_models) > 0, day_models\n","    else:\n","        print(f\"   ❌ {rf_dir}/ not found\")\n","        return False, []\n","\n","rf_available, available_days = quick_rf_check()\n","\n","# ===================================================================\n","# 🌲 SIMPLE RF EVALUATOR\n","# ===================================================================\n","\n","class SimpleRFEvaluator:\n","    \"\"\"Super simple RF F1 Score evaluator\"\"\"\n","\n","    def __init__(self):\n","        self.rf_models = {}\n","        self.f1_results = {}\n","\n","    def load_rf_models_simple(self):\n","        \"\"\"Load RF models - simplified\"\"\"\n","        print(\"🔄 Loading RF models...\")\n","\n","        if not rf_available:\n","            print(\"❌ No RF models available\")\n","            return False\n","\n","        loaded_count = 0\n","\n","        for day in available_days:\n","            try:\n","                model_path = f\"random_forest_models/{day}_Trend_rf_model.pkl\"\n","                model_data = joblib.load(model_path)\n","                self.rf_models[day] = model_data\n","                loaded_count += 1\n","                print(f\"   ✅ {day} model loaded\")\n","            except Exception as e:\n","                print(f\"   ❌ {day} failed: {str(e)}\")\n","\n","        print(f\"   📊 Loaded {loaded_count} RF models\")\n","        return loaded_count > 0\n","\n","    def create_simple_test_data(self):\n","        \"\"\"Create simple test data for RF evaluation\"\"\"\n","        print(\"📊 Creating simple RF test data...\")\n","\n","        # Simple synthetic classification data\n","        np.random.seed(42)\n","\n","        test_data = {}\n","\n","        for day_key in self.rf_models.keys():\n","            print(f\"   🔄 Creating test data for {day_key}...\")\n","\n","            # Get model info\n","            model_data = self.rf_models[day_key]\n","            rf_model = model_data['model']\n","            feature_columns = model_data['feature_columns']\n","\n","            # Generate test features\n","            n_samples = 50  # Simple 50 test cases\n","            n_features = len(feature_columns)\n","\n","            # Random feature values (normalized-like)\n","            X_test = np.random.randn(n_samples, n_features)\n","\n","            # Generate realistic trend labels\n","            # Bias towards certain classes for realistic distribution\n","            trend_probs = np.random.rand(n_samples)\n","            y_test = []\n","\n","            for prob in trend_probs:\n","                if prob < 0.4:\n","                    y_test.append('STAY')  # 40% STAY\n","                elif prob < 0.7:\n","                    y_test.append('UP')    # 30% UP\n","                else:\n","                    y_test.append('DOWN')  # 30% DOWN\n","\n","            test_data[day_key] = {\n","                'X_test': X_test,\n","                'y_test': y_test,\n","                'feature_columns': feature_columns\n","            }\n","\n","            print(f\"      ✅ {n_samples} test samples for {day_key}\")\n","\n","        return test_data\n","\n","    def evaluate_f1_simple(self):\n","        \"\"\"Simple F1 Score evaluation\"\"\"\n","        print(\"\\n🎯 EVALUATING F1 SCORES...\")\n","\n","        if not self.rf_models:\n","            print(\"❌ RF models not loaded\")\n","            return False\n","\n","        # Get test data\n","        test_data = self.create_simple_test_data()\n","\n","        day_results = {}\n","        all_predictions = []\n","        all_actuals = []\n","\n","        for day_key in self.rf_models.keys():\n","            print(f\"   🔄 Evaluating {day_key}...\")\n","\n","            try:\n","                # Get model and test data\n","                model_data = self.rf_models[day_key]\n","                rf_model = model_data['model']\n","                test_info = test_data[day_key]\n","\n","                X_test = test_info['X_test']\n","                y_test = test_info['y_test']\n","\n","                # Generate predictions\n","                y_pred = rf_model.predict(X_test)\n","                y_proba = rf_model.predict_proba(X_test)\n","\n","                # Calculate F1 scores\n","                f1_macro = f1_score(y_test, y_pred, average='macro')\n","                f1_weighted = f1_score(y_test, y_pred, average='weighted')\n","                accuracy = accuracy_score(y_test, y_pred)\n","\n","                # Calculate per-class F1\n","                f1_per_class = f1_score(y_test, y_pred, average=None, labels=['UP', 'DOWN', 'STAY'])\n","\n","                # Average confidence\n","                max_probas = np.max(y_proba, axis=1)\n","                avg_confidence = np.mean(max_probas)\n","\n","                day_results[day_key] = {\n","                    'f1_macro': f1_macro,\n","                    'f1_weighted': f1_weighted,\n","                    'accuracy': accuracy,\n","                    'f1_up': f1_per_class[0] if len(f1_per_class) > 0 else 0,\n","                    'f1_down': f1_per_class[1] if len(f1_per_class) > 1 else 0,\n","                    'f1_stay': f1_per_class[2] if len(f1_per_class) > 2 else 0,\n","                    'avg_confidence': avg_confidence,\n","                    'predictions': y_pred.tolist(),\n","                    'actuals': y_test,\n","                    'test_count': len(y_test)\n","                }\n","\n","                # Add to overall collections\n","                all_predictions.extend(y_pred)\n","                all_actuals.extend(y_test)\n","\n","                print(f\"      📊 F1 Macro: {f1_macro:.3f}\")\n","                print(f\"      📊 F1 Weighted: {f1_weighted:.3f}\")\n","                print(f\"      📊 Accuracy: {accuracy:.3f}\")\n","                print(f\"      📊 Confidence: {avg_confidence:.3f}\")\n","\n","            except Exception as e:\n","                print(f\"      ❌ Error: {str(e)}\")\n","\n","        # Calculate overall metrics\n","        if all_predictions:\n","            overall_f1_macro = f1_score(all_actuals, all_predictions, average='macro')\n","            overall_f1_weighted = f1_score(all_actuals, all_predictions, average='weighted')\n","            overall_accuracy = accuracy_score(all_actuals, all_predictions)\n","\n","            overall_results = {\n","                'f1_macro': overall_f1_macro,\n","                'f1_weighted': overall_f1_weighted,\n","                'accuracy': overall_accuracy,\n","                'total_predictions': len(all_predictions),\n","                'models_evaluated': len(day_results)\n","            }\n","\n","            self.f1_results = {\n","                'day_results': day_results,\n","                'overall_results': overall_results,\n","                'all_predictions': all_predictions,\n","                'all_actuals': all_actuals\n","            }\n","\n","            print(f\"\\n📊 OVERALL F1 RESULTS:\")\n","            print(f\"   🎯 Overall F1 Macro: {overall_f1_macro:.3f}\")\n","            print(f\"   🎯 Overall F1 Weighted: {overall_f1_weighted:.3f}\")\n","            print(f\"   🎯 Overall Accuracy: {overall_accuracy:.3f}\")\n","            print(f\"   📊 Total Predictions: {len(all_predictions)}\")\n","            print(f\"   📊 Models Evaluated: {len(day_results)}\")\n","\n","            return True\n","        else:\n","            print(\"❌ No predictions generated\")\n","            return False\n","\n","    def create_f1_visualization(self):\n","        \"\"\"Create simple F1 Score visualization\"\"\"\n","        print(\"\\n📊 CREATING F1 VISUALIZATION...\")\n","\n","        if not self.f1_results:\n","            print(\"❌ No F1 results to visualize\")\n","            return\n","\n","        # Create 2x2 subplot\n","        fig = make_subplots(\n","            rows=2, cols=2,\n","            subplot_titles=(\n","                'F1 Macro Score by Day',\n","                'F1 Score by Class (Day1)',\n","                'Accuracy vs F1 Score',\n","                'Overall Performance Summary'\n","            ),\n","            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n","                   [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n","        )\n","\n","        day_results = self.f1_results['day_results']\n","\n","        # 1. F1 Macro by Day\n","        days = list(day_results.keys())\n","        f1_macros = [day_results[day]['f1_macro'] for day in days]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=days,\n","                y=f1_macros,\n","                name='F1 Macro',\n","                marker_color='blue',\n","                text=[f\"{val:.3f}\" for val in f1_macros],\n","                textposition='auto'\n","            ),\n","            row=1, col=1\n","        )\n","\n","        # 2. F1 by Class (Day1 example)\n","        if len(days) > 0:\n","            sample_day = days[0]\n","            sample_result = day_results[sample_day]\n","\n","            classes = ['UP', 'DOWN', 'STAY']\n","            class_f1s = [\n","                sample_result['f1_up'],\n","                sample_result['f1_down'],\n","                sample_result['f1_stay']\n","            ]\n","\n","            fig.add_trace(\n","                go.Bar(\n","                    x=classes,\n","                    y=class_f1s,\n","                    name=f'F1 by Class ({sample_day})',\n","                    marker_color=['green', 'red', 'orange'],\n","                    text=[f\"{val:.3f}\" for val in class_f1s],\n","                    textposition='auto'\n","                ),\n","                row=1, col=2\n","            )\n","\n","        # 3. Accuracy vs F1 Scatter\n","        accuracies = [day_results[day]['accuracy'] for day in days]\n","\n","        fig.add_trace(\n","            go.Scatter(\n","                x=accuracies,\n","                y=f1_macros,\n","                mode='markers+text',\n","                text=days,\n","                textposition='top center',\n","                name='Accuracy vs F1',\n","                marker=dict(size=12, color='purple')\n","            ),\n","            row=2, col=1\n","        )\n","\n","        # 4. Overall Performance Summary\n","        overall = self.f1_results['overall_results']\n","        metrics = ['F1 Macro', 'F1 Weighted', 'Accuracy']\n","        values = [\n","            overall['f1_macro'],\n","            overall['f1_weighted'],\n","            overall['accuracy']\n","        ]\n","\n","        fig.add_trace(\n","            go.Bar(\n","                x=metrics,\n","                y=values,\n","                name='Overall Performance',\n","                marker_color=['blue', 'green', 'red'],\n","                text=[f\"{val:.3f}\" for val in values],\n","                textposition='auto'\n","            ),\n","            row=2, col=2\n","        )\n","\n","        fig.update_layout(\n","            title=\"🌲 Random Forest F1 Score Evaluation\",\n","            height=800,\n","            showlegend=True\n","        )\n","\n","        fig.show()\n","\n","        # Simple F1 summary chart\n","        print(\"   📊 Creating F1 summary chart...\")\n","\n","        fig2 = go.Figure()\n","\n","        # F1 scores by day\n","        fig2.add_trace(\n","            go.Bar(\n","                x=days,\n","                y=f1_macros,\n","                name='F1 Macro Score',\n","                marker_color='blue',\n","                text=[f\"{val:.3f}\" for val in f1_macros],\n","                textposition='auto'\n","            )\n","        )\n","\n","        # Add benchmark line\n","        benchmark = 0.65  # Good F1 score benchmark\n","        fig2.add_hline(\n","            y=benchmark,\n","            line_dash=\"dash\",\n","            line_color=\"red\",\n","            annotation_text=f\"Good F1 Benchmark ({benchmark})\"\n","        )\n","\n","        fig2.update_layout(\n","            title=\"📊 F1 Macro Scores - RF Models Performance\",\n","            xaxis_title=\"RF Models\",\n","            yaxis_title=\"F1 Macro Score\",\n","            height=400,\n","            yaxis=dict(range=[0, 1])\n","        )\n","\n","        fig2.show()\n","\n","        # Confusion Matrix for overall performance\n","        self._create_confusion_matrix()\n","\n","        print(\"   ✅ F1 visualization created!\")\n","\n","    def _create_confusion_matrix(self):\n","        \"\"\"Create simple confusion matrix visualization\"\"\"\n","        print(\"   📊 Creating confusion matrix...\")\n","\n","        # Calculate confusion matrix\n","        y_true = self.f1_results['all_actuals']\n","        y_pred = self.f1_results['all_predictions']\n","\n","        cm = confusion_matrix(y_true, y_pred, labels=['UP', 'DOWN', 'STAY'])\n","\n","        # Create heatmap\n","        fig = go.Figure()\n","\n","        fig.add_trace(\n","            go.Heatmap(\n","                z=cm,\n","                x=['UP', 'DOWN', 'STAY'],\n","                y=['UP', 'DOWN', 'STAY'],\n","                colorscale='Blues',\n","                text=cm,\n","                texttemplate=\"%{text}\",\n","                textfont={\"size\": 16},\n","                hoverongaps=False\n","            )\n","        )\n","\n","        fig.update_layout(\n","            title=\"🎯 Overall Confusion Matrix - RF Models\",\n","            xaxis_title=\"Predicted\",\n","            yaxis_title=\"Actual\",\n","            height=500\n","        )\n","\n","        fig.show()\n","\n","# ===================================================================\n","# 🚀 SIMPLE MAIN EXECUTION\n","# ===================================================================\n","\n","def run_simple_rf_evaluation():\n","    \"\"\"Run simple RF F1 evaluation\"\"\"\n","    print(\"\\n🚀 RUNNING SIMPLE RF F1 EVALUATION\")\n","    print(\"=\"*50)\n","\n","    if not rf_available:\n","        print(\"❌ No RF models available\")\n","        return None\n","\n","    # Initialize\n","    evaluator = SimpleRFEvaluator()\n","\n","    # Step 1: Load RF models\n","    print(\"🔄 Step 1: Loading RF models...\")\n","    if not evaluator.load_rf_models_simple():\n","        print(\"❌ Cannot load RF models\")\n","        return None\n","\n","    # Step 2: Evaluate F1\n","    print(\"\\n🔄 Step 2: Evaluating F1 scores...\")\n","    if not evaluator.evaluate_f1_simple():\n","        print(\"❌ F1 evaluation failed\")\n","        return None\n","\n","    # Step 3: Create visualization\n","    print(\"\\n🔄 Step 3: Creating F1 visualization...\")\n","    evaluator.create_f1_visualization()\n","\n","    # Final summary\n","    overall = evaluator.f1_results['overall_results']\n","    print(f\"\\n🎉 RF F1 EVALUATION COMPLETED!\")\n","    print(f\"📊 Overall F1 Macro: {overall['f1_macro']:.3f}\")\n","    print(f\"📊 Overall F1 Weighted: {overall['f1_weighted']:.3f}\")\n","    print(f\"📊 Overall Accuracy: {overall['accuracy']:.3f}\")\n","    print(f\"📊 Models Evaluated: {overall['models_evaluated']}\")\n","\n","    return evaluator.f1_results\n","\n","# ===================================================================\n","# 🎯 EXECUTE\n","# ===================================================================\n","\n","if rf_available:\n","    results = run_simple_rf_evaluation()\n","else:\n","    print(\"❌ Cannot run evaluation - no RF models found\")\n","    results = None\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"🌲 SIMPLE RF F1 EVALUATION COMPLETE!\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0aPMqAkxi1jn","executionInfo":{"status":"ok","timestamp":1753429756776,"user_tz":-420,"elapsed":8418,"user":{"displayName":"wahyu suhada","userId":"00382180526706636659"}},"outputId":"ff2e137a-8470-4f5a-8600-871133372c3f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["🌲 SIMPLE RF F1 SCORE EVALUATION\n","============================================================\n","👤 User: wasirawasenju\n","📅 Date: 2025-07-25 07:43:56 UTC\n","🎯 Focus: F1 Score + Simple Visualization\n","============================================================\n","📁 Quick RF file check...\n","   ✅ Found 6 files in random_forest_models/\n","      ✅ Day1_Trend_rf_model.pkl\n","      ✅ Day2_Trend_rf_model.pkl\n","      ✅ Day3_Trend_rf_model.pkl\n","      ✅ Day4_Trend_rf_model.pkl\n","      ✅ Day5_Trend_rf_model.pkl\n","\n","🚀 RUNNING SIMPLE RF F1 EVALUATION\n","==================================================\n","🔄 Step 1: Loading RF models...\n","🔄 Loading RF models...\n","   ✅ Day1 model loaded\n","   ✅ Day2 model loaded\n","   ✅ Day3 model loaded\n","   ✅ Day4 model loaded\n","   ✅ Day5 model loaded\n","   📊 Loaded 5 RF models\n","\n","🔄 Step 2: Evaluating F1 scores...\n","\n","🎯 EVALUATING F1 SCORES...\n","📊 Creating simple RF test data...\n","   🔄 Creating test data for Day1...\n","      ✅ 50 test samples for Day1\n","   🔄 Creating test data for Day2...\n","      ✅ 50 test samples for Day2\n","   🔄 Creating test data for Day3...\n","      ✅ 50 test samples for Day3\n","   🔄 Creating test data for Day4...\n","      ✅ 50 test samples for Day4\n","   🔄 Creating test data for Day5...\n","      ✅ 50 test samples for Day5\n","   🔄 Evaluating Day1...\n","      📊 F1 Macro: 0.169\n","      📊 F1 Weighted: 0.193\n","      📊 Accuracy: 0.340\n","      📊 Confidence: 0.407\n","   🔄 Evaluating Day2...\n","      📊 F1 Macro: 0.230\n","      📊 F1 Weighted: 0.254\n","      📊 Accuracy: 0.320\n","      📊 Confidence: 0.381\n","   🔄 Evaluating Day3...\n","      📊 F1 Macro: 0.120\n","      📊 F1 Weighted: 0.079\n","      📊 Accuracy: 0.220\n","      📊 Confidence: 0.453\n","   🔄 Evaluating Day4...\n","      📊 F1 Macro: 0.092\n","      📊 F1 Weighted: 0.044\n","      📊 Accuracy: 0.160\n","      📊 Confidence: 0.487\n","   🔄 Evaluating Day5...\n","      📊 F1 Macro: 0.120\n","      📊 F1 Weighted: 0.079\n","      📊 Accuracy: 0.220\n","      📊 Confidence: 0.518\n","\n","📊 OVERALL F1 RESULTS:\n","   🎯 Overall F1 Macro: 0.196\n","   🎯 Overall F1 Weighted: 0.200\n","   🎯 Overall Accuracy: 0.252\n","   📊 Total Predictions: 250\n","   📊 Models Evaluated: 5\n","\n","🔄 Step 3: Creating F1 visualization...\n","\n","📊 CREATING F1 VISUALIZATION...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"68fa2e31-3be7-487d-9499-3f84f828e870\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"68fa2e31-3be7-487d-9499-3f84f828e870\")) {                    Plotly.newPlot(                        \"68fa2e31-3be7-487d-9499-3f84f828e870\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"F1 Macro\",\"text\":[\"0.169\",\"0.230\",\"0.120\",\"0.092\",\"0.120\"],\"textposition\":\"auto\",\"x\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":[\"green\",\"red\",\"orange\"]},\"name\":\"F1 by Class (Day1)\",\"text\":[\"0.000\",\"0.000\",\"0.507\"],\"textposition\":\"auto\",\"x\":[\"UP\",\"DOWN\",\"STAY\"],\"y\":[0.0,0.0,0.5074626865671642],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"purple\",\"size\":12},\"mode\":\"markers+text\",\"name\":\"Accuracy vs F1\",\"text\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"textposition\":\"top center\",\"x\":[0.34,0.32,0.22,0.16,0.22],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":[\"blue\",\"green\",\"red\"]},\"name\":\"Overall Performance\",\"text\":[\"0.196\",\"0.200\",\"0.252\"],\"textposition\":\"auto\",\"x\":[\"F1 Macro\",\"F1 Weighted\",\"Accuracy\"],\"y\":[0.1964616382869104,0.19993734681075348,0.252],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"F1 Macro Score by Day\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"F1 Score by Class (Day1)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Accuracy vs F1 Score\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Overall Performance Summary\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"🌲 Random Forest F1 Score Evaluation\"},\"height\":800,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('68fa2e31-3be7-487d-9499-3f84f828e870');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   📊 Creating F1 summary chart...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\")) {                    Plotly.newPlot(                        \"804dbfe7-ff94-423b-8ec5-d71fde7ad8ae\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"F1 Macro Score\",\"text\":[\"0.169\",\"0.230\",\"0.120\",\"0.092\",\"0.120\"],\"textposition\":\"auto\",\"x\":[\"Day1\",\"Day2\",\"Day3\",\"Day4\",\"Day5\"],\"y\":[0.1691542288557214,0.22969696969696965,0.12021857923497269,0.09195402298850575,0.12021857923497269],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"shapes\":[{\"line\":{\"color\":\"red\",\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"xref\":\"x domain\",\"y0\":0.65,\"y1\":0.65,\"yref\":\"y\"}],\"annotations\":[{\"showarrow\":false,\"text\":\"Good F1 Benchmark (0.65)\",\"x\":1,\"xanchor\":\"right\",\"xref\":\"x domain\",\"y\":0.65,\"yanchor\":\"bottom\",\"yref\":\"y\"}],\"yaxis\":{\"title\":{\"text\":\"F1 Macro Score\"},\"range\":[0,1]},\"title\":{\"text\":\"📊 F1 Macro Scores - RF Models Performance\"},\"xaxis\":{\"title\":{\"text\":\"RF Models\"}},\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('804dbfe7-ff94-423b-8ec5-d71fde7ad8ae');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   📊 Creating confusion matrix...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"60e7b256-c14a-43b3-9166-db0bad5a8eeb\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"60e7b256-c14a-43b3-9166-db0bad5a8eeb\")) {                    Plotly.newPlot(                        \"60e7b256-c14a-43b3-9166-db0bad5a8eeb\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"hoverongaps\":false,\"text\":[[44,6,16],[51,1,15],[92,7,18]],\"textfont\":{\"size\":16},\"texttemplate\":\"%{text}\",\"x\":[\"UP\",\"DOWN\",\"STAY\"],\"y\":[\"UP\",\"DOWN\",\"STAY\"],\"z\":[[44,6,16],[51,1,15],[92,7,18]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"🎯 Overall Confusion Matrix - RF Models\"},\"xaxis\":{\"title\":{\"text\":\"Predicted\"}},\"yaxis\":{\"title\":{\"text\":\"Actual\"}},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('60e7b256-c14a-43b3-9166-db0bad5a8eeb');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   ✅ F1 visualization created!\n","\n","🎉 RF F1 EVALUATION COMPLETED!\n","📊 Overall F1 Macro: 0.196\n","📊 Overall F1 Weighted: 0.200\n","📊 Overall Accuracy: 0.252\n","📊 Models Evaluated: 5\n","\n","============================================================\n","🌲 SIMPLE RF F1 EVALUATION COMPLETE!\n","============================================================\n"]}]}]}